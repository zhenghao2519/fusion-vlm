2025-02-28 21:48:34,186 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
PyTorch: 1.13.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.1
OpenCV: 4.11.0
MMCV: 1.6.2
MMCV Compiler: GCC 11.4
MMCV CUDA Compiler: 11.8
MMDetection: 2.28.2
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+7175f0d
spconv2.0: True
------------------------------------------------------------

2025-02-28 21:48:34,869 - mmdet - INFO - Distributed training: True
2025-02-28 21:48:35,648 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'CustomNuScenesDataset'
data_root = './data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadAnnotations3D',
        with_bbox_3d=True,
        with_label_3d=True,
        with_bbox=True,
        with_label=True,
        with_bbox_depth=True),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='ResizeCropFlipRotImage',
        data_aug_conf=dict(
            resize_lim=(0.37, 0.45),
            final_dim=(320, 640),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(0.0, 0.0),
            H=900,
            W=1600,
            rand_flip=False),
        training=True),
    dict(
        type='ResizeMultiview3D',
        img_scale=(640, 640),
        keep_ratio=False,
        multiscale_mode='value'),
    dict(
        type='LoadAnnoatationVQA',
        base_vqa_path='./data/nuscenes/vqa/train/',
        base_desc_path='./data/nuscenes/desc/train/',
        base_conv_path='./data/nuscenes/conv/train/',
        base_key_path='./data/nuscenes/keywords/train/',
        tokenizer='ckpts/pretrain_qformer/',
        max_length=2048,
        ignore_type=[],
        lane_objs_info='./data/nuscenes/lane_obj_train.pkl'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='PETRFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        collect_keys=[
            'lidar2img', 'intrinsics', 'extrinsics', 'timestamp',
            'img_timestamp', 'ego_pose', 'ego_pose_inv', 'command', 'can_bus',
            'prev_exists'
        ]),
    dict(
        type='Collect3D',
        keys=[
            'lane_pts', 'input_ids', 'vlm_labels', 'gt_bboxes_3d',
            'gt_labels_3d', 'img', 'gt_bboxes', 'gt_labels', 'centers2d',
            'depths', 'prev_exists', 'lidar2img', 'intrinsics', 'extrinsics',
            'timestamp', 'img_timestamp', 'ego_pose', 'ego_pose_inv',
            'command', 'can_bus'
        ],
        meta_keys=('filename', 'ori_shape', 'img_shape', 'pad_shape',
                   'scale_factor', 'flip', 'box_mode_3d', 'box_type_3d',
                   'img_norm_cfg', 'scene_token', 'gt_bboxes_3d',
                   'gt_labels_3d'))
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='ResizeCropFlipRotImage',
        data_aug_conf=dict(
            resize_lim=(0.37, 0.45),
            final_dim=(320, 640),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(0.0, 0.0),
            H=900,
            W=1600,
            rand_flip=False),
        training=False),
    dict(
        type='ResizeMultiview3D',
        img_scale=(640, 640),
        keep_ratio=False,
        multiscale_mode='value'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnoatationVQATest',
        base_vqa_path='./data/nuscenes/vqa/val/',
        base_conv_path='./data/nuscenes/conv/val/',
        base_counter_path='./data/nuscenes/eval_cf/',
        load_type=['planning'],
        tokenizer='ckpts/pretrain_qformer/',
        max_length=2048),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='PETRFormatBundle3D',
                collect_keys=[
                    'lidar2img', 'intrinsics', 'extrinsics', 'timestamp',
                    'img_timestamp', 'ego_pose', 'ego_pose_inv', 'command',
                    'can_bus'
                ],
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(
                type='Collect3D',
                keys=[
                    'input_ids', 'img', 'lidar2img', 'intrinsics',
                    'extrinsics', 'timestamp', 'img_timestamp', 'ego_pose',
                    'ego_pose_inv', 'command', 'can_bus'
                ],
                meta_keys=('sample_idx', 'vlm_labels', 'filename', 'ori_shape',
                           'img_shape', 'pad_shape', 'scale_factor', 'flip',
                           'box_mode_3d', 'box_type_3d', 'img_norm_cfg',
                           'scene_token'))
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=2,
    train=dict(
        type='CustomNuScenesDataset',
        data_root='./data/nuscenes/',
        ann_file='./data/nuscenes/nuscenes2d_ego_temporal_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_bbox=True,
                with_label=True,
                with_bbox_depth=True),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=True),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(
                type='LoadAnnoatationVQA',
                base_vqa_path='./data/nuscenes/vqa/train/',
                base_desc_path='./data/nuscenes/desc/train/',
                base_conv_path='./data/nuscenes/conv/train/',
                base_key_path='./data/nuscenes/keywords/train/',
                tokenizer='ckpts/pretrain_qformer/',
                max_length=2048,
                ignore_type=[],
                lane_objs_info='./data/nuscenes/lane_obj_train.pkl'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='PETRFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                collect_keys=[
                    'lidar2img', 'intrinsics', 'extrinsics', 'timestamp',
                    'img_timestamp', 'ego_pose', 'ego_pose_inv', 'command',
                    'can_bus', 'prev_exists'
                ]),
            dict(
                type='Collect3D',
                keys=[
                    'lane_pts', 'input_ids', 'vlm_labels', 'gt_bboxes_3d',
                    'gt_labels_3d', 'img', 'gt_bboxes', 'gt_labels',
                    'centers2d', 'depths', 'prev_exists', 'lidar2img',
                    'intrinsics', 'extrinsics', 'timestamp', 'img_timestamp',
                    'ego_pose', 'ego_pose_inv', 'command', 'can_bus'
                ],
                meta_keys=('filename', 'ori_shape', 'img_shape', 'pad_shape',
                           'scale_factor', 'flip', 'box_mode_3d',
                           'box_type_3d', 'img_norm_cfg', 'scene_token',
                           'gt_bboxes_3d', 'gt_labels_3d'))
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        seq_split_num=1,
        seq_mode=True,
        use_valid_flag=True,
        filter_empty_gt=False),
    val=dict(
        type='CustomNuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='./data/nuscenes/nuscenes2d_ego_temporal_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=False),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnoatationVQATest',
                base_vqa_path='./data/nuscenes/vqa/val/',
                base_conv_path='./data/nuscenes/conv/val/',
                base_counter_path='./data/nuscenes/eval_cf/',
                load_type=['planning'],
                tokenizer='ckpts/pretrain_qformer/',
                max_length=2048),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='PETRFormatBundle3D',
                        collect_keys=[
                            'lidar2img', 'intrinsics', 'extrinsics',
                            'timestamp', 'img_timestamp', 'ego_pose',
                            'ego_pose_inv', 'command', 'can_bus'
                        ],
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='Collect3D',
                        keys=[
                            'input_ids', 'img', 'lidar2img', 'intrinsics',
                            'extrinsics', 'timestamp', 'img_timestamp',
                            'ego_pose', 'ego_pose_inv', 'command', 'can_bus'
                        ],
                        meta_keys=('sample_idx', 'vlm_labels', 'filename',
                                   'ori_shape', 'img_shape', 'pad_shape',
                                   'scale_factor', 'flip', 'box_mode_3d',
                                   'box_type_3d', 'img_norm_cfg',
                                   'scene_token'))
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        eval_mode=['lane', 'det']),
    test=dict(
        type='CustomNuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='./data/nuscenes/nuscenes2d_ego_temporal_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=False),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnoatationVQATest',
                base_vqa_path='./data/nuscenes/vqa/val/',
                base_conv_path='./data/nuscenes/conv/val/',
                base_counter_path='./data/nuscenes/eval_cf/',
                load_type=['planning'],
                tokenizer='ckpts/pretrain_qformer/',
                max_length=2048),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='PETRFormatBundle3D',
                        collect_keys=[
                            'lidar2img', 'intrinsics', 'extrinsics',
                            'timestamp', 'img_timestamp', 'ego_pose',
                            'ego_pose_inv', 'command', 'can_bus'
                        ],
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(
                        type='Collect3D',
                        keys=[
                            'input_ids', 'img', 'lidar2img', 'intrinsics',
                            'extrinsics', 'timestamp', 'img_timestamp',
                            'ego_pose', 'ego_pose_inv', 'command', 'can_bus'
                        ],
                        meta_keys=('sample_idx', 'vlm_labels', 'filename',
                                   'ori_shape', 'img_shape', 'pad_shape',
                                   'scale_factor', 'flip', 'box_mode_3d',
                                   'box_type_3d', 'img_norm_cfg',
                                   'scene_token'))
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        eval_mode=['lane', 'det']),
    shuffler_sampler=dict(
        type='InfiniteGroupEachSampleInBatchSampler',
        seq_split_num=2,
        warmup_split_num=10,
        num_iters_to_seq=28130),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=168780,
    pipeline=[
        dict(type='LoadMultiViewImageFromFiles', to_float32=True),
        dict(
            type='ResizeCropFlipRotImage',
            data_aug_conf=dict(
                resize_lim=(0.37, 0.45),
                final_dim=(320, 640),
                bot_pct_lim=(0.0, 0.0),
                rot_lim=(0.0, 0.0),
                H=900,
                W=1600,
                rand_flip=False),
            training=False),
        dict(
            type='ResizeMultiview3D',
            img_scale=(640, 640),
            keep_ratio=False,
            multiscale_mode='value'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[123.675, 116.28, 103.53],
            std=[58.395, 57.12, 57.375],
            to_rgb=True),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnoatationVQATest',
            base_vqa_path='./data/nuscenes/vqa/val/',
            base_conv_path='./data/nuscenes/conv/val/',
            base_counter_path='./data/nuscenes/eval_cf/',
            load_type=['planning'],
            tokenizer='ckpts/pretrain_qformer/',
            max_length=2048),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1333, 800),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='PETRFormatBundle3D',
                    collect_keys=[
                        'lidar2img', 'intrinsics', 'extrinsics', 'timestamp',
                        'img_timestamp', 'ego_pose', 'ego_pose_inv', 'command',
                        'can_bus'
                    ],
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(
                    type='Collect3D',
                    keys=[
                        'input_ids', 'img', 'lidar2img', 'intrinsics',
                        'extrinsics', 'timestamp', 'img_timestamp', 'ego_pose',
                        'ego_pose_inv', 'command', 'can_bus'
                    ],
                    meta_keys=('sample_idx', 'vlm_labels', 'filename',
                               'ori_shape', 'img_shape', 'pad_shape',
                               'scale_factor', 'flip', 'box_mode_3d',
                               'box_type_3d', 'img_norm_cfg', 'scene_token'))
            ])
    ])
checkpoint_config = dict(interval=14065, max_keep_ckpts=3)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = 'work_dirs/mask_eva_lane_det_vlm/'
load_from = 'ckpts/fcos3d_vovnet_imgbackbone-remapped.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
backbone_norm_cfg = dict(type='LN', requires_grad=True)
plugin = True
plugin_dir = 'projects/mmdet3d_plugin/'
voxel_size = [0.2, 0.2, 8]
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
num_gpus = 1
batch_size = 1
num_iters_per_epoch = 28130
num_epochs = 6
llm_path = 'ckpts/pretrain_qformer/'
collect_keys = [
    'lidar2img', 'intrinsics', 'extrinsics', 'timestamp', 'img_timestamp',
    'ego_pose', 'ego_pose_inv', 'command', 'can_bus'
]
model = dict(
    type='Petr3D',
    save_path='./results_planning_only/',
    use_grid_mask=True,
    frozen=False,
    use_lora=True,
    tokenizer='ckpts/pretrain_qformer/',
    lm_head='ckpts/pretrain_qformer/',
    img_backbone=dict(
        type='VoVNet',
        spec_name='V-99-eSE',
        norm_eval=True,
        frozen_stages=-1,
        input_ch=3,
        out_features='stage4'),
    img_neck=dict(
        type='CPFPN', in_channels=[768], out_channels=256, num_outs=2),
    map_head=dict(
        type='PETRHeadM',
        num_classes=1,
        in_channels=256,
        out_dims=4096,
        memory_len=600,
        with_mask=True,
        topk_proposals=300,
        num_lane=1800,
        num_lanes_one2one=300,
        k_one2many=5,
        lambda_one2many=1.0,
        num_extra=256,
        n_control=11,
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        code_weights=[1.0, 1.0],
        transformer=dict(
            type='PETRTemporalTransformer',
            input_dimension=256,
            output_dimension=256,
            num_layers=6,
            embed_dims=256,
            num_heads=8,
            feedforward_dims=2048,
            dropout=0.1,
            with_cp=True,
            flash_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='LaneHungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=1.5),
                reg_cost=dict(type='LaneL1Cost', weight=0.02),
                iou_cost=dict(type='IoUCost', weight=0.0))),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.5),
        loss_bbox=dict(type='L1Loss', loss_weight=0.02),
        loss_dir=dict(type='PtsDirCosLoss', loss_weight=0.0)),
    pts_bbox_head=dict(
        type='StreamPETRHead',
        num_classes=10,
        in_channels=256,
        out_dims=4096,
        num_query=600,
        with_mask=True,
        memory_len=600,
        topk_proposals=300,
        num_propagated=300,
        num_extra=256,
        n_control=11,
        match_with_velo=False,
        scalar=10,
        noise_scale=1.0,
        dn_weight=1.0,
        split=0.75,
        code_weights=[2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
        transformer=dict(
            type='PETRTemporalTransformer',
            input_dimension=256,
            output_dimension=256,
            num_layers=6,
            embed_dims=256,
            num_heads=8,
            feedforward_dims=2048,
            dropout=0.1,
            with_cp=True,
            flash_attn=True),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=10),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
ida_aug_conf = dict(
    resize_lim=(0.37, 0.45),
    final_dim=(320, 640),
    bot_pct_lim=(0.0, 0.0),
    rot_lim=(0.0, 0.0),
    H=900,
    W=1600,
    rand_flip=False)
optimizer = dict(
    constructor='LearningRateDecayOptimizerConstructor',
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.0001,
    paramwise_cfg=dict(
        decay_rate=0.9,
        head_decay_rate=4.0,
        lm_head_decay_rate=0.1,
        decay_type='vit_wise',
        num_layers=24))
optimizer_config = dict(
    type='Fp16OptimizerHook',
    loss_scale='dynamic',
    grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
find_unused_parameters = False
runner = dict(type='IterBasedRunner', max_iters=168780)
gpu_ids = range(0, 1)

2025-02-28 21:48:35,649 - mmdet - INFO - Set random seed to 0, deterministic: False
2025-02-28 21:49:18,834 - mmdet - INFO - initialize CPFPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

position_range - torch.Size([6]): 
The value is the same before and after calling `init_weights` of Petr3D  

coords_d - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.match_costs - torch.Size([10]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.pc_range - torch.Size([6]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.input_projection.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.input_projection.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.output_projection.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.output_projection.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.reference_points.weight - torch.Size([600, 3]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.pseudo_reference_points.weight - torch.Size([300, 3]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.query_embedding.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.can_bus_embed.0.weight - torch.Size([1024, 74]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.can_bus_embed.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.can_bus_embed.2.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.can_bus_embed.2.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in StreamPETRHead  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.query_pos.0.weight - torch.Size([256, 396]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.query_pos.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.query_pos.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.query_pos.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.time_embedding.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.time_embedding.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.time_embedding.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.time_embedding.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.ego_pose_pe.reduce.0.weight - torch.Size([256, 156]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.ego_pose_pe.reduce.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.ego_pose_pe.gamma.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.ego_pose_pe.gamma.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.ego_pose_pe.beta.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

pts_bbox_head.ego_pose_pe.beta.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_1/conv.weight - torch.Size([64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_1/norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_1/norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_2/conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_2/norm.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_2/norm.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_3/conv.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_3/norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stem.stem_3/norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.0.OSA2_1_0/conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.0.OSA2_1_0/norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.0.OSA2_1_0/norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.1.OSA2_1_1/conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.1.OSA2_1_1/norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.1.OSA2_1_1/norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.2.OSA2_1_2/conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.2.OSA2_1_2/norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.2.OSA2_1_2/norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.3.OSA2_1_3/conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.3.OSA2_1_3/norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.3.OSA2_1_3/norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.4.OSA2_1_4/conv.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.4.OSA2_1_4/norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.layers.4.OSA2_1_4/norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.concat.OSA2_1_concat/conv.weight - torch.Size([256, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.concat.OSA2_1_concat/norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.concat.OSA2_1_concat/norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.ese.fc.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage2.OSA2_1.ese.fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.0.OSA3_1_0/conv.weight - torch.Size([160, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.0.OSA3_1_0/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.0.OSA3_1_0/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.1.OSA3_1_1/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.1.OSA3_1_1/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.1.OSA3_1_1/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.2.OSA3_1_2/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.2.OSA3_1_2/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.2.OSA3_1_2/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.3.OSA3_1_3/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.3.OSA3_1_3/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.3.OSA3_1_3/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.4.OSA3_1_4/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.4.OSA3_1_4/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.layers.4.OSA3_1_4/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.concat.OSA3_1_concat/conv.weight - torch.Size([512, 1056, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.concat.OSA3_1_concat/norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.concat.OSA3_1_concat/norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.ese.fc.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_1.ese.fc.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.0.OSA3_2_0/conv.weight - torch.Size([160, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.0.OSA3_2_0/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.0.OSA3_2_0/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.1.OSA3_2_1/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.1.OSA3_2_1/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.1.OSA3_2_1/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.2.OSA3_2_2/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.2.OSA3_2_2/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.2.OSA3_2_2/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.3.OSA3_2_3/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.3.OSA3_2_3/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.3.OSA3_2_3/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.4.OSA3_2_4/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.4.OSA3_2_4/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.layers.4.OSA3_2_4/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.concat.OSA3_2_concat/conv.weight - torch.Size([512, 1312, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.concat.OSA3_2_concat/norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.concat.OSA3_2_concat/norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.ese.fc.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_2.ese.fc.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.0.OSA3_3_0/conv.weight - torch.Size([160, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.0.OSA3_3_0/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.0.OSA3_3_0/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.1.OSA3_3_1/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.1.OSA3_3_1/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.1.OSA3_3_1/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.2.OSA3_3_2/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.2.OSA3_3_2/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.2.OSA3_3_2/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.3.OSA3_3_3/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.3.OSA3_3_3/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.3.OSA3_3_3/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.4.OSA3_3_4/conv.weight - torch.Size([160, 160, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.4.OSA3_3_4/norm.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.layers.4.OSA3_3_4/norm.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.concat.OSA3_3_concat/conv.weight - torch.Size([512, 1312, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.concat.OSA3_3_concat/norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.concat.OSA3_3_concat/norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.ese.fc.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage3.OSA3_3.ese.fc.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.0.OSA4_1_0/conv.weight - torch.Size([192, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.0.OSA4_1_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.0.OSA4_1_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.1.OSA4_1_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.1.OSA4_1_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.1.OSA4_1_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.2.OSA4_1_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.2.OSA4_1_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.2.OSA4_1_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.3.OSA4_1_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.3.OSA4_1_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.3.OSA4_1_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.4.OSA4_1_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.4.OSA4_1_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.layers.4.OSA4_1_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.concat.OSA4_1_concat/conv.weight - torch.Size([768, 1472, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.concat.OSA4_1_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.concat.OSA4_1_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_1.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.0.OSA4_2_0/conv.weight - torch.Size([192, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.0.OSA4_2_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.0.OSA4_2_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.1.OSA4_2_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.1.OSA4_2_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.1.OSA4_2_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.2.OSA4_2_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.2.OSA4_2_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.2.OSA4_2_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.3.OSA4_2_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.3.OSA4_2_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.3.OSA4_2_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.4.OSA4_2_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.4.OSA4_2_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.layers.4.OSA4_2_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.concat.OSA4_2_concat/conv.weight - torch.Size([768, 1728, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.concat.OSA4_2_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.concat.OSA4_2_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_2.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.0.OSA4_3_0/conv.weight - torch.Size([192, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.0.OSA4_3_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.0.OSA4_3_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.1.OSA4_3_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.1.OSA4_3_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.1.OSA4_3_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.2.OSA4_3_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.2.OSA4_3_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.2.OSA4_3_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.3.OSA4_3_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.3.OSA4_3_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.3.OSA4_3_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.4.OSA4_3_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.4.OSA4_3_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.layers.4.OSA4_3_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.concat.OSA4_3_concat/conv.weight - torch.Size([768, 1728, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.concat.OSA4_3_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.concat.OSA4_3_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_3.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.0.OSA4_4_0/conv.weight - torch.Size([192, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.0.OSA4_4_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.0.OSA4_4_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.1.OSA4_4_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.1.OSA4_4_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.1.OSA4_4_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.2.OSA4_4_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.2.OSA4_4_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.2.OSA4_4_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.3.OSA4_4_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.3.OSA4_4_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.3.OSA4_4_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.4.OSA4_4_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.4.OSA4_4_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.layers.4.OSA4_4_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.concat.OSA4_4_concat/conv.weight - torch.Size([768, 1728, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.concat.OSA4_4_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.concat.OSA4_4_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_4.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.0.OSA4_5_0/conv.weight - torch.Size([192, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.0.OSA4_5_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.0.OSA4_5_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.1.OSA4_5_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.1.OSA4_5_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.1.OSA4_5_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.2.OSA4_5_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.2.OSA4_5_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.2.OSA4_5_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.3.OSA4_5_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.3.OSA4_5_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.3.OSA4_5_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.4.OSA4_5_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.4.OSA4_5_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.layers.4.OSA4_5_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.concat.OSA4_5_concat/conv.weight - torch.Size([768, 1728, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.concat.OSA4_5_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.concat.OSA4_5_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_5.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.0.OSA4_6_0/conv.weight - torch.Size([192, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.0.OSA4_6_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.0.OSA4_6_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.1.OSA4_6_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.1.OSA4_6_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.1.OSA4_6_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.2.OSA4_6_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.2.OSA4_6_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.2.OSA4_6_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.3.OSA4_6_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.3.OSA4_6_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.3.OSA4_6_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.4.OSA4_6_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.4.OSA4_6_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.layers.4.OSA4_6_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.concat.OSA4_6_concat/conv.weight - torch.Size([768, 1728, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.concat.OSA4_6_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.concat.OSA4_6_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_6.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.0.OSA4_7_0/conv.weight - torch.Size([192, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.0.OSA4_7_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.0.OSA4_7_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.1.OSA4_7_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.1.OSA4_7_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.1.OSA4_7_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.2.OSA4_7_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.2.OSA4_7_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.2.OSA4_7_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.3.OSA4_7_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.3.OSA4_7_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.3.OSA4_7_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.4.OSA4_7_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.4.OSA4_7_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.layers.4.OSA4_7_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.concat.OSA4_7_concat/conv.weight - torch.Size([768, 1728, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.concat.OSA4_7_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.concat.OSA4_7_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_7.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.0.OSA4_8_0/conv.weight - torch.Size([192, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.0.OSA4_8_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.0.OSA4_8_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.1.OSA4_8_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.1.OSA4_8_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.1.OSA4_8_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.2.OSA4_8_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.2.OSA4_8_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.2.OSA4_8_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.3.OSA4_8_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.3.OSA4_8_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.3.OSA4_8_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.4.OSA4_8_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.4.OSA4_8_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.layers.4.OSA4_8_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.concat.OSA4_8_concat/conv.weight - torch.Size([768, 1728, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.concat.OSA4_8_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.concat.OSA4_8_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_8.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.0.OSA4_9_0/conv.weight - torch.Size([192, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.0.OSA4_9_0/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.0.OSA4_9_0/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.1.OSA4_9_1/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.1.OSA4_9_1/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.1.OSA4_9_1/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.2.OSA4_9_2/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.2.OSA4_9_2/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.2.OSA4_9_2/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.3.OSA4_9_3/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.3.OSA4_9_3/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.3.OSA4_9_3/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.4.OSA4_9_4/conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.4.OSA4_9_4/norm.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.layers.4.OSA4_9_4/norm.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.concat.OSA4_9_concat/conv.weight - torch.Size([768, 1728, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.concat.OSA4_9_concat/norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.concat.OSA4_9_concat/norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.ese.fc.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_backbone.stage4.OSA4_9.ese.fc.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 768, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.code_weights - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.match_costs - torch.Size([2]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.pc_range - torch.Size([6]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.6.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.cls_branches.0.6.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.reg_branches.0.4.weight - torch.Size([33, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.reg_branches.0.4.bias - torch.Size([33]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.input_projection.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.input_projection.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.output_projection.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.output_projection.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.reference_points_lane.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.reference_points_lane.bias - torch.Size([3]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.points_embedding_lane.weight - torch.Size([11, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.instance_embedding_lane.weight - torch.Size([1800, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.query_embedding.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.0.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.1.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.2.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.3.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.4.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in PETRHeadM  

map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.5.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

map_head.transformer.query_decoder._layers.5.transformer_layers.5.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

position_encoder.0.weight - torch.Size([1024, 192]): 
The value is the same before and after calling `init_weights` of Petr3D  

position_encoder.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of Petr3D  

position_encoder.2.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of Petr3D  

position_encoder.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.embed_tokens.weight - torch.Size([32000, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.0.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.0.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.1.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.1.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.2.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.2.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.3.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.3.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.4.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.4.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.5.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.5.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.6.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.6.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.7.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.7.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.8.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.8.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.9.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.9.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.10.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.10.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.11.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.11.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.12.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.12.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.13.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.13.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.14.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.14.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.15.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.15.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.16.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.16.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.17.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.17.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.18.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.18.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.19.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.19.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.20.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.20.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.21.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.21.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.22.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.22.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.23.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.23.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.24.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.24.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.25.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.25.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.26.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.26.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.27.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.27.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.28.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.28.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.29.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.29.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.30.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.30.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.self_attn.q_proj.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight - torch.Size([4096, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight - torch.Size([2, 4096]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight - torch.Size([4096, 2]): 
Initialized by user-defined `init_weights` in PeftModelForCausalLM  

lm_head.base_model.model.model.layers.31.mlp.gate_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.mlp.up_proj.weight - torch.Size([11008, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.mlp.down_proj.weight - torch.Size([4096, 11008]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.input_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.layers.31.post_attention_layernorm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.model.norm.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of Petr3D  

lm_head.base_model.model.lm_head.weight - torch.Size([32000, 4096]): 
The value is the same before and after calling `init_weights` of Petr3D  
2025-02-28 21:49:20,166 - mmdet - INFO - Model:
Petr3D(
  (pts_bbox_head): StreamPETRHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (input_projection): Linear(in_features=256, out_features=256, bias=True)
    (output_projection): Linear(in_features=256, out_features=4096, bias=True)
    (reference_points): Embedding(600, 3)
    (pseudo_reference_points): Embedding(300, 3)
    (query_embedding): Embedding(256, 256)
    (can_bus_embed): Sequential(
      (0): Linear(in_features=74, out_features=1024, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1024, out_features=4096, bias=True)
    )
    (loss_iou): GIoULoss()
    (transformer): PETRTemporalTransformer(
      (query_decoder): PETRTransformerDecoder(
        (_layers): ModuleList(
          (0): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (query_pos): Sequential(
      (0): Linear(in_features=396, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (time_embedding): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (ego_pose_pe): MLN(
      (reduce): Sequential(
        (0): Linear(in_features=156, out_features=256, bias=True)
        (1): ReLU()
      )
      (gamma): Linear(in_features=256, out_features=256, bias=True)
      (beta): Linear(in_features=256, out_features=256, bias=True)
      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
    )
  )
  (img_backbone): VoVNet(
    (stem): Sequential(
      (stem_1/conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (stem_1/norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (stem_1/relu): ReLU(inplace=True)
      (stem_2/conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (stem_2/norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (stem_2/relu): ReLU(inplace=True)
      (stem_3/conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (stem_3/norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (stem_3/relu): ReLU(inplace=True)
    )
    (stage2): _OSA_stage(
      (OSA2_1): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA2_1_0/conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA2_1_0/norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA2_1_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA2_1_1/conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA2_1_1/norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA2_1_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA2_1_2/conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA2_1_2/norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA2_1_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA2_1_3/conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA2_1_3/norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA2_1_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA2_1_4/conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA2_1_4/norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA2_1_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA2_1_concat/conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA2_1_concat/norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA2_1_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
    )
    (stage3): _OSA_stage(
      (Pooling): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
      (OSA3_1): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA3_1_0/conv): Conv2d(256, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_1_0/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_1_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA3_1_1/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_1_1/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_1_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA3_1_2/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_1_2/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_1_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA3_1_3/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_1_3/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_1_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA3_1_4/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_1_4/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_1_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA3_1_concat/conv): Conv2d(1056, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA3_1_concat/norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA3_1_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA3_2): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA3_2_0/conv): Conv2d(512, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_2_0/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_2_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA3_2_1/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_2_1/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_2_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA3_2_2/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_2_2/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_2_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA3_2_3/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_2_3/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_2_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA3_2_4/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_2_4/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_2_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA3_2_concat/conv): Conv2d(1312, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA3_2_concat/norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA3_2_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA3_3): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA3_3_0/conv): Conv2d(512, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_3_0/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_3_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA3_3_1/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_3_1/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_3_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA3_3_2/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_3_2/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_3_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA3_3_3/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_3_3/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_3_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA3_3_4/conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA3_3_4/norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA3_3_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA3_3_concat/conv): Conv2d(1312, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA3_3_concat/norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA3_3_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
    )
    (stage4): _OSA_stage(
      (Pooling): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
      (OSA4_1): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_1_0/conv): Conv2d(512, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_1_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_1_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_1_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_1_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_1_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_1_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_1_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_1_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_1_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_1_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_1_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_1_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_1_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_1_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_1_concat/conv): Conv2d(1472, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_1_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_1_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA4_2): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_2_0/conv): Conv2d(768, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_2_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_2_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_2_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_2_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_2_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_2_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_2_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_2_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_2_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_2_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_2_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_2_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_2_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_2_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_2_concat/conv): Conv2d(1728, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_2_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_2_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA4_3): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_3_0/conv): Conv2d(768, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_3_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_3_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_3_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_3_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_3_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_3_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_3_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_3_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_3_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_3_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_3_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_3_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_3_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_3_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_3_concat/conv): Conv2d(1728, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_3_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_3_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA4_4): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_4_0/conv): Conv2d(768, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_4_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_4_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_4_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_4_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_4_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_4_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_4_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_4_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_4_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_4_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_4_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_4_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_4_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_4_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_4_concat/conv): Conv2d(1728, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_4_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_4_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA4_5): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_5_0/conv): Conv2d(768, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_5_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_5_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_5_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_5_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_5_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_5_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_5_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_5_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_5_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_5_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_5_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_5_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_5_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_5_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_5_concat/conv): Conv2d(1728, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_5_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_5_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA4_6): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_6_0/conv): Conv2d(768, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_6_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_6_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_6_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_6_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_6_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_6_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_6_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_6_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_6_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_6_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_6_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_6_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_6_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_6_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_6_concat/conv): Conv2d(1728, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_6_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_6_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA4_7): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_7_0/conv): Conv2d(768, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_7_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_7_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_7_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_7_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_7_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_7_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_7_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_7_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_7_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_7_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_7_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_7_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_7_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_7_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_7_concat/conv): Conv2d(1728, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_7_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_7_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA4_8): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_8_0/conv): Conv2d(768, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_8_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_8_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_8_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_8_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_8_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_8_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_8_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_8_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_8_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_8_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_8_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_8_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_8_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_8_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_8_concat/conv): Conv2d(1728, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_8_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_8_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
      (OSA4_9): _OSA_module(
        (layers): ModuleList(
          (0): Sequential(
            (OSA4_9_0/conv): Conv2d(768, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_9_0/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_9_0/relu): ReLU(inplace=True)
          )
          (1): Sequential(
            (OSA4_9_1/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_9_1/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_9_1/relu): ReLU(inplace=True)
          )
          (2): Sequential(
            (OSA4_9_2/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_9_2/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_9_2/relu): ReLU(inplace=True)
          )
          (3): Sequential(
            (OSA4_9_3/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_9_3/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_9_3/relu): ReLU(inplace=True)
          )
          (4): Sequential(
            (OSA4_9_4/conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (OSA4_9_4/norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (OSA4_9_4/relu): ReLU(inplace=True)
          )
        )
        (concat): Sequential(
          (OSA4_9_concat/conv): Conv2d(1728, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (OSA4_9_concat/norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (OSA4_9_concat/relu): ReLU(inplace=True)
        )
        (ese): eSEModule(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (fc): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
          (hsigmoid): Hsigmoid()
        )
      )
    )
  )
  (img_neck): CPFPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
  (query_pos): Sequential(
    (0): Linear(in_features=396, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (time_embedding): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (ego_pose_pe): MLN(
    (reduce): Sequential(
      (0): Linear(in_features=156, out_features=256, bias=True)
      (1): ReLU()
    )
    (gamma): Linear(in_features=256, out_features=256, bias=True)
    (beta): Linear(in_features=256, out_features=256, bias=True)
    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
  )
  (map_head): PETRHeadM(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=1, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=1, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=1, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=1, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=1, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=1, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=33, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=33, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=33, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=33, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=33, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=33, bias=True)
      )
    )
    (input_projection): Linear(in_features=256, out_features=256, bias=True)
    (output_projection): Linear(in_features=256, out_features=4096, bias=True)
    (reference_points_lane): Linear(in_features=256, out_features=3, bias=True)
    (points_embedding_lane): Embedding(11, 256)
    (instance_embedding_lane): Embedding(1800, 256)
    (query_embedding): Embedding(256, 256)
    (loss_dir): PtsDirCosLoss()
    (transformer): PETRTemporalTransformer(
      (query_decoder): PETRTransformerDecoder(
        (_layers): ModuleList(
          (0): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (query_pos): Sequential(
      (0): Linear(in_features=396, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (time_embedding): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (ego_pose_pe): MLN(
      (reduce): Sequential(
        (0): Linear(in_features=156, out_features=256, bias=True)
        (1): ReLU()
      )
      (gamma): Linear(in_features=256, out_features=256, bias=True)
      (beta): Linear(in_features=256, out_features=256, bias=True)
      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
    )
  )
  (position_encoder): Sequential(
    (0): Linear(in_features=192, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=256, bias=True)
  )
  (lm_head): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlavaLlamaForCausalLM(
        (model): LlavaLlamaModel(
          (embed_tokens): Embedding(32000, 4096, padding_idx=0)
          (layers): ModuleList(
            (0): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (1): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (2): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (3): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (4): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (5): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (6): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (7): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (8): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (9): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (10): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (11): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (12): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (13): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (14): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (15): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (16): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (17): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (18): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (19): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (20): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (21): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (22): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (23): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (24): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (25): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (26): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (27): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (28): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (29): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (30): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
            (31): LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=2, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=2, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
          )
          (norm): LlamaRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
      )
    )
  )
)
2025-02-28 21:49:51,818 - mmdet - INFO - load checkpoint from local path: ckpts/fcos3d_vovnet_imgbackbone-remapped.pth
2025-02-28 21:49:52,252 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: bbox_head.cls_convs.0.conv.weight, bbox_head.cls_convs.0.conv.bias, bbox_head.cls_convs.0.gn.weight, bbox_head.cls_convs.0.gn.bias, bbox_head.cls_convs.1.conv.weight, bbox_head.cls_convs.1.conv.bias, bbox_head.cls_convs.1.gn.weight, bbox_head.cls_convs.1.gn.bias, bbox_head.reg_convs.0.conv.weight, bbox_head.reg_convs.0.conv.bias, bbox_head.reg_convs.0.gn.weight, bbox_head.reg_convs.0.gn.bias, bbox_head.reg_convs.1.conv.weight, bbox_head.reg_convs.1.conv.bias, bbox_head.reg_convs.1.gn.weight, bbox_head.reg_convs.1.gn.bias, bbox_head.conv_cls_prev.0.conv.weight, bbox_head.conv_cls_prev.0.conv.bias, bbox_head.conv_cls_prev.0.gn.weight, bbox_head.conv_cls_prev.0.gn.bias, bbox_head.conv_cls.weight, bbox_head.conv_cls.bias, bbox_head.conv_reg_prevs.0.0.conv.weight, bbox_head.conv_reg_prevs.0.0.conv.bias, bbox_head.conv_reg_prevs.0.0.gn.weight, bbox_head.conv_reg_prevs.0.0.gn.bias, bbox_head.conv_reg_prevs.1.0.conv.weight, bbox_head.conv_reg_prevs.1.0.conv.bias, bbox_head.conv_reg_prevs.1.0.gn.weight, bbox_head.conv_reg_prevs.1.0.gn.bias, bbox_head.conv_reg_prevs.2.0.conv.weight, bbox_head.conv_reg_prevs.2.0.conv.bias, bbox_head.conv_reg_prevs.2.0.gn.weight, bbox_head.conv_reg_prevs.2.0.gn.bias, bbox_head.conv_reg_prevs.3.0.conv.weight, bbox_head.conv_reg_prevs.3.0.conv.bias, bbox_head.conv_reg_prevs.3.0.gn.weight, bbox_head.conv_reg_prevs.3.0.gn.bias, bbox_head.conv_regs.0.weight, bbox_head.conv_regs.0.bias, bbox_head.conv_regs.1.weight, bbox_head.conv_regs.1.bias, bbox_head.conv_regs.2.weight, bbox_head.conv_regs.2.bias, bbox_head.conv_regs.3.weight, bbox_head.conv_regs.3.bias, bbox_head.conv_regs.4.weight, bbox_head.conv_regs.4.bias, bbox_head.conv_dir_cls_prev.0.conv.weight, bbox_head.conv_dir_cls_prev.0.conv.bias, bbox_head.conv_dir_cls_prev.0.gn.weight, bbox_head.conv_dir_cls_prev.0.gn.bias, bbox_head.conv_dir_cls.weight, bbox_head.conv_dir_cls.bias, bbox_head.conv_attr_prev.0.conv.weight, bbox_head.conv_attr_prev.0.conv.bias, bbox_head.conv_attr_prev.0.gn.weight, bbox_head.conv_attr_prev.0.gn.bias, bbox_head.conv_attr.weight, bbox_head.conv_attr.bias, bbox_head.conv_centerness_prev.0.conv.weight, bbox_head.conv_centerness_prev.0.conv.bias, bbox_head.conv_centerness_prev.0.gn.weight, bbox_head.conv_centerness_prev.0.gn.bias, bbox_head.conv_centerness.weight, bbox_head.conv_centerness.bias, bbox_head.scales.0.0.scale, bbox_head.scales.0.1.scale, bbox_head.scales.0.2.scale, bbox_head.scales.1.0.scale, bbox_head.scales.1.1.scale, bbox_head.scales.1.2.scale, bbox_head.scales.2.0.scale, bbox_head.scales.2.1.scale, bbox_head.scales.2.2.scale, bbox_head.scales.3.0.scale, bbox_head.scales.3.1.scale, bbox_head.scales.3.2.scale, bbox_head.scales.4.0.scale, bbox_head.scales.4.1.scale, bbox_head.scales.4.2.scale, img_backbone.stage5.OSA5_1.layers.0.OSA5_1_0/conv.weight, img_backbone.stage5.OSA5_1.layers.0.OSA5_1_0/norm.weight, img_backbone.stage5.OSA5_1.layers.0.OSA5_1_0/norm.bias, img_backbone.stage5.OSA5_1.layers.0.OSA5_1_0/norm.running_mean, img_backbone.stage5.OSA5_1.layers.0.OSA5_1_0/norm.running_var, img_backbone.stage5.OSA5_1.layers.0.OSA5_1_0/norm.num_batches_tracked, img_backbone.stage5.OSA5_1.layers.1.OSA5_1_1/conv.weight, img_backbone.stage5.OSA5_1.layers.1.OSA5_1_1/norm.weight, img_backbone.stage5.OSA5_1.layers.1.OSA5_1_1/norm.bias, img_backbone.stage5.OSA5_1.layers.1.OSA5_1_1/norm.running_mean, img_backbone.stage5.OSA5_1.layers.1.OSA5_1_1/norm.running_var, img_backbone.stage5.OSA5_1.layers.1.OSA5_1_1/norm.num_batches_tracked, img_backbone.stage5.OSA5_1.layers.2.OSA5_1_2/conv.weight, img_backbone.stage5.OSA5_1.layers.2.OSA5_1_2/norm.weight, img_backbone.stage5.OSA5_1.layers.2.OSA5_1_2/norm.bias, img_backbone.stage5.OSA5_1.layers.2.OSA5_1_2/norm.running_mean, img_backbone.stage5.OSA5_1.layers.2.OSA5_1_2/norm.running_var, img_backbone.stage5.OSA5_1.layers.2.OSA5_1_2/norm.num_batches_tracked, img_backbone.stage5.OSA5_1.layers.3.OSA5_1_3/conv.weight, img_backbone.stage5.OSA5_1.layers.3.OSA5_1_3/norm.weight, img_backbone.stage5.OSA5_1.layers.3.OSA5_1_3/norm.bias, img_backbone.stage5.OSA5_1.layers.3.OSA5_1_3/norm.running_mean, img_backbone.stage5.OSA5_1.layers.3.OSA5_1_3/norm.running_var, img_backbone.stage5.OSA5_1.layers.3.OSA5_1_3/norm.num_batches_tracked, img_backbone.stage5.OSA5_1.layers.4.OSA5_1_4/conv.weight, img_backbone.stage5.OSA5_1.layers.4.OSA5_1_4/norm.weight, img_backbone.stage5.OSA5_1.layers.4.OSA5_1_4/norm.bias, img_backbone.stage5.OSA5_1.layers.4.OSA5_1_4/norm.running_mean, img_backbone.stage5.OSA5_1.layers.4.OSA5_1_4/norm.running_var, img_backbone.stage5.OSA5_1.layers.4.OSA5_1_4/norm.num_batches_tracked, img_backbone.stage5.OSA5_1.concat.OSA5_1_concat/conv.weight, img_backbone.stage5.OSA5_1.concat.OSA5_1_concat/norm.weight, img_backbone.stage5.OSA5_1.concat.OSA5_1_concat/norm.bias, img_backbone.stage5.OSA5_1.concat.OSA5_1_concat/norm.running_mean, img_backbone.stage5.OSA5_1.concat.OSA5_1_concat/norm.running_var, img_backbone.stage5.OSA5_1.concat.OSA5_1_concat/norm.num_batches_tracked, img_backbone.stage5.OSA5_1.ese.fc.weight, img_backbone.stage5.OSA5_1.ese.fc.bias, img_backbone.stage5.OSA5_2.layers.0.OSA5_2_0/conv.weight, img_backbone.stage5.OSA5_2.layers.0.OSA5_2_0/norm.weight, img_backbone.stage5.OSA5_2.layers.0.OSA5_2_0/norm.bias, img_backbone.stage5.OSA5_2.layers.0.OSA5_2_0/norm.running_mean, img_backbone.stage5.OSA5_2.layers.0.OSA5_2_0/norm.running_var, img_backbone.stage5.OSA5_2.layers.0.OSA5_2_0/norm.num_batches_tracked, img_backbone.stage5.OSA5_2.layers.1.OSA5_2_1/conv.weight, img_backbone.stage5.OSA5_2.layers.1.OSA5_2_1/norm.weight, img_backbone.stage5.OSA5_2.layers.1.OSA5_2_1/norm.bias, img_backbone.stage5.OSA5_2.layers.1.OSA5_2_1/norm.running_mean, img_backbone.stage5.OSA5_2.layers.1.OSA5_2_1/norm.running_var, img_backbone.stage5.OSA5_2.layers.1.OSA5_2_1/norm.num_batches_tracked, img_backbone.stage5.OSA5_2.layers.2.OSA5_2_2/conv.weight, img_backbone.stage5.OSA5_2.layers.2.OSA5_2_2/norm.weight, img_backbone.stage5.OSA5_2.layers.2.OSA5_2_2/norm.bias, img_backbone.stage5.OSA5_2.layers.2.OSA5_2_2/norm.running_mean, img_backbone.stage5.OSA5_2.layers.2.OSA5_2_2/norm.running_var, img_backbone.stage5.OSA5_2.layers.2.OSA5_2_2/norm.num_batches_tracked, img_backbone.stage5.OSA5_2.layers.3.OSA5_2_3/conv.weight, img_backbone.stage5.OSA5_2.layers.3.OSA5_2_3/norm.weight, img_backbone.stage5.OSA5_2.layers.3.OSA5_2_3/norm.bias, img_backbone.stage5.OSA5_2.layers.3.OSA5_2_3/norm.running_mean, img_backbone.stage5.OSA5_2.layers.3.OSA5_2_3/norm.running_var, img_backbone.stage5.OSA5_2.layers.3.OSA5_2_3/norm.num_batches_tracked, img_backbone.stage5.OSA5_2.layers.4.OSA5_2_4/conv.weight, img_backbone.stage5.OSA5_2.layers.4.OSA5_2_4/norm.weight, img_backbone.stage5.OSA5_2.layers.4.OSA5_2_4/norm.bias, img_backbone.stage5.OSA5_2.layers.4.OSA5_2_4/norm.running_mean, img_backbone.stage5.OSA5_2.layers.4.OSA5_2_4/norm.running_var, img_backbone.stage5.OSA5_2.layers.4.OSA5_2_4/norm.num_batches_tracked, img_backbone.stage5.OSA5_2.concat.OSA5_2_concat/conv.weight, img_backbone.stage5.OSA5_2.concat.OSA5_2_concat/norm.weight, img_backbone.stage5.OSA5_2.concat.OSA5_2_concat/norm.bias, img_backbone.stage5.OSA5_2.concat.OSA5_2_concat/norm.running_mean, img_backbone.stage5.OSA5_2.concat.OSA5_2_concat/norm.running_var, img_backbone.stage5.OSA5_2.concat.OSA5_2_concat/norm.num_batches_tracked, img_backbone.stage5.OSA5_2.ese.fc.weight, img_backbone.stage5.OSA5_2.ese.fc.bias, img_backbone.stage5.OSA5_3.layers.0.OSA5_3_0/conv.weight, img_backbone.stage5.OSA5_3.layers.0.OSA5_3_0/norm.weight, img_backbone.stage5.OSA5_3.layers.0.OSA5_3_0/norm.bias, img_backbone.stage5.OSA5_3.layers.0.OSA5_3_0/norm.running_mean, img_backbone.stage5.OSA5_3.layers.0.OSA5_3_0/norm.running_var, img_backbone.stage5.OSA5_3.layers.0.OSA5_3_0/norm.num_batches_tracked, img_backbone.stage5.OSA5_3.layers.1.OSA5_3_1/conv.weight, img_backbone.stage5.OSA5_3.layers.1.OSA5_3_1/norm.weight, img_backbone.stage5.OSA5_3.layers.1.OSA5_3_1/norm.bias, img_backbone.stage5.OSA5_3.layers.1.OSA5_3_1/norm.running_mean, img_backbone.stage5.OSA5_3.layers.1.OSA5_3_1/norm.running_var, img_backbone.stage5.OSA5_3.layers.1.OSA5_3_1/norm.num_batches_tracked, img_backbone.stage5.OSA5_3.layers.2.OSA5_3_2/conv.weight, img_backbone.stage5.OSA5_3.layers.2.OSA5_3_2/norm.weight, img_backbone.stage5.OSA5_3.layers.2.OSA5_3_2/norm.bias, img_backbone.stage5.OSA5_3.layers.2.OSA5_3_2/norm.running_mean, img_backbone.stage5.OSA5_3.layers.2.OSA5_3_2/norm.running_var, img_backbone.stage5.OSA5_3.layers.2.OSA5_3_2/norm.num_batches_tracked, img_backbone.stage5.OSA5_3.layers.3.OSA5_3_3/conv.weight, img_backbone.stage5.OSA5_3.layers.3.OSA5_3_3/norm.weight, img_backbone.stage5.OSA5_3.layers.3.OSA5_3_3/norm.bias, img_backbone.stage5.OSA5_3.layers.3.OSA5_3_3/norm.running_mean, img_backbone.stage5.OSA5_3.layers.3.OSA5_3_3/norm.running_var, img_backbone.stage5.OSA5_3.layers.3.OSA5_3_3/norm.num_batches_tracked, img_backbone.stage5.OSA5_3.layers.4.OSA5_3_4/conv.weight, img_backbone.stage5.OSA5_3.layers.4.OSA5_3_4/norm.weight, img_backbone.stage5.OSA5_3.layers.4.OSA5_3_4/norm.bias, img_backbone.stage5.OSA5_3.layers.4.OSA5_3_4/norm.running_mean, img_backbone.stage5.OSA5_3.layers.4.OSA5_3_4/norm.running_var, img_backbone.stage5.OSA5_3.layers.4.OSA5_3_4/norm.num_batches_tracked, img_backbone.stage5.OSA5_3.concat.OSA5_3_concat/conv.weight, img_backbone.stage5.OSA5_3.concat.OSA5_3_concat/norm.weight, img_backbone.stage5.OSA5_3.concat.OSA5_3_concat/norm.bias, img_backbone.stage5.OSA5_3.concat.OSA5_3_concat/norm.running_mean, img_backbone.stage5.OSA5_3.concat.OSA5_3_concat/norm.running_var, img_backbone.stage5.OSA5_3.concat.OSA5_3_concat/norm.num_batches_tracked, img_backbone.stage5.OSA5_3.ese.fc.weight, img_backbone.stage5.OSA5_3.ese.fc.bias

missing keys in source state_dict: position_range, coords_d, pts_bbox_head.code_weights, pts_bbox_head.match_costs, pts_bbox_head.pc_range, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias, pts_bbox_head.reg_branches.0.0.weight, pts_bbox_head.reg_branches.0.0.bias, pts_bbox_head.reg_branches.0.2.weight, pts_bbox_head.reg_branches.0.2.bias, pts_bbox_head.reg_branches.0.4.weight, pts_bbox_head.reg_branches.0.4.bias, pts_bbox_head.reg_branches.1.0.weight, pts_bbox_head.reg_branches.1.0.bias, pts_bbox_head.reg_branches.1.2.weight, pts_bbox_head.reg_branches.1.2.bias, pts_bbox_head.reg_branches.1.4.weight, pts_bbox_head.reg_branches.1.4.bias, pts_bbox_head.reg_branches.2.0.weight, pts_bbox_head.reg_branches.2.0.bias, pts_bbox_head.reg_branches.2.2.weight, pts_bbox_head.reg_branches.2.2.bias, pts_bbox_head.reg_branches.2.4.weight, pts_bbox_head.reg_branches.2.4.bias, pts_bbox_head.reg_branches.3.0.weight, pts_bbox_head.reg_branches.3.0.bias, pts_bbox_head.reg_branches.3.2.weight, pts_bbox_head.reg_branches.3.2.bias, pts_bbox_head.reg_branches.3.4.weight, pts_bbox_head.reg_branches.3.4.bias, pts_bbox_head.reg_branches.4.0.weight, pts_bbox_head.reg_branches.4.0.bias, pts_bbox_head.reg_branches.4.2.weight, pts_bbox_head.reg_branches.4.2.bias, pts_bbox_head.reg_branches.4.4.weight, pts_bbox_head.reg_branches.4.4.bias, pts_bbox_head.reg_branches.5.0.weight, pts_bbox_head.reg_branches.5.0.bias, pts_bbox_head.reg_branches.5.2.weight, pts_bbox_head.reg_branches.5.2.bias, pts_bbox_head.reg_branches.5.4.weight, pts_bbox_head.reg_branches.5.4.bias, pts_bbox_head.input_projection.weight, pts_bbox_head.input_projection.bias, pts_bbox_head.output_projection.weight, pts_bbox_head.output_projection.bias, pts_bbox_head.reference_points.weight, pts_bbox_head.pseudo_reference_points.weight, pts_bbox_head.query_embedding.weight, pts_bbox_head.can_bus_embed.0.weight, pts_bbox_head.can_bus_embed.0.bias, pts_bbox_head.can_bus_embed.2.weight, pts_bbox_head.can_bus_embed.2.bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.1.weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.1.bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.5.weight, pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.5.bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.1.weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.1.bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.5.weight, pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.5.bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.1.weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.1.bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.5.weight, pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.5.bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.1.weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.1.bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.5.weight, pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.5.bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.1.weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.1.bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.5.weight, pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.5.bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.1.weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.1.bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.bias, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.5.weight, pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.5.bias, pts_bbox_head.query_pos.0.weight, pts_bbox_head.query_pos.0.bias, pts_bbox_head.query_pos.2.weight, pts_bbox_head.query_pos.2.bias, pts_bbox_head.time_embedding.0.weight, pts_bbox_head.time_embedding.0.bias, pts_bbox_head.time_embedding.1.weight, pts_bbox_head.time_embedding.1.bias, pts_bbox_head.ego_pose_pe.reduce.0.weight, pts_bbox_head.ego_pose_pe.reduce.0.bias, pts_bbox_head.ego_pose_pe.gamma.weight, pts_bbox_head.ego_pose_pe.gamma.bias, pts_bbox_head.ego_pose_pe.beta.weight, pts_bbox_head.ego_pose_pe.beta.bias, img_neck.lateral_convs.0.conv.weight, img_neck.lateral_convs.0.conv.bias, img_neck.fpn_convs.0.conv.weight, img_neck.fpn_convs.0.conv.bias, query_pos.0.weight, query_pos.0.bias, query_pos.2.weight, query_pos.2.bias, time_embedding.0.weight, time_embedding.0.bias, time_embedding.1.weight, time_embedding.1.bias, ego_pose_pe.reduce.0.weight, ego_pose_pe.reduce.0.bias, ego_pose_pe.gamma.weight, ego_pose_pe.gamma.bias, ego_pose_pe.beta.weight, ego_pose_pe.beta.bias, map_head.code_weights, map_head.match_costs, map_head.pc_range, map_head.cls_branches.0.0.weight, map_head.cls_branches.0.0.bias, map_head.cls_branches.0.1.weight, map_head.cls_branches.0.1.bias, map_head.cls_branches.0.3.weight, map_head.cls_branches.0.3.bias, map_head.cls_branches.0.4.weight, map_head.cls_branches.0.4.bias, map_head.cls_branches.0.6.weight, map_head.cls_branches.0.6.bias, map_head.cls_branches.1.0.weight, map_head.cls_branches.1.0.bias, map_head.cls_branches.1.1.weight, map_head.cls_branches.1.1.bias, map_head.cls_branches.1.3.weight, map_head.cls_branches.1.3.bias, map_head.cls_branches.1.4.weight, map_head.cls_branches.1.4.bias, map_head.cls_branches.1.6.weight, map_head.cls_branches.1.6.bias, map_head.cls_branches.2.0.weight, map_head.cls_branches.2.0.bias, map_head.cls_branches.2.1.weight, map_head.cls_branches.2.1.bias, map_head.cls_branches.2.3.weight, map_head.cls_branches.2.3.bias, map_head.cls_branches.2.4.weight, map_head.cls_branches.2.4.bias, map_head.cls_branches.2.6.weight, map_head.cls_branches.2.6.bias, map_head.cls_branches.3.0.weight, map_head.cls_branches.3.0.bias, map_head.cls_branches.3.1.weight, map_head.cls_branches.3.1.bias, map_head.cls_branches.3.3.weight, map_head.cls_branches.3.3.bias, map_head.cls_branches.3.4.weight, map_head.cls_branches.3.4.bias, map_head.cls_branches.3.6.weight, map_head.cls_branches.3.6.bias, map_head.cls_branches.4.0.weight, map_head.cls_branches.4.0.bias, map_head.cls_branches.4.1.weight, map_head.cls_branches.4.1.bias, map_head.cls_branches.4.3.weight, map_head.cls_branches.4.3.bias, map_head.cls_branches.4.4.weight, map_head.cls_branches.4.4.bias, map_head.cls_branches.4.6.weight, map_head.cls_branches.4.6.bias, map_head.cls_branches.5.0.weight, map_head.cls_branches.5.0.bias, map_head.cls_branches.5.1.weight, map_head.cls_branches.5.1.bias, map_head.cls_branches.5.3.weight, map_head.cls_branches.5.3.bias, map_head.cls_branches.5.4.weight, map_head.cls_branches.5.4.bias, map_head.cls_branches.5.6.weight, map_head.cls_branches.5.6.bias, map_head.reg_branches.0.0.weight, map_head.reg_branches.0.0.bias, map_head.reg_branches.0.2.weight, map_head.reg_branches.0.2.bias, map_head.reg_branches.0.4.weight, map_head.reg_branches.0.4.bias, map_head.reg_branches.1.0.weight, map_head.reg_branches.1.0.bias, map_head.reg_branches.1.2.weight, map_head.reg_branches.1.2.bias, map_head.reg_branches.1.4.weight, map_head.reg_branches.1.4.bias, map_head.reg_branches.2.0.weight, map_head.reg_branches.2.0.bias, map_head.reg_branches.2.2.weight, map_head.reg_branches.2.2.bias, map_head.reg_branches.2.4.weight, map_head.reg_branches.2.4.bias, map_head.reg_branches.3.0.weight, map_head.reg_branches.3.0.bias, map_head.reg_branches.3.2.weight, map_head.reg_branches.3.2.bias, map_head.reg_branches.3.4.weight, map_head.reg_branches.3.4.bias, map_head.reg_branches.4.0.weight, map_head.reg_branches.4.0.bias, map_head.reg_branches.4.2.weight, map_head.reg_branches.4.2.bias, map_head.reg_branches.4.4.weight, map_head.reg_branches.4.4.bias, map_head.reg_branches.5.0.weight, map_head.reg_branches.5.0.bias, map_head.reg_branches.5.2.weight, map_head.reg_branches.5.2.bias, map_head.reg_branches.5.4.weight, map_head.reg_branches.5.4.bias, map_head.input_projection.weight, map_head.input_projection.bias, map_head.output_projection.weight, map_head.output_projection.bias, map_head.reference_points_lane.weight, map_head.reference_points_lane.bias, map_head.points_embedding_lane.weight, map_head.instance_embedding_lane.weight, map_head.query_embedding.weight, map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_weight, map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_bias, map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.weight, map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.bias, map_head.transformer.query_decoder._layers.0.transformer_layers.1.weight, map_head.transformer.query_decoder._layers.0.transformer_layers.1.bias, map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_weight, map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_bias, map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.weight, map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.bias, map_head.transformer.query_decoder._layers.0.transformer_layers.3.weight, map_head.transformer.query_decoder._layers.0.transformer_layers.3.bias, map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.weight, map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.bias, map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.weight, map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.bias, map_head.transformer.query_decoder._layers.0.transformer_layers.5.weight, map_head.transformer.query_decoder._layers.0.transformer_layers.5.bias, map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_weight, map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_bias, map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.weight, map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.bias, map_head.transformer.query_decoder._layers.1.transformer_layers.1.weight, map_head.transformer.query_decoder._layers.1.transformer_layers.1.bias, map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_weight, map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_bias, map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.weight, map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.bias, map_head.transformer.query_decoder._layers.1.transformer_layers.3.weight, map_head.transformer.query_decoder._layers.1.transformer_layers.3.bias, map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.weight, map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.bias, map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.weight, map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.bias, map_head.transformer.query_decoder._layers.1.transformer_layers.5.weight, map_head.transformer.query_decoder._layers.1.transformer_layers.5.bias, map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_weight, map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_bias, map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.weight, map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.bias, map_head.transformer.query_decoder._layers.2.transformer_layers.1.weight, map_head.transformer.query_decoder._layers.2.transformer_layers.1.bias, map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_weight, map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_bias, map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.weight, map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.bias, map_head.transformer.query_decoder._layers.2.transformer_layers.3.weight, map_head.transformer.query_decoder._layers.2.transformer_layers.3.bias, map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.weight, map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.bias, map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.weight, map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.bias, map_head.transformer.query_decoder._layers.2.transformer_layers.5.weight, map_head.transformer.query_decoder._layers.2.transformer_layers.5.bias, map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_weight, map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_bias, map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.weight, map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.bias, map_head.transformer.query_decoder._layers.3.transformer_layers.1.weight, map_head.transformer.query_decoder._layers.3.transformer_layers.1.bias, map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_weight, map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_bias, map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.weight, map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.bias, map_head.transformer.query_decoder._layers.3.transformer_layers.3.weight, map_head.transformer.query_decoder._layers.3.transformer_layers.3.bias, map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.weight, map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.bias, map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.weight, map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.bias, map_head.transformer.query_decoder._layers.3.transformer_layers.5.weight, map_head.transformer.query_decoder._layers.3.transformer_layers.5.bias, map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_weight, map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_bias, map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.weight, map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.bias, map_head.transformer.query_decoder._layers.4.transformer_layers.1.weight, map_head.transformer.query_decoder._layers.4.transformer_layers.1.bias, map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_weight, map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_bias, map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.weight, map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.bias, map_head.transformer.query_decoder._layers.4.transformer_layers.3.weight, map_head.transformer.query_decoder._layers.4.transformer_layers.3.bias, map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.weight, map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.bias, map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.weight, map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.bias, map_head.transformer.query_decoder._layers.4.transformer_layers.5.weight, map_head.transformer.query_decoder._layers.4.transformer_layers.5.bias, map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_weight, map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_bias, map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.weight, map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.bias, map_head.transformer.query_decoder._layers.5.transformer_layers.1.weight, map_head.transformer.query_decoder._layers.5.transformer_layers.1.bias, map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_weight, map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_bias, map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.weight, map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.bias, map_head.transformer.query_decoder._layers.5.transformer_layers.3.weight, map_head.transformer.query_decoder._layers.5.transformer_layers.3.bias, map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.weight, map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.bias, map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.weight, map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.bias, map_head.transformer.query_decoder._layers.5.transformer_layers.5.weight, map_head.transformer.query_decoder._layers.5.transformer_layers.5.bias, map_head.query_pos.0.weight, map_head.query_pos.0.bias, map_head.query_pos.2.weight, map_head.query_pos.2.bias, map_head.time_embedding.0.weight, map_head.time_embedding.0.bias, map_head.time_embedding.1.weight, map_head.time_embedding.1.bias, map_head.ego_pose_pe.reduce.0.weight, map_head.ego_pose_pe.reduce.0.bias, map_head.ego_pose_pe.gamma.weight, map_head.ego_pose_pe.gamma.bias, map_head.ego_pose_pe.beta.weight, map_head.ego_pose_pe.beta.bias, position_encoder.0.weight, position_encoder.0.bias, position_encoder.2.weight, position_encoder.2.bias, lm_head.base_model.model.weighted_mask, lm_head.base_model.model.model.embed_tokens.weight, lm_head.base_model.model.model.layers.0.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.0.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.0.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.0.mlp.up_proj.weight, lm_head.base_model.model.model.layers.0.mlp.down_proj.weight, lm_head.base_model.model.model.layers.0.input_layernorm.weight, lm_head.base_model.model.model.layers.0.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.1.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.1.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.1.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.1.mlp.up_proj.weight, lm_head.base_model.model.model.layers.1.mlp.down_proj.weight, lm_head.base_model.model.model.layers.1.input_layernorm.weight, lm_head.base_model.model.model.layers.1.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.2.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.2.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.2.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.2.mlp.up_proj.weight, lm_head.base_model.model.model.layers.2.mlp.down_proj.weight, lm_head.base_model.model.model.layers.2.input_layernorm.weight, lm_head.base_model.model.model.layers.2.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.3.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.3.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.3.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.3.mlp.up_proj.weight, lm_head.base_model.model.model.layers.3.mlp.down_proj.weight, lm_head.base_model.model.model.layers.3.input_layernorm.weight, lm_head.base_model.model.model.layers.3.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.4.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.4.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.4.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.4.mlp.up_proj.weight, lm_head.base_model.model.model.layers.4.mlp.down_proj.weight, lm_head.base_model.model.model.layers.4.input_layernorm.weight, lm_head.base_model.model.model.layers.4.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.5.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.5.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.5.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.5.mlp.up_proj.weight, lm_head.base_model.model.model.layers.5.mlp.down_proj.weight, lm_head.base_model.model.model.layers.5.input_layernorm.weight, lm_head.base_model.model.model.layers.5.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.6.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.6.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.6.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.6.mlp.up_proj.weight, lm_head.base_model.model.model.layers.6.mlp.down_proj.weight, lm_head.base_model.model.model.layers.6.input_layernorm.weight, lm_head.base_model.model.model.layers.6.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.7.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.7.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.7.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.7.mlp.up_proj.weight, lm_head.base_model.model.model.layers.7.mlp.down_proj.weight, lm_head.base_model.model.model.layers.7.input_layernorm.weight, lm_head.base_model.model.model.layers.7.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.8.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.8.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.8.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.8.mlp.up_proj.weight, lm_head.base_model.model.model.layers.8.mlp.down_proj.weight, lm_head.base_model.model.model.layers.8.input_layernorm.weight, lm_head.base_model.model.model.layers.8.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.9.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.9.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.9.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.9.mlp.up_proj.weight, lm_head.base_model.model.model.layers.9.mlp.down_proj.weight, lm_head.base_model.model.model.layers.9.input_layernorm.weight, lm_head.base_model.model.model.layers.9.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.10.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.10.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.10.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.10.mlp.up_proj.weight, lm_head.base_model.model.model.layers.10.mlp.down_proj.weight, lm_head.base_model.model.model.layers.10.input_layernorm.weight, lm_head.base_model.model.model.layers.10.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.11.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.11.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.11.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.11.mlp.up_proj.weight, lm_head.base_model.model.model.layers.11.mlp.down_proj.weight, lm_head.base_model.model.model.layers.11.input_layernorm.weight, lm_head.base_model.model.model.layers.11.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.12.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.12.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.12.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.12.mlp.up_proj.weight, lm_head.base_model.model.model.layers.12.mlp.down_proj.weight, lm_head.base_model.model.model.layers.12.input_layernorm.weight, lm_head.base_model.model.model.layers.12.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.13.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.13.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.13.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.13.mlp.up_proj.weight, lm_head.base_model.model.model.layers.13.mlp.down_proj.weight, lm_head.base_model.model.model.layers.13.input_layernorm.weight, lm_head.base_model.model.model.layers.13.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.14.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.14.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.14.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.14.mlp.up_proj.weight, lm_head.base_model.model.model.layers.14.mlp.down_proj.weight, lm_head.base_model.model.model.layers.14.input_layernorm.weight, lm_head.base_model.model.model.layers.14.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.15.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.15.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.15.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.15.mlp.up_proj.weight, lm_head.base_model.model.model.layers.15.mlp.down_proj.weight, lm_head.base_model.model.model.layers.15.input_layernorm.weight, lm_head.base_model.model.model.layers.15.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.16.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.16.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.16.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.16.mlp.up_proj.weight, lm_head.base_model.model.model.layers.16.mlp.down_proj.weight, lm_head.base_model.model.model.layers.16.input_layernorm.weight, lm_head.base_model.model.model.layers.16.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.17.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.17.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.17.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.17.mlp.up_proj.weight, lm_head.base_model.model.model.layers.17.mlp.down_proj.weight, lm_head.base_model.model.model.layers.17.input_layernorm.weight, lm_head.base_model.model.model.layers.17.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.18.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.18.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.18.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.18.mlp.up_proj.weight, lm_head.base_model.model.model.layers.18.mlp.down_proj.weight, lm_head.base_model.model.model.layers.18.input_layernorm.weight, lm_head.base_model.model.model.layers.18.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.19.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.19.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.19.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.19.mlp.up_proj.weight, lm_head.base_model.model.model.layers.19.mlp.down_proj.weight, lm_head.base_model.model.model.layers.19.input_layernorm.weight, lm_head.base_model.model.model.layers.19.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.20.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.20.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.20.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.20.mlp.up_proj.weight, lm_head.base_model.model.model.layers.20.mlp.down_proj.weight, lm_head.base_model.model.model.layers.20.input_layernorm.weight, lm_head.base_model.model.model.layers.20.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.21.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.21.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.21.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.21.mlp.up_proj.weight, lm_head.base_model.model.model.layers.21.mlp.down_proj.weight, lm_head.base_model.model.model.layers.21.input_layernorm.weight, lm_head.base_model.model.model.layers.21.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.22.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.22.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.22.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.22.mlp.up_proj.weight, lm_head.base_model.model.model.layers.22.mlp.down_proj.weight, lm_head.base_model.model.model.layers.22.input_layernorm.weight, lm_head.base_model.model.model.layers.22.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.23.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.23.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.23.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.23.mlp.up_proj.weight, lm_head.base_model.model.model.layers.23.mlp.down_proj.weight, lm_head.base_model.model.model.layers.23.input_layernorm.weight, lm_head.base_model.model.model.layers.23.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.24.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.24.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.24.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.24.mlp.up_proj.weight, lm_head.base_model.model.model.layers.24.mlp.down_proj.weight, lm_head.base_model.model.model.layers.24.input_layernorm.weight, lm_head.base_model.model.model.layers.24.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.25.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.25.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.25.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.25.mlp.up_proj.weight, lm_head.base_model.model.model.layers.25.mlp.down_proj.weight, lm_head.base_model.model.model.layers.25.input_layernorm.weight, lm_head.base_model.model.model.layers.25.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.26.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.26.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.26.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.26.mlp.up_proj.weight, lm_head.base_model.model.model.layers.26.mlp.down_proj.weight, lm_head.base_model.model.model.layers.26.input_layernorm.weight, lm_head.base_model.model.model.layers.26.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.27.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.27.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.27.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.27.mlp.up_proj.weight, lm_head.base_model.model.model.layers.27.mlp.down_proj.weight, lm_head.base_model.model.model.layers.27.input_layernorm.weight, lm_head.base_model.model.model.layers.27.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.28.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.28.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.28.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.28.mlp.up_proj.weight, lm_head.base_model.model.model.layers.28.mlp.down_proj.weight, lm_head.base_model.model.model.layers.28.input_layernorm.weight, lm_head.base_model.model.model.layers.28.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.29.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.29.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.29.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.29.mlp.up_proj.weight, lm_head.base_model.model.model.layers.29.mlp.down_proj.weight, lm_head.base_model.model.model.layers.29.input_layernorm.weight, lm_head.base_model.model.model.layers.29.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.30.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.30.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.30.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.30.mlp.up_proj.weight, lm_head.base_model.model.model.layers.30.mlp.down_proj.weight, lm_head.base_model.model.model.layers.30.input_layernorm.weight, lm_head.base_model.model.model.layers.30.post_attention_layernorm.weight, lm_head.base_model.model.model.layers.31.self_attn.q_proj.weight, lm_head.base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight, lm_head.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight, lm_head.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight, lm_head.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight, lm_head.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight, lm_head.base_model.model.model.layers.31.self_attn.rotary_emb.inv_freq, lm_head.base_model.model.model.layers.31.mlp.gate_proj.weight, lm_head.base_model.model.model.layers.31.mlp.up_proj.weight, lm_head.base_model.model.model.layers.31.mlp.down_proj.weight, lm_head.base_model.model.model.layers.31.input_layernorm.weight, lm_head.base_model.model.model.layers.31.post_attention_layernorm.weight, lm_head.base_model.model.model.norm.weight, lm_head.base_model.model.lm_head.weight

2025-02-28 21:49:52,254 - mmdet - INFO - Start running, host: mist_sophia@DESKTOP-FV3EM2A, work_dir: /home/mist_sophia/ad/test/OmniDrive/work_dirs/mask_eva_lane_det_vlm
2025-02-28 21:49:52,254 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) CustomDistEvalHook                 
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CustomDistEvalHook                 
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CustomDistEvalHook                 
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) CustomDistEvalHook                 
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) CustomDistEvalHook                 
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-02-28 21:49:52,254 - mmdet - INFO - workflow: [('train', 1)], max: 168780 iters
2025-02-28 21:49:52,257 - mmdet - INFO - Checkpoints will be saved to /home/mist_sophia/ad/test/OmniDrive/work_dirs/mask_eva_lane_det_vlm by HardDiskBackend.
2025-02-28 21:52:29,522 - mmdet - INFO - Iter [50/168780]	lr: 1.595e-04, eta: 6 days, 3:17:17, time: 3.143, data_time: 0.027, memory: 22019, loss_cls: 1.6071, loss_bbox: 2.9472, d0.loss_cls: 1.6580, d0.loss_bbox: 2.9417, d1.loss_cls: 1.6320, d1.loss_bbox: 2.9544, d2.loss_cls: 1.6159, d2.loss_bbox: 2.9525, d3.loss_cls: 1.6372, d3.loss_bbox: 2.9352, d4.loss_cls: 1.6273, d4.loss_bbox: 2.9618, dn_loss_cls: 1.4124, dn_loss_bbox: 2.3522, d0.dn_loss_cls: 1.5137, d0.dn_loss_bbox: 2.3611, d1.dn_loss_cls: 1.4301, d1.dn_loss_bbox: 2.4185, d2.dn_loss_cls: 1.4097, d2.dn_loss_bbox: 2.4248, d3.dn_loss_cls: 1.4029, d3.dn_loss_bbox: 2.2877, d4.dn_loss_cls: 1.4055, d4.dn_loss_bbox: 2.3375, loss_cls_lane: 0.6133, loss_cls_H: 0.6129, loss_bbox_lane: 3.1792, loss_bbox_H: 3.1353, d0.loss_cls_lane: 0.6408, d0.loss_cls_H: 0.6474, d0.loss_bbox_lane: 3.5227, d0.loss_bbox_H: 3.4720, d1.loss_cls_lane: 0.6257, d1.loss_cls_H: 0.6270, d1.loss_bbox_lane: 3.3220, d1.loss_bbox_H: 3.2592, d2.loss_cls_lane: 0.6235, d2.loss_cls_H: 0.6186, d2.loss_bbox_lane: 3.2620, d2.loss_bbox_H: 3.2011, d3.loss_cls_lane: 0.6164, d3.loss_cls_H: 0.6096, d3.loss_bbox_lane: 3.1980, d3.loss_bbox_H: 3.1496, d4.loss_cls_lane: 0.6118, d4.loss_cls_H: 0.6086, d4.loss_bbox_lane: 3.1808, d4.loss_bbox_H: 3.1402, vlm_loss: 1.9832, loss: 98.6876, grad_norm: nan
2025-02-28 21:55:06,290 - mmdet - INFO - Iter [100/168780]	lr: 1.861e-04, eta: 6 days, 3:04:35, time: 3.135, data_time: 0.018, memory: 22027, loss_cls: 1.3361, loss_bbox: 2.9423, d0.loss_cls: 1.3292, d0.loss_bbox: 2.9381, d1.loss_cls: 1.3294, d1.loss_bbox: 2.9392, d2.loss_cls: 1.3137, d2.loss_bbox: 2.9141, d3.loss_cls: 1.3152, d3.loss_bbox: 2.9358, d4.loss_cls: 1.3207, d4.loss_bbox: 2.9259, dn_loss_cls: 1.1026, dn_loss_bbox: 2.1943, d0.dn_loss_cls: 1.0895, d0.dn_loss_bbox: 2.2003, d1.dn_loss_cls: 1.0931, d1.dn_loss_bbox: 2.1759, d2.dn_loss_cls: 1.0976, d2.dn_loss_bbox: 2.1740, d3.dn_loss_cls: 1.0941, d3.dn_loss_bbox: 2.1774, d4.dn_loss_cls: 1.0915, d4.dn_loss_bbox: 2.2014, loss_cls_lane: 0.5770, loss_cls_H: 0.5760, loss_bbox_lane: 2.3495, loss_bbox_H: 2.3195, d0.loss_cls_lane: 0.5759, d0.loss_cls_H: 0.5768, d0.loss_bbox_lane: 2.3749, d0.loss_bbox_H: 2.3588, d1.loss_cls_lane: 0.5790, d1.loss_cls_H: 0.5797, d1.loss_bbox_lane: 2.3611, d1.loss_bbox_H: 2.3439, d2.loss_cls_lane: 0.5832, d2.loss_cls_H: 0.5826, d2.loss_bbox_lane: 2.3338, d2.loss_bbox_H: 2.3183, d3.loss_cls_lane: 0.5809, d3.loss_cls_H: 0.5805, d3.loss_bbox_lane: 2.3426, d3.loss_bbox_H: 2.3303, d4.loss_cls_lane: 0.5812, d4.loss_cls_H: 0.5803, d4.loss_bbox_lane: 2.3626, d4.loss_bbox_H: 2.3425, vlm_loss: 1.0496, loss: 81.3716, grad_norm: 190.5621
2025-02-28 21:57:40,299 - mmdet - INFO - Iter [150/168780]	lr: 2.128e-04, eta: 6 days, 2:06:54, time: 3.080, data_time: 0.018, memory: 22027, loss_cls: 1.2960, loss_bbox: 2.5555, d0.loss_cls: 1.3062, d0.loss_bbox: 2.5465, d1.loss_cls: 1.3093, d1.loss_bbox: 2.5370, d2.loss_cls: 1.2968, d2.loss_bbox: 2.5328, d3.loss_cls: 1.3086, d3.loss_bbox: 2.5361, d4.loss_cls: 1.3001, d4.loss_bbox: 2.5565, dn_loss_cls: 0.9341, dn_loss_bbox: 1.7943, d0.dn_loss_cls: 0.9377, d0.dn_loss_bbox: 1.8122, d1.dn_loss_cls: 0.9393, d1.dn_loss_bbox: 1.7748, d2.dn_loss_cls: 0.9412, d2.dn_loss_bbox: 1.7866, d3.dn_loss_cls: 0.9411, d3.dn_loss_bbox: 1.7757, d4.dn_loss_cls: 0.9347, d4.dn_loss_bbox: 1.7846, loss_cls_lane: 0.5970, loss_cls_H: 0.5972, loss_bbox_lane: 2.0616, loss_bbox_H: 2.0542, d0.loss_cls_lane: 0.5943, d0.loss_cls_H: 0.5950, d0.loss_bbox_lane: 2.0724, d0.loss_bbox_H: 2.0736, d1.loss_cls_lane: 0.5989, d1.loss_cls_H: 0.5982, d1.loss_bbox_lane: 2.0637, d1.loss_bbox_H: 2.0632, d2.loss_cls_lane: 0.6005, d2.loss_cls_H: 0.6001, d2.loss_bbox_lane: 2.0427, d2.loss_bbox_H: 2.0450, d3.loss_cls_lane: 0.5964, d3.loss_cls_H: 0.5968, d3.loss_bbox_lane: 2.0559, d3.loss_bbox_H: 2.0614, d4.loss_cls_lane: 0.6007, d4.loss_cls_H: 0.6002, d4.loss_bbox_lane: 2.0866, d4.loss_bbox_H: 2.0691, vlm_loss: 0.9324, loss: 72.2945, grad_norm: 181.9487
2025-02-28 22:00:28,496 - mmdet - INFO - Iter [200/168780]	lr: 2.395e-04, eta: 6 days, 4:56:07, time: 3.364, data_time: 0.019, memory: 22027, loss_cls: 1.4322, loss_bbox: 2.6404, d0.loss_cls: 1.3991, d0.loss_bbox: 2.6309, d1.loss_cls: 1.4029, d1.loss_bbox: 2.6559, d2.loss_cls: 1.4197, d2.loss_bbox: 2.6265, d3.loss_cls: 1.3998, d3.loss_bbox: 2.6166, d4.loss_cls: 1.4200, d4.loss_bbox: 2.6266, dn_loss_cls: 1.0668, dn_loss_bbox: 2.0081, d0.dn_loss_cls: 1.0327, d0.dn_loss_bbox: 1.9714, d1.dn_loss_cls: 1.0533, d1.dn_loss_bbox: 1.9514, d2.dn_loss_cls: 1.0667, d2.dn_loss_bbox: 1.9506, d3.dn_loss_cls: 1.0638, d3.dn_loss_bbox: 1.9610, d4.dn_loss_cls: 1.0668, d4.dn_loss_bbox: 1.9575, loss_cls_lane: 0.5297, loss_cls_H: 0.5297, loss_bbox_lane: 2.0340, loss_bbox_H: 2.0559, d0.loss_cls_lane: 0.5235, d0.loss_cls_H: 0.5239, d0.loss_bbox_lane: 2.0305, d0.loss_bbox_H: 2.0670, d1.loss_cls_lane: 0.5255, d1.loss_cls_H: 0.5252, d1.loss_bbox_lane: 2.0340, d1.loss_bbox_H: 2.0568, d2.loss_cls_lane: 0.5272, d2.loss_cls_H: 0.5271, d2.loss_bbox_lane: 2.0407, d2.loss_bbox_H: 2.0565, d3.loss_cls_lane: 0.5257, d3.loss_cls_H: 0.5256, d3.loss_bbox_lane: 2.0596, d3.loss_bbox_H: 2.0749, d4.loss_cls_lane: 0.5259, d4.loss_cls_H: 0.5261, d4.loss_bbox_lane: 2.0375, d4.loss_bbox_H: 2.0572, vlm_loss: 0.8766, loss: 74.2174, grad_norm: 222.2742
2025-02-28 22:03:07,895 - mmdet - INFO - Iter [250/168780]	lr: 2.661e-04, eta: 6 days, 4:57:39, time: 3.188, data_time: 0.018, memory: 22027, loss_cls: 1.7157, loss_bbox: 2.9968, d0.loss_cls: 1.5317, d0.loss_bbox: 2.8951, d1.loss_cls: 1.5958, d1.loss_bbox: 3.0142, d2.loss_cls: 1.6605, d2.loss_bbox: 2.9997, d3.loss_cls: 1.6726, d3.loss_bbox: 2.9637, d4.loss_cls: 1.6804, d4.loss_bbox: 3.0004, dn_loss_cls: 1.0482, dn_loss_bbox: 2.2214, d0.dn_loss_cls: 0.9148, d0.dn_loss_bbox: 2.1652, d1.dn_loss_cls: 0.9897, d1.dn_loss_bbox: 2.2133, d2.dn_loss_cls: 1.0343, d2.dn_loss_bbox: 2.2218, d3.dn_loss_cls: 1.0835, d3.dn_loss_bbox: 2.3748, d4.dn_loss_cls: 1.0650, d4.dn_loss_bbox: 2.2000, loss_cls_lane: 0.5839, loss_cls_H: 0.5842, loss_bbox_lane: 2.0145, loss_bbox_H: 2.0420, d0.loss_cls_lane: 0.5835, d0.loss_cls_H: 0.5835, d0.loss_bbox_lane: 2.0105, d0.loss_bbox_H: 2.0303, d1.loss_cls_lane: 0.5820, d1.loss_cls_H: 0.5820, d1.loss_bbox_lane: 2.0579, d1.loss_bbox_H: 2.0632, d2.loss_cls_lane: 0.5866, d2.loss_cls_H: 0.5866, d2.loss_bbox_lane: 2.0125, d2.loss_bbox_H: 2.0302, d3.loss_cls_lane: 0.5865, d3.loss_cls_H: 0.5865, d3.loss_bbox_lane: 2.0280, d3.loss_bbox_H: 2.0391, d4.loss_cls_lane: 0.5852, d4.loss_cls_H: 0.5853, d4.loss_bbox_lane: 2.0379, d4.loss_bbox_H: 2.0497, vlm_loss: 0.8566, loss: 79.5470, grad_norm: inf
2025-02-28 22:06:00,158 - mmdet - INFO - Iter [300/168780]	lr: 2.928e-04, eta: 6 days, 6:58:12, time: 3.445, data_time: 0.018, memory: 22027, loss_cls: 1.2686, loss_bbox: 2.6288, d0.loss_cls: 1.2399, d0.loss_bbox: 2.5961, d1.loss_cls: 1.2255, d1.loss_bbox: 2.5467, d2.loss_cls: 1.2295, d2.loss_bbox: 2.6356, d3.loss_cls: 1.2401, d3.loss_bbox: 2.6239, d4.loss_cls: 1.2540, d4.loss_bbox: 2.6270, dn_loss_cls: 0.9544, dn_loss_bbox: 1.8096, d0.dn_loss_cls: 0.8582, d0.dn_loss_bbox: 1.8033, d1.dn_loss_cls: 0.8768, d1.dn_loss_bbox: 1.7966, d2.dn_loss_cls: 0.9022, d2.dn_loss_bbox: 1.7875, d3.dn_loss_cls: 0.9401, d3.dn_loss_bbox: 1.7937, d4.dn_loss_cls: 0.9417, d4.dn_loss_bbox: 1.7940, loss_cls_lane: 0.5466, loss_cls_H: 0.5468, loss_bbox_lane: 2.3105, loss_bbox_H: 2.3328, d0.loss_cls_lane: 0.5453, d0.loss_cls_H: 0.5456, d0.loss_bbox_lane: 2.2325, d0.loss_bbox_H: 2.2926, d1.loss_cls_lane: 0.5456, d1.loss_cls_H: 0.5457, d1.loss_bbox_lane: 2.2902, d1.loss_bbox_H: 2.3275, d2.loss_cls_lane: 0.5458, d2.loss_cls_H: 0.5459, d2.loss_bbox_lane: 2.2779, d2.loss_bbox_H: 2.3100, d3.loss_cls_lane: 0.5456, d3.loss_cls_H: 0.5455, d3.loss_bbox_lane: 2.3058, d3.loss_bbox_H: 2.3260, d4.loss_cls_lane: 0.5461, d4.loss_cls_H: 0.5462, d4.loss_bbox_lane: 2.2814, d4.loss_bbox_H: 2.3161, vlm_loss: 0.8354, loss: 74.3633, grad_norm: inf
2025-02-28 22:08:34,067 - mmdet - INFO - Iter [350/168780]	lr: 3.195e-04, eta: 6 days, 5:56:18, time: 3.078, data_time: 0.019, memory: 22027, loss_cls: 1.4091, loss_bbox: 2.5597, d0.loss_cls: 1.3423, d0.loss_bbox: 2.5811, d1.loss_cls: 1.3607, d1.loss_bbox: 2.6582, d2.loss_cls: 1.3716, d2.loss_bbox: 2.6732, d3.loss_cls: 1.3704, d3.loss_bbox: 2.6728, d4.loss_cls: 1.3768, d4.loss_bbox: 2.6573, dn_loss_cls: 0.9182, dn_loss_bbox: 1.9016, d0.dn_loss_cls: 0.8591, d0.dn_loss_bbox: 1.8688, d1.dn_loss_cls: 0.8542, d1.dn_loss_bbox: 1.8787, d2.dn_loss_cls: 0.8542, d2.dn_loss_bbox: 1.8841, d3.dn_loss_cls: 0.8626, d3.dn_loss_bbox: 1.8856, d4.dn_loss_cls: 0.8831, d4.dn_loss_bbox: 1.9146, loss_cls_lane: 0.5349, loss_cls_H: 0.5349, loss_bbox_lane: 2.1754, loss_bbox_H: 2.1677, d0.loss_cls_lane: 0.5351, d0.loss_cls_H: 0.5358, d0.loss_bbox_lane: 2.0638, d0.loss_bbox_H: 2.1347, d1.loss_cls_lane: 0.5379, d1.loss_cls_H: 0.5380, d1.loss_bbox_lane: 2.1600, d1.loss_bbox_H: 2.1787, d2.loss_cls_lane: 0.5363, d2.loss_cls_H: 0.5362, d2.loss_bbox_lane: 2.1665, d2.loss_bbox_H: 2.1776, d3.loss_cls_lane: 0.5356, d3.loss_cls_H: 0.5356, d3.loss_bbox_lane: 2.1754, d3.loss_bbox_H: 2.1810, d4.loss_cls_lane: 0.5354, d4.loss_cls_H: 0.5355, d4.loss_bbox_lane: 2.1789, d4.loss_bbox_H: 2.1865, vlm_loss: 0.8853, loss: 73.8609, grad_norm: 180.1493
2025-02-28 22:11:10,056 - mmdet - INFO - Iter [400/168780]	lr: 3.461e-04, eta: 6 days, 5:23:49, time: 3.120, data_time: 0.017, memory: 22027, loss_cls: 1.7456, loss_bbox: 2.6407, d0.loss_cls: 1.7005, d0.loss_bbox: 2.5631, d1.loss_cls: 1.6929, d1.loss_bbox: 2.5333, d2.loss_cls: 1.7072, d2.loss_bbox: 2.5008, d3.loss_cls: 1.7316, d3.loss_bbox: 2.5844, d4.loss_cls: 1.7563, d4.loss_bbox: 2.5347, dn_loss_cls: 0.9521, dn_loss_bbox: 1.8482, d0.dn_loss_cls: 0.9058, d0.dn_loss_bbox: 1.8013, d1.dn_loss_cls: 0.9122, d1.dn_loss_bbox: 1.8032, d2.dn_loss_cls: 0.9099, d2.dn_loss_bbox: 1.8046, d3.dn_loss_cls: 0.9127, d3.dn_loss_bbox: 1.8108, d4.dn_loss_cls: 0.9260, d4.dn_loss_bbox: 1.8200, loss_cls_lane: 0.5696, loss_cls_H: 0.5699, loss_bbox_lane: 1.9791, loss_bbox_H: 1.9812, d0.loss_cls_lane: 0.5670, d0.loss_cls_H: 0.5682, d0.loss_bbox_lane: 1.8637, d0.loss_bbox_H: 1.9166, d1.loss_cls_lane: 0.5699, d1.loss_cls_H: 0.5700, d1.loss_bbox_lane: 1.9016, d1.loss_bbox_H: 1.9755, d2.loss_cls_lane: 0.5701, d2.loss_cls_H: 0.5699, d2.loss_bbox_lane: 1.9853, d2.loss_bbox_H: 2.0081, d3.loss_cls_lane: 0.5692, d3.loss_cls_H: 0.5695, d3.loss_bbox_lane: 1.9994, d3.loss_bbox_H: 2.0066, d4.loss_cls_lane: 0.5698, d4.loss_cls_H: 0.5698, d4.loss_bbox_lane: 1.9867, d4.loss_bbox_H: 2.0017, vlm_loss: 0.8190, loss: 73.3553, grad_norm: 219.7611
2025-02-28 22:13:43,127 - mmdet - INFO - Iter [450/168780]	lr: 3.728e-04, eta: 6 days, 4:39:46, time: 3.061, data_time: 0.019, memory: 22027, loss_cls: 1.3927, loss_bbox: 2.5294, d0.loss_cls: 1.3522, d0.loss_bbox: 2.4884, d1.loss_cls: 1.3687, d1.loss_bbox: 2.5144, d2.loss_cls: 1.3634, d2.loss_bbox: 2.4920, d3.loss_cls: 1.4076, d3.loss_bbox: 2.5172, d4.loss_cls: 1.4362, d4.loss_bbox: 2.5725, dn_loss_cls: 0.9096, dn_loss_bbox: 2.0592, d0.dn_loss_cls: 0.8885, d0.dn_loss_bbox: 1.8987, d1.dn_loss_cls: 0.8897, d1.dn_loss_bbox: 1.9077, d2.dn_loss_cls: 0.8972, d2.dn_loss_bbox: 1.9776, d3.dn_loss_cls: 0.8978, d3.dn_loss_bbox: 1.9205, d4.dn_loss_cls: 0.9025, d4.dn_loss_bbox: 1.9389, loss_cls_lane: 0.5459, loss_cls_H: 0.5459, loss_bbox_lane: 1.8187, loss_bbox_H: 1.8263, d0.loss_cls_lane: 0.5411, d0.loss_cls_H: 0.5417, d0.loss_bbox_lane: 1.7328, d0.loss_bbox_H: 1.7414, d1.loss_cls_lane: 0.5437, d1.loss_cls_H: 0.5439, d1.loss_bbox_lane: 1.7684, d1.loss_bbox_H: 1.7856, d2.loss_cls_lane: 0.5445, d2.loss_cls_H: 0.5447, d2.loss_bbox_lane: 1.8197, d2.loss_bbox_H: 1.8418, d3.loss_cls_lane: 0.5436, d3.loss_cls_H: 0.5435, d3.loss_bbox_lane: 1.8414, d3.loss_bbox_H: 1.8538, d4.loss_cls_lane: 0.5443, d4.loss_cls_H: 0.5442, d4.loss_bbox_lane: 1.8323, d4.loss_bbox_H: 1.8499, vlm_loss: 0.7896, loss: 69.5508, grad_norm: inf
2025-02-28 22:16:14,684 - mmdet - INFO - Iter [500/168780]	lr: 3.995e-04, eta: 6 days, 3:55:32, time: 3.031, data_time: 0.018, memory: 22027, loss_cls: 1.4279, loss_bbox: 2.6641, d0.loss_cls: 1.4128, d0.loss_bbox: 2.5887, d1.loss_cls: 1.4081, d1.loss_bbox: 2.5899, d2.loss_cls: 1.4029, d2.loss_bbox: 2.5721, d3.loss_cls: 1.4316, d3.loss_bbox: 2.5807, d4.loss_cls: 1.4174, d4.loss_bbox: 2.6193, dn_loss_cls: 0.8995, dn_loss_bbox: 1.9453, d0.dn_loss_cls: 0.8881, d0.dn_loss_bbox: 1.9078, d1.dn_loss_cls: 0.8885, d1.dn_loss_bbox: 1.9088, d2.dn_loss_cls: 0.8934, d2.dn_loss_bbox: 1.9234, d3.dn_loss_cls: 0.8979, d3.dn_loss_bbox: 1.9183, d4.dn_loss_cls: 0.9039, d4.dn_loss_bbox: 1.9249, loss_cls_lane: 0.6365, loss_cls_H: 0.6366, loss_bbox_lane: 2.0448, loss_bbox_H: 2.0395, d0.loss_cls_lane: 0.6385, d0.loss_cls_H: 0.6427, d0.loss_bbox_lane: 1.9950, d0.loss_bbox_H: 1.9983, d1.loss_cls_lane: 0.6335, d1.loss_cls_H: 0.6345, d1.loss_bbox_lane: 1.9865, d1.loss_bbox_H: 2.0079, d2.loss_cls_lane: 0.6340, d2.loss_cls_H: 0.6345, d2.loss_bbox_lane: 2.0247, d2.loss_bbox_H: 2.0317, d3.loss_cls_lane: 0.6350, d3.loss_cls_H: 0.6350, d3.loss_bbox_lane: 2.1098, d3.loss_bbox_H: 2.1088, d4.loss_cls_lane: 0.6358, d4.loss_cls_H: 0.6357, d4.loss_bbox_lane: 2.0785, d4.loss_bbox_H: 2.0744, vlm_loss: 0.7595, loss: 73.9068, grad_norm: nan
2025-02-28 22:18:41,790 - mmdet - INFO - Iter [550/168780]	lr: 4.000e-04, eta: 6 days, 2:56:12, time: 2.942, data_time: 0.018, memory: 22027, loss_cls: 1.5011, loss_bbox: 2.7796, d0.loss_cls: 1.5011, d0.loss_bbox: 2.7796, d1.loss_cls: 1.5011, d1.loss_bbox: 2.7796, d2.loss_cls: 1.5011, d2.loss_bbox: 2.7796, d3.loss_cls: 1.5011, d3.loss_bbox: 2.7796, d4.loss_cls: 1.5011, d4.loss_bbox: 2.7796, dn_loss_cls: 0.9297, dn_loss_bbox: 2.4588, d0.dn_loss_cls: 0.9297, d0.dn_loss_bbox: 2.4588, d1.dn_loss_cls: 0.9297, d1.dn_loss_bbox: 2.4588, d2.dn_loss_cls: 0.9297, d2.dn_loss_bbox: 2.4588, d3.dn_loss_cls: 0.9297, d3.dn_loss_bbox: 2.4588, d4.dn_loss_cls: 0.9297, d4.dn_loss_bbox: 2.4588, loss_cls_lane: 0.6393, loss_cls_H: 0.7467, loss_bbox_lane: 5.9545, loss_bbox_H: 5.9569, d0.loss_cls_lane: 0.6393, d0.loss_cls_H: 0.7467, d0.loss_bbox_lane: 5.9545, d0.loss_bbox_H: 5.9569, d1.loss_cls_lane: 0.6393, d1.loss_cls_H: 0.7467, d1.loss_bbox_lane: 5.9545, d1.loss_bbox_H: 5.9569, d2.loss_cls_lane: 0.6393, d2.loss_cls_H: 0.7467, d2.loss_bbox_lane: 5.9545, d2.loss_bbox_H: 5.9569, d3.loss_cls_lane: 0.6393, d3.loss_cls_H: 0.7467, d3.loss_bbox_lane: 5.9545, d3.loss_bbox_H: 5.9569, d4.loss_cls_lane: 0.6393, d4.loss_cls_H: 0.7467, d4.loss_bbox_lane: 5.9545, d4.loss_bbox_H: 5.9569, vlm_loss: 0.0000, loss: 125.7998, grad_norm: nan
2025-02-28 22:21:09,836 - mmdet - INFO - Iter [600/168780]	lr: 4.000e-04, eta: 6 days, 2:10:44, time: 2.961, data_time: 0.018, memory: 22027, loss_cls: 2.0183, loss_bbox: 2.6138, d0.loss_cls: 2.0183, d0.loss_bbox: 2.6138, d1.loss_cls: 2.0183, d1.loss_bbox: 2.6138, d2.loss_cls: 2.0183, d2.loss_bbox: 2.6138, d3.loss_cls: 2.0183, d3.loss_bbox: 2.6138, d4.loss_cls: 2.0183, d4.loss_bbox: 2.6138, dn_loss_cls: 0.8445, dn_loss_bbox: 2.2554, d0.dn_loss_cls: 0.8445, d0.dn_loss_bbox: 2.2554, d1.dn_loss_cls: 0.8445, d1.dn_loss_bbox: 2.2554, d2.dn_loss_cls: 0.8445, d2.dn_loss_bbox: 2.2554, d3.dn_loss_cls: 0.8445, d3.dn_loss_bbox: 2.2554, d4.dn_loss_cls: 0.8445, d4.dn_loss_bbox: 2.2554, loss_cls_lane: 0.6346, loss_cls_H: 0.6346, loss_bbox_lane: 6.5892, loss_bbox_H: 6.5852, d0.loss_cls_lane: 0.6346, d0.loss_cls_H: 0.6346, d0.loss_bbox_lane: 6.5892, d0.loss_bbox_H: 6.5852, d1.loss_cls_lane: 0.6346, d1.loss_cls_H: 0.6346, d1.loss_bbox_lane: 6.5892, d1.loss_bbox_H: 6.5852, d2.loss_cls_lane: 0.6346, d2.loss_cls_H: 0.6346, d2.loss_bbox_lane: 6.5892, d2.loss_bbox_H: 6.5852, d3.loss_cls_lane: 0.6346, d3.loss_cls_H: 0.6346, d3.loss_bbox_lane: 6.5892, d3.loss_bbox_H: 6.5852, d4.loss_cls_lane: 0.6346, d4.loss_cls_H: 0.6346, d4.loss_bbox_lane: 6.5892, d4.loss_bbox_H: 6.5852, vlm_loss: 0.0000, loss: 133.0536, grad_norm: nan
2025-02-28 22:23:38,686 - mmdet - INFO - Iter [650/168780]	lr: 4.000e-04, eta: 6 days, 1:35:21, time: 2.977, data_time: 0.018, memory: 22027, loss_cls: 1.6554, loss_bbox: 2.8343, d0.loss_cls: 1.6554, d0.loss_bbox: 2.8343, d1.loss_cls: 1.6554, d1.loss_bbox: 2.8343, d2.loss_cls: 1.6554, d2.loss_bbox: 2.8343, d3.loss_cls: 1.6554, d3.loss_bbox: 2.8343, d4.loss_cls: 1.6554, d4.loss_bbox: 2.8343, dn_loss_cls: 0.8760, dn_loss_bbox: 2.4842, d0.dn_loss_cls: 0.8760, d0.dn_loss_bbox: 2.4842, d1.dn_loss_cls: 0.8760, d1.dn_loss_bbox: 2.4842, d2.dn_loss_cls: 0.8760, d2.dn_loss_bbox: 2.4842, d3.dn_loss_cls: 0.8760, d3.dn_loss_bbox: 2.4842, d4.dn_loss_cls: 0.8760, d4.dn_loss_bbox: 2.4842, loss_cls_lane: 0.6292, loss_cls_H: 0.6292, loss_bbox_lane: 6.6256, loss_bbox_H: 6.6194, d0.loss_cls_lane: 0.6292, d0.loss_cls_H: 0.6292, d0.loss_bbox_lane: 6.6256, d0.loss_bbox_H: 6.6194, d1.loss_cls_lane: 0.6292, d1.loss_cls_H: 0.6292, d1.loss_bbox_lane: 6.6256, d1.loss_bbox_H: 6.6194, d2.loss_cls_lane: 0.6292, d2.loss_cls_H: 0.6292, d2.loss_bbox_lane: 6.6256, d2.loss_bbox_H: 6.6194, d3.loss_cls_lane: 0.6292, d3.loss_cls_H: 0.6292, d3.loss_bbox_lane: 6.6256, d3.loss_bbox_H: 6.6194, d4.loss_cls_lane: 0.6292, d4.loss_cls_H: 0.6292, d4.loss_bbox_lane: 6.6256, d4.loss_bbox_H: 6.6194, vlm_loss: 0.0000, loss: 134.1204, grad_norm: nan
2025-02-28 22:26:08,232 - mmdet - INFO - Iter [700/168780]	lr: 4.000e-04, eta: 6 days, 1:07:27, time: 2.991, data_time: 0.018, memory: 22027, loss_cls: 1.2531, loss_bbox: 2.6627, d0.loss_cls: 1.2531, d0.loss_bbox: 2.6627, d1.loss_cls: 1.2531, d1.loss_bbox: 2.6627, d2.loss_cls: 1.2531, d2.loss_bbox: 2.6627, d3.loss_cls: 1.2531, d3.loss_bbox: 2.6627, d4.loss_cls: 1.2531, d4.loss_bbox: 2.6627, dn_loss_cls: 0.9299, dn_loss_bbox: 2.3095, d0.dn_loss_cls: 0.9299, d0.dn_loss_bbox: 2.3095, d1.dn_loss_cls: 0.9299, d1.dn_loss_bbox: 2.3095, d2.dn_loss_cls: 0.9299, d2.dn_loss_bbox: 2.3095, d3.dn_loss_cls: 0.9299, d3.dn_loss_bbox: 2.3095, d4.dn_loss_cls: 0.9299, d4.dn_loss_bbox: 2.3095, loss_cls_lane: 0.6354, loss_cls_H: 0.6354, loss_bbox_lane: 6.3954, loss_bbox_H: 6.3995, d0.loss_cls_lane: 0.6354, d0.loss_cls_H: 0.6354, d0.loss_bbox_lane: 6.3954, d0.loss_bbox_H: 6.3995, d1.loss_cls_lane: 0.6354, d1.loss_cls_H: 0.6354, d1.loss_bbox_lane: 6.3954, d1.loss_bbox_H: 6.3995, d2.loss_cls_lane: 0.6354, d2.loss_cls_H: 0.6354, d2.loss_bbox_lane: 6.3954, d2.loss_bbox_H: 6.3995, d3.loss_cls_lane: 0.6354, d3.loss_cls_H: 0.6354, d3.loss_bbox_lane: 6.3954, d3.loss_bbox_H: 6.3995, d4.loss_cls_lane: 0.6354, d4.loss_cls_H: 0.6354, d4.loss_bbox_lane: 6.3954, d4.loss_bbox_H: 6.3995, vlm_loss: 0.0000, loss: 127.3257, grad_norm: nan
2025-02-28 22:28:35,958 - mmdet - INFO - Iter [750/168780]	lr: 4.000e-04, eta: 6 days, 0:36:09, time: 2.955, data_time: 0.018, memory: 22027, loss_cls: 1.4695, loss_bbox: 2.8429, d0.loss_cls: 1.4695, d0.loss_bbox: 2.8429, d1.loss_cls: 1.4695, d1.loss_bbox: 2.8429, d2.loss_cls: 1.4695, d2.loss_bbox: 2.8429, d3.loss_cls: 1.4695, d3.loss_bbox: 2.8429, d4.loss_cls: 1.4695, d4.loss_bbox: 2.8429, dn_loss_cls: 0.9491, dn_loss_bbox: 2.4605, d0.dn_loss_cls: 0.9491, d0.dn_loss_bbox: 2.4605, d1.dn_loss_cls: 0.9491, d1.dn_loss_bbox: 2.4605, d2.dn_loss_cls: 0.9491, d2.dn_loss_bbox: 2.4605, d3.dn_loss_cls: 0.9491, d3.dn_loss_bbox: 2.4605, d4.dn_loss_cls: 0.9491, d4.dn_loss_bbox: 2.4605, loss_cls_lane: 0.6385, loss_cls_H: 0.6385, loss_bbox_lane: 6.6062, loss_bbox_H: 6.6190, d0.loss_cls_lane: 0.6385, d0.loss_cls_H: 0.6385, d0.loss_bbox_lane: 6.6062, d0.loss_bbox_H: 6.6190, d1.loss_cls_lane: 0.6385, d1.loss_cls_H: 0.6385, d1.loss_bbox_lane: 6.6062, d1.loss_bbox_H: 6.6190, d2.loss_cls_lane: 0.6385, d2.loss_cls_H: 0.6385, d2.loss_bbox_lane: 6.6062, d2.loss_bbox_H: 6.6190, d3.loss_cls_lane: 0.6385, d3.loss_cls_H: 0.6385, d3.loss_bbox_lane: 6.6062, d3.loss_bbox_H: 6.6190, d4.loss_cls_lane: 0.6385, d4.loss_cls_H: 0.6385, d4.loss_bbox_lane: 6.6062, d4.loss_bbox_H: 6.6190, vlm_loss: 0.0000, loss: 133.3449, grad_norm: nan
2025-02-28 22:31:05,426 - mmdet - INFO - Iter [800/168780]	lr: 4.000e-04, eta: 6 days, 0:14:33, time: 2.989, data_time: 0.018, memory: 22027, loss_cls: 1.4899, loss_bbox: 2.8547, d0.loss_cls: 1.4899, d0.loss_bbox: 2.8547, d1.loss_cls: 1.4899, d1.loss_bbox: 2.8547, d2.loss_cls: 1.4899, d2.loss_bbox: 2.8547, d3.loss_cls: 1.4899, d3.loss_bbox: 2.8547, d4.loss_cls: 1.4899, d4.loss_bbox: 2.8547, dn_loss_cls: 0.9055, dn_loss_bbox: 2.6393, d0.dn_loss_cls: 0.9055, d0.dn_loss_bbox: 2.6393, d1.dn_loss_cls: 0.9055, d1.dn_loss_bbox: 2.6393, d2.dn_loss_cls: 0.9055, d2.dn_loss_bbox: 2.6393, d3.dn_loss_cls: 0.9055, d3.dn_loss_bbox: 2.6393, d4.dn_loss_cls: 0.9055, d4.dn_loss_bbox: 2.6393, loss_cls_lane: 0.6272, loss_cls_H: 0.6272, loss_bbox_lane: 6.5070, loss_bbox_H: 6.5159, d0.loss_cls_lane: 0.6272, d0.loss_cls_H: 0.6272, d0.loss_bbox_lane: 6.5070, d0.loss_bbox_H: 6.5159, d1.loss_cls_lane: 0.6272, d1.loss_cls_H: 0.6272, d1.loss_bbox_lane: 6.5070, d1.loss_bbox_H: 6.5159, d2.loss_cls_lane: 0.6272, d2.loss_cls_H: 0.6272, d2.loss_bbox_lane: 6.5070, d2.loss_bbox_H: 6.5159, d3.loss_cls_lane: 0.6272, d3.loss_cls_H: 0.6272, d3.loss_bbox_lane: 6.5070, d3.loss_bbox_H: 6.5159, d4.loss_cls_lane: 0.6272, d4.loss_cls_H: 0.6272, d4.loss_bbox_lane: 6.5070, d4.loss_bbox_H: 6.5159, vlm_loss: 0.0000, loss: 132.9994, grad_norm: nan
2025-02-28 22:33:57,014 - mmdet - INFO - Iter [850/168780]	lr: 4.000e-04, eta: 6 days, 1:08:01, time: 3.432, data_time: 0.018, memory: 22027, loss_cls: 1.3728, loss_bbox: 2.9438, d0.loss_cls: 1.3728, d0.loss_bbox: 2.9438, d1.loss_cls: 1.3728, d1.loss_bbox: 2.9438, d2.loss_cls: 1.3728, d2.loss_bbox: 2.9438, d3.loss_cls: 1.3728, d3.loss_bbox: 2.9438, d4.loss_cls: 1.3728, d4.loss_bbox: 2.9438, dn_loss_cls: 0.9211, dn_loss_bbox: 2.5749, d0.dn_loss_cls: 0.9211, d0.dn_loss_bbox: 2.5749, d1.dn_loss_cls: 0.9211, d1.dn_loss_bbox: 2.5749, d2.dn_loss_cls: 0.9211, d2.dn_loss_bbox: 2.5749, d3.dn_loss_cls: 0.9211, d3.dn_loss_bbox: 2.5749, d4.dn_loss_cls: 0.9211, d4.dn_loss_bbox: 2.5749, loss_cls_lane: 0.6339, loss_cls_H: 0.6339, loss_bbox_lane: 6.6388, loss_bbox_H: 6.6359, d0.loss_cls_lane: 0.6339, d0.loss_cls_H: 0.6339, d0.loss_bbox_lane: 6.6388, d0.loss_bbox_H: 6.6359, d1.loss_cls_lane: 0.6339, d1.loss_cls_H: 0.6339, d1.loss_bbox_lane: 6.6388, d1.loss_bbox_H: 6.6359, d2.loss_cls_lane: 0.6339, d2.loss_cls_H: 0.6339, d2.loss_bbox_lane: 6.6388, d2.loss_bbox_H: 6.6359, d3.loss_cls_lane: 0.6339, d3.loss_cls_H: 0.6339, d3.loss_bbox_lane: 6.6388, d3.loss_bbox_H: 6.6359, d4.loss_cls_lane: 0.6339, d4.loss_cls_H: 0.6339, d4.loss_bbox_lane: 6.6388, d4.loss_bbox_H: 6.6359, vlm_loss: 0.0000, loss: 134.1308, grad_norm: nan
2025-02-28 22:36:25,446 - mmdet - INFO - Iter [900/168780]	lr: 4.000e-04, eta: 6 days, 0:43:15, time: 2.969, data_time: 0.018, memory: 22027, loss_cls: 1.4711, loss_bbox: 2.8216, d0.loss_cls: 1.4711, d0.loss_bbox: 2.8216, d1.loss_cls: 1.4711, d1.loss_bbox: 2.8216, d2.loss_cls: 1.4711, d2.loss_bbox: 2.8216, d3.loss_cls: 1.4711, d3.loss_bbox: 2.8216, d4.loss_cls: 1.4711, d4.loss_bbox: 2.8216, dn_loss_cls: 0.9234, dn_loss_bbox: 2.5448, d0.dn_loss_cls: 0.9234, d0.dn_loss_bbox: 2.5448, d1.dn_loss_cls: 0.9234, d1.dn_loss_bbox: 2.5448, d2.dn_loss_cls: 0.9234, d2.dn_loss_bbox: 2.5448, d3.dn_loss_cls: 0.9234, d3.dn_loss_bbox: 2.5448, d4.dn_loss_cls: 0.9234, d4.dn_loss_bbox: 2.5448, loss_cls_lane: 0.6331, loss_cls_H: 0.6331, loss_bbox_lane: 6.3680, loss_bbox_H: 6.3854, d0.loss_cls_lane: 0.6331, d0.loss_cls_H: 0.6331, d0.loss_bbox_lane: 6.3680, d0.loss_bbox_H: 6.3854, d1.loss_cls_lane: 0.6331, d1.loss_cls_H: 0.6331, d1.loss_bbox_lane: 6.3680, d1.loss_bbox_H: 6.3854, d2.loss_cls_lane: 0.6331, d2.loss_cls_H: 0.6331, d2.loss_bbox_lane: 6.3680, d2.loss_bbox_H: 6.3854, d3.loss_cls_lane: 0.6331, d3.loss_cls_H: 0.6331, d3.loss_bbox_lane: 6.3680, d3.loss_bbox_H: 6.3854, d4.loss_cls_lane: 0.6331, d4.loss_cls_H: 0.6331, d4.loss_bbox_lane: 6.3680, d4.loss_bbox_H: 6.3854, vlm_loss: 0.0000, loss: 130.6834, grad_norm: nan
2025-02-28 22:38:53,741 - mmdet - INFO - Iter [950/168780]	lr: 4.000e-04, eta: 6 days, 0:20:26, time: 2.966, data_time: 0.017, memory: 22027, loss_cls: 1.6616, loss_bbox: 2.7600, d0.loss_cls: 1.6616, d0.loss_bbox: 2.7600, d1.loss_cls: 1.6616, d1.loss_bbox: 2.7600, d2.loss_cls: 1.6616, d2.loss_bbox: 2.7600, d3.loss_cls: 1.6616, d3.loss_bbox: 2.7600, d4.loss_cls: 1.6616, d4.loss_bbox: 2.7600, dn_loss_cls: 1.0206, dn_loss_bbox: 2.4946, d0.dn_loss_cls: 1.0206, d0.dn_loss_bbox: 2.4946, d1.dn_loss_cls: 1.0206, d1.dn_loss_bbox: 2.4946, d2.dn_loss_cls: 1.0206, d2.dn_loss_bbox: 2.4946, d3.dn_loss_cls: 1.0206, d3.dn_loss_bbox: 2.4946, d4.dn_loss_cls: 1.0206, d4.dn_loss_bbox: 2.4946, loss_cls_lane: 0.6372, loss_cls_H: 0.6372, loss_bbox_lane: 6.5456, loss_bbox_H: 6.5428, d0.loss_cls_lane: 0.6372, d0.loss_cls_H: 0.6372, d0.loss_bbox_lane: 6.5456, d0.loss_bbox_H: 6.5428, d1.loss_cls_lane: 0.6372, d1.loss_cls_H: 0.6372, d1.loss_bbox_lane: 6.5456, d1.loss_bbox_H: 6.5428, d2.loss_cls_lane: 0.6372, d2.loss_cls_H: 0.6372, d2.loss_bbox_lane: 6.5456, d2.loss_bbox_H: 6.5428, d3.loss_cls_lane: 0.6372, d3.loss_cls_H: 0.6372, d3.loss_bbox_lane: 6.5456, d3.loss_bbox_H: 6.5428, d4.loss_cls_lane: 0.6372, d4.loss_cls_H: 0.6372, d4.loss_bbox_lane: 6.5456, d4.loss_bbox_H: 6.5428, vlm_loss: 0.0000, loss: 133.7972, grad_norm: nan
2025-02-28 22:41:37,449 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-02-28 22:41:37,450 - mmdet - INFO - Iter [1000/168780]	lr: 4.000e-04, eta: 6 days, 0:42:44, time: 3.274, data_time: 0.017, memory: 22027, loss_cls: 2.0870, loss_bbox: 2.8922, d0.loss_cls: 2.0870, d0.loss_bbox: 2.8922, d1.loss_cls: 2.0870, d1.loss_bbox: 2.8922, d2.loss_cls: 2.0870, d2.loss_bbox: 2.8922, d3.loss_cls: 2.0870, d3.loss_bbox: 2.8922, d4.loss_cls: 2.0870, d4.loss_bbox: 2.8922, dn_loss_cls: 1.1245, dn_loss_bbox: 2.5190, d0.dn_loss_cls: 1.1245, d0.dn_loss_bbox: 2.5190, d1.dn_loss_cls: 1.1245, d1.dn_loss_bbox: 2.5190, d2.dn_loss_cls: 1.1245, d2.dn_loss_bbox: 2.5190, d3.dn_loss_cls: 1.1245, d3.dn_loss_bbox: 2.5190, d4.dn_loss_cls: 1.1245, d4.dn_loss_bbox: 2.5190, loss_cls_lane: 0.6464, loss_cls_H: 0.6464, loss_bbox_lane: 6.1837, loss_bbox_H: 6.1774, d0.loss_cls_lane: 0.6464, d0.loss_cls_H: 0.6464, d0.loss_bbox_lane: 6.1837, d0.loss_bbox_H: 6.1774, d1.loss_cls_lane: 0.6464, d1.loss_cls_H: 0.6464, d1.loss_bbox_lane: 6.1837, d1.loss_bbox_H: 6.1774, d2.loss_cls_lane: 0.6464, d2.loss_cls_H: 0.6464, d2.loss_bbox_lane: 6.1837, d2.loss_bbox_H: 6.1774, d3.loss_cls_lane: 0.6464, d3.loss_cls_H: 0.6464, d3.loss_bbox_lane: 6.1837, d3.loss_bbox_H: 6.1774, d4.loss_cls_lane: 0.6464, d4.loss_cls_H: 0.6464, d4.loss_bbox_lane: 6.1837, d4.loss_bbox_H: 6.1774, vlm_loss: 0.0000, loss: 133.6599, grad_norm: nan
2025-02-28 22:44:07,487 - mmdet - INFO - Iter [1050/168780]	lr: 4.000e-04, eta: 6 days, 0:26:16, time: 3.001, data_time: 0.018, memory: 22027, loss_cls: 1.4897, loss_bbox: 2.8817, d0.loss_cls: 1.4897, d0.loss_bbox: 2.8817, d1.loss_cls: 1.4897, d1.loss_bbox: 2.8817, d2.loss_cls: 1.4897, d2.loss_bbox: 2.8817, d3.loss_cls: 1.4897, d3.loss_bbox: 2.8817, d4.loss_cls: 1.4897, d4.loss_bbox: 2.8817, dn_loss_cls: 0.9046, dn_loss_bbox: 2.5441, d0.dn_loss_cls: 0.9046, d0.dn_loss_bbox: 2.5441, d1.dn_loss_cls: 0.9046, d1.dn_loss_bbox: 2.5441, d2.dn_loss_cls: 0.9046, d2.dn_loss_bbox: 2.5441, d3.dn_loss_cls: 0.9046, d3.dn_loss_bbox: 2.5441, d4.dn_loss_cls: 0.9046, d4.dn_loss_bbox: 2.5441, loss_cls_lane: 0.6314, loss_cls_H: 0.6314, loss_bbox_lane: 6.3663, loss_bbox_H: 6.3809, d0.loss_cls_lane: 0.6314, d0.loss_cls_H: 0.6314, d0.loss_bbox_lane: 6.3663, d0.loss_bbox_H: 6.3809, d1.loss_cls_lane: 0.6314, d1.loss_cls_H: 0.6314, d1.loss_bbox_lane: 6.3663, d1.loss_bbox_H: 6.3809, d2.loss_cls_lane: 0.6314, d2.loss_cls_H: 0.6314, d2.loss_bbox_lane: 6.3663, d2.loss_bbox_H: 6.3809, d3.loss_cls_lane: 0.6314, d3.loss_cls_H: 0.6314, d3.loss_bbox_lane: 6.3663, d3.loss_bbox_H: 6.3809, d4.loss_cls_lane: 0.6314, d4.loss_cls_H: 0.6314, d4.loss_bbox_lane: 6.3663, d4.loss_bbox_H: 6.3809, vlm_loss: 0.0000, loss: 130.9806, grad_norm: nan
2025-02-28 22:46:39,429 - mmdet - INFO - Iter [1100/168780]	lr: 4.000e-04, eta: 6 days, 0:15:54, time: 3.039, data_time: 0.019, memory: 22027, loss_cls: 1.5644, loss_bbox: 2.6540, d0.loss_cls: 1.5644, d0.loss_bbox: 2.6540, d1.loss_cls: 1.5644, d1.loss_bbox: 2.6540, d2.loss_cls: 1.5644, d2.loss_bbox: 2.6540, d3.loss_cls: 1.5644, d3.loss_bbox: 2.6540, d4.loss_cls: 1.5644, d4.loss_bbox: 2.6540, dn_loss_cls: 0.9830, dn_loss_bbox: 2.3167, d0.dn_loss_cls: 0.9830, d0.dn_loss_bbox: 2.3167, d1.dn_loss_cls: 0.9830, d1.dn_loss_bbox: 2.3167, d2.dn_loss_cls: 0.9830, d2.dn_loss_bbox: 2.3167, d3.dn_loss_cls: 0.9830, d3.dn_loss_bbox: 2.3167, d4.dn_loss_cls: 0.9830, d4.dn_loss_bbox: 2.3167, loss_cls_lane: 0.6323, loss_cls_H: 0.6323, loss_bbox_lane: 6.7289, loss_bbox_H: 6.7216, d0.loss_cls_lane: 0.6323, d0.loss_cls_H: 0.6323, d0.loss_bbox_lane: 6.7289, d0.loss_bbox_H: 6.7216, d1.loss_cls_lane: 0.6323, d1.loss_cls_H: 0.6323, d1.loss_bbox_lane: 6.7289, d1.loss_bbox_H: 6.7216, d2.loss_cls_lane: 0.6323, d2.loss_cls_H: 0.6323, d2.loss_bbox_lane: 6.7289, d2.loss_bbox_H: 6.7216, d3.loss_cls_lane: 0.6323, d3.loss_cls_H: 0.6323, d3.loss_bbox_lane: 6.7289, d3.loss_bbox_H: 6.7216, d4.loss_cls_lane: 0.6323, d4.loss_cls_H: 0.6323, d4.loss_bbox_lane: 6.7289, d4.loss_bbox_H: 6.7216, vlm_loss: 0.0000, loss: 133.3996, grad_norm: nan
2025-02-28 22:49:07,642 - mmdet - INFO - Iter [1150/168780]	lr: 4.000e-04, eta: 5 days, 23:57:10, time: 2.964, data_time: 0.018, memory: 22027, loss_cls: 2.2171, loss_bbox: 2.9355, d0.loss_cls: 2.2171, d0.loss_bbox: 2.9355, d1.loss_cls: 2.2171, d1.loss_bbox: 2.9355, d2.loss_cls: 2.2171, d2.loss_bbox: 2.9355, d3.loss_cls: 2.2171, d3.loss_bbox: 2.9355, d4.loss_cls: 2.2171, d4.loss_bbox: 2.9355, dn_loss_cls: 0.9368, dn_loss_bbox: 2.6016, d0.dn_loss_cls: 0.9368, d0.dn_loss_bbox: 2.6016, d1.dn_loss_cls: 0.9368, d1.dn_loss_bbox: 2.6016, d2.dn_loss_cls: 0.9368, d2.dn_loss_bbox: 2.6016, d3.dn_loss_cls: 0.9368, d3.dn_loss_bbox: 2.6016, d4.dn_loss_cls: 0.9368, d4.dn_loss_bbox: 2.6016, loss_cls_lane: 0.6360, loss_cls_H: 0.6360, loss_bbox_lane: 6.5888, loss_bbox_H: 6.5739, d0.loss_cls_lane: 0.6360, d0.loss_cls_H: 0.6360, d0.loss_bbox_lane: 6.5888, d0.loss_bbox_H: 6.5739, d1.loss_cls_lane: 0.6360, d1.loss_cls_H: 0.6360, d1.loss_bbox_lane: 6.5888, d1.loss_bbox_H: 6.5739, d2.loss_cls_lane: 0.6360, d2.loss_cls_H: 0.6360, d2.loss_bbox_lane: 6.5888, d2.loss_bbox_H: 6.5739, d3.loss_cls_lane: 0.6360, d3.loss_cls_H: 0.6360, d3.loss_bbox_lane: 6.5888, d3.loss_bbox_H: 6.5739, d4.loss_cls_lane: 0.6360, d4.loss_cls_H: 0.6360, d4.loss_bbox_lane: 6.5888, d4.loss_bbox_H: 6.5739, vlm_loss: 0.0000, loss: 138.7543, grad_norm: nan
2025-02-28 22:51:34,919 - mmdet - INFO - Iter [1200/168780]	lr: 4.000e-04, eta: 5 days, 23:37:36, time: 2.946, data_time: 0.017, memory: 22027, loss_cls: 2.1834, loss_bbox: 2.8767, d0.loss_cls: 2.1834, d0.loss_bbox: 2.8767, d1.loss_cls: 2.1834, d1.loss_bbox: 2.8767, d2.loss_cls: 2.1834, d2.loss_bbox: 2.8767, d3.loss_cls: 2.1834, d3.loss_bbox: 2.8767, d4.loss_cls: 2.1834, d4.loss_bbox: 2.8767, dn_loss_cls: 0.9050, dn_loss_bbox: 2.5217, d0.dn_loss_cls: 0.9050, d0.dn_loss_bbox: 2.5217, d1.dn_loss_cls: 0.9050, d1.dn_loss_bbox: 2.5217, d2.dn_loss_cls: 0.9050, d2.dn_loss_bbox: 2.5217, d3.dn_loss_cls: 0.9050, d3.dn_loss_bbox: 2.5217, d4.dn_loss_cls: 0.9050, d4.dn_loss_bbox: 2.5217, loss_cls_lane: 0.6352, loss_cls_H: 0.6352, loss_bbox_lane: 6.2293, loss_bbox_H: 6.2405, d0.loss_cls_lane: 0.6352, d0.loss_cls_H: 0.6352, d0.loss_bbox_lane: 6.2293, d0.loss_bbox_H: 6.2405, d1.loss_cls_lane: 0.6352, d1.loss_cls_H: 0.6352, d1.loss_bbox_lane: 6.2293, d1.loss_bbox_H: 6.2405, d2.loss_cls_lane: 0.6352, d2.loss_cls_H: 0.6352, d2.loss_bbox_lane: 6.2293, d2.loss_bbox_H: 6.2405, d3.loss_cls_lane: 0.6352, d3.loss_cls_H: 0.6352, d3.loss_bbox_lane: 6.2293, d3.loss_bbox_H: 6.2405, d4.loss_cls_lane: 0.6352, d4.loss_cls_H: 0.6352, d4.loss_bbox_lane: 6.2293, d4.loss_bbox_H: 6.2405, vlm_loss: 0.0000, loss: 133.3621, grad_norm: nan
2025-02-28 22:54:04,022 - mmdet - INFO - Iter [1250/168780]	lr: 3.999e-04, eta: 5 days, 23:23:29, time: 2.982, data_time: 0.017, memory: 22027, loss_cls: 1.5742, loss_bbox: 2.6921, d0.loss_cls: 1.5742, d0.loss_bbox: 2.6921, d1.loss_cls: 1.5742, d1.loss_bbox: 2.6921, d2.loss_cls: 1.5742, d2.loss_bbox: 2.6921, d3.loss_cls: 1.5742, d3.loss_bbox: 2.6921, d4.loss_cls: 1.5742, d4.loss_bbox: 2.6921, dn_loss_cls: 0.9750, dn_loss_bbox: 2.4209, d0.dn_loss_cls: 0.9750, d0.dn_loss_bbox: 2.4209, d1.dn_loss_cls: 0.9750, d1.dn_loss_bbox: 2.4209, d2.dn_loss_cls: 0.9750, d2.dn_loss_bbox: 2.4209, d3.dn_loss_cls: 0.9750, d3.dn_loss_bbox: 2.4209, d4.dn_loss_cls: 0.9750, d4.dn_loss_bbox: 2.4209, loss_cls_lane: 0.6365, loss_cls_H: 0.6365, loss_bbox_lane: 6.6759, loss_bbox_H: 6.6686, d0.loss_cls_lane: 0.6365, d0.loss_cls_H: 0.6365, d0.loss_bbox_lane: 6.6759, d0.loss_bbox_H: 6.6686, d1.loss_cls_lane: 0.6365, d1.loss_cls_H: 0.6365, d1.loss_bbox_lane: 6.6759, d1.loss_bbox_H: 6.6686, d2.loss_cls_lane: 0.6365, d2.loss_cls_H: 0.6365, d2.loss_bbox_lane: 6.6759, d2.loss_bbox_H: 6.6686, d3.loss_cls_lane: 0.6365, d3.loss_cls_H: 0.6365, d3.loss_bbox_lane: 6.6759, d3.loss_bbox_H: 6.6686, d4.loss_cls_lane: 0.6365, d4.loss_cls_H: 0.6365, d4.loss_bbox_lane: 6.6759, d4.loss_bbox_H: 6.6686, vlm_loss: 0.0000, loss: 133.6789, grad_norm: nan
2025-02-28 22:56:32,529 - mmdet - INFO - Iter [1300/168780]	lr: 3.999e-04, eta: 5 days, 23:08:59, time: 2.970, data_time: 0.018, memory: 22027, loss_cls: 1.5244, loss_bbox: 2.7040, d0.loss_cls: 1.5244, d0.loss_bbox: 2.7040, d1.loss_cls: 1.5244, d1.loss_bbox: 2.7040, d2.loss_cls: 1.5244, d2.loss_bbox: 2.7040, d3.loss_cls: 1.5244, d3.loss_bbox: 2.7040, d4.loss_cls: 1.5244, d4.loss_bbox: 2.7040, dn_loss_cls: 0.9150, dn_loss_bbox: 2.2997, d0.dn_loss_cls: 0.9150, d0.dn_loss_bbox: 2.2997, d1.dn_loss_cls: 0.9150, d1.dn_loss_bbox: 2.2997, d2.dn_loss_cls: 0.9150, d2.dn_loss_bbox: 2.2997, d3.dn_loss_cls: 0.9150, d3.dn_loss_bbox: 2.2997, d4.dn_loss_cls: 0.9150, d4.dn_loss_bbox: 2.2997, loss_cls_lane: 0.6370, loss_cls_H: 0.6370, loss_bbox_lane: 6.3804, loss_bbox_H: 6.3735, d0.loss_cls_lane: 0.6370, d0.loss_cls_H: 0.6370, d0.loss_bbox_lane: 6.3804, d0.loss_bbox_H: 6.3735, d1.loss_cls_lane: 0.6370, d1.loss_cls_H: 0.6370, d1.loss_bbox_lane: 6.3804, d1.loss_bbox_H: 6.3735, d2.loss_cls_lane: 0.6370, d2.loss_cls_H: 0.6370, d2.loss_bbox_lane: 6.3804, d2.loss_bbox_H: 6.3735, d3.loss_cls_lane: 0.6370, d3.loss_cls_H: 0.6370, d3.loss_bbox_lane: 6.3804, d3.loss_bbox_H: 6.3735, d4.loss_cls_lane: 0.6370, d4.loss_cls_H: 0.6370, d4.loss_bbox_lane: 6.3804, d4.loss_bbox_H: 6.3735, vlm_loss: 0.0000, loss: 128.8259, grad_norm: nan
2025-02-28 22:59:27,232 - mmdet - INFO - Iter [1350/168780]	lr: 3.999e-04, eta: 5 days, 23:49:31, time: 3.494, data_time: 0.018, memory: 22027, loss_cls: 1.4616, loss_bbox: 2.9649, d0.loss_cls: 1.4616, d0.loss_bbox: 2.9649, d1.loss_cls: 1.4616, d1.loss_bbox: 2.9649, d2.loss_cls: 1.4616, d2.loss_bbox: 2.9649, d3.loss_cls: 1.4616, d3.loss_bbox: 2.9649, d4.loss_cls: 1.4616, d4.loss_bbox: 2.9649, dn_loss_cls: 0.9711, dn_loss_bbox: 2.6722, d0.dn_loss_cls: 0.9711, d0.dn_loss_bbox: 2.6722, d1.dn_loss_cls: 0.9711, d1.dn_loss_bbox: 2.6722, d2.dn_loss_cls: 0.9711, d2.dn_loss_bbox: 2.6722, d3.dn_loss_cls: 0.9711, d3.dn_loss_bbox: 2.6722, d4.dn_loss_cls: 0.9711, d4.dn_loss_bbox: 2.6722, loss_cls_lane: 0.6380, loss_cls_H: 0.6918, loss_bbox_lane: 6.3831, loss_bbox_H: 6.3820, d0.loss_cls_lane: 0.6380, d0.loss_cls_H: 0.6918, d0.loss_bbox_lane: 6.3831, d0.loss_bbox_H: 6.3820, d1.loss_cls_lane: 0.6380, d1.loss_cls_H: 0.6918, d1.loss_bbox_lane: 6.3831, d1.loss_bbox_H: 6.3820, d2.loss_cls_lane: 0.6380, d2.loss_cls_H: 0.6918, d2.loss_bbox_lane: 6.3831, d2.loss_bbox_H: 6.3820, d3.loss_cls_lane: 0.6380, d3.loss_cls_H: 0.6918, d3.loss_bbox_lane: 6.3831, d3.loss_bbox_H: 6.3820, d4.loss_cls_lane: 0.6380, d4.loss_cls_H: 0.6918, d4.loss_bbox_lane: 6.3831, d4.loss_bbox_H: 6.3820, vlm_loss: 0.0000, loss: 132.9881, grad_norm: nan
2025-02-28 23:01:55,155 - mmdet - INFO - Iter [1400/168780]	lr: 3.999e-04, eta: 5 days, 23:33:35, time: 2.958, data_time: 0.018, memory: 22027, loss_cls: 1.7234, loss_bbox: 3.0036, d0.loss_cls: 1.7234, d0.loss_bbox: 3.0036, d1.loss_cls: 1.7234, d1.loss_bbox: 3.0036, d2.loss_cls: 1.7234, d2.loss_bbox: 3.0036, d3.loss_cls: 1.7234, d3.loss_bbox: 3.0036, d4.loss_cls: 1.7234, d4.loss_bbox: 3.0036, dn_loss_cls: 0.9588, dn_loss_bbox: 2.5677, d0.dn_loss_cls: 0.9588, d0.dn_loss_bbox: 2.5677, d1.dn_loss_cls: 0.9588, d1.dn_loss_bbox: 2.5677, d2.dn_loss_cls: 0.9588, d2.dn_loss_bbox: 2.5677, d3.dn_loss_cls: 0.9588, d3.dn_loss_bbox: 2.5677, d4.dn_loss_cls: 0.9588, d4.dn_loss_bbox: 2.5677, loss_cls_lane: 0.6418, loss_cls_H: 0.6418, loss_bbox_lane: 6.5926, loss_bbox_H: 6.5911, d0.loss_cls_lane: 0.6418, d0.loss_cls_H: 0.6418, d0.loss_bbox_lane: 6.5926, d0.loss_bbox_H: 6.5911, d1.loss_cls_lane: 0.6418, d1.loss_cls_H: 0.6418, d1.loss_bbox_lane: 6.5926, d1.loss_bbox_H: 6.5911, d2.loss_cls_lane: 0.6418, d2.loss_cls_H: 0.6418, d2.loss_bbox_lane: 6.5926, d2.loss_bbox_H: 6.5911, d3.loss_cls_lane: 0.6418, d3.loss_cls_H: 0.6418, d3.loss_bbox_lane: 6.5926, d3.loss_bbox_H: 6.5911, d4.loss_cls_lane: 0.6418, d4.loss_cls_H: 0.6418, d4.loss_bbox_lane: 6.5926, d4.loss_bbox_H: 6.5911, vlm_loss: 0.0000, loss: 136.3239, grad_norm: nan
2025-02-28 23:04:23,851 - mmdet - INFO - Iter [1450/168780]	lr: 3.999e-04, eta: 5 days, 23:20:04, time: 2.974, data_time: 0.019, memory: 22027, loss_cls: 1.3844, loss_bbox: 2.7121, d0.loss_cls: 1.3844, d0.loss_bbox: 2.7121, d1.loss_cls: 1.3844, d1.loss_bbox: 2.7121, d2.loss_cls: 1.3844, d2.loss_bbox: 2.7121, d3.loss_cls: 1.3844, d3.loss_bbox: 2.7121, d4.loss_cls: 1.3844, d4.loss_bbox: 2.7121, dn_loss_cls: 0.9147, dn_loss_bbox: 2.4059, d0.dn_loss_cls: 0.9147, d0.dn_loss_bbox: 2.4059, d1.dn_loss_cls: 0.9147, d1.dn_loss_bbox: 2.4059, d2.dn_loss_cls: 0.9147, d2.dn_loss_bbox: 2.4059, d3.dn_loss_cls: 0.9147, d3.dn_loss_bbox: 2.4059, d4.dn_loss_cls: 0.9147, d4.dn_loss_bbox: 2.4059, loss_cls_lane: 0.6284, loss_cls_H: 0.6284, loss_bbox_lane: 6.5310, loss_bbox_H: 6.5291, d0.loss_cls_lane: 0.6284, d0.loss_cls_H: 0.6284, d0.loss_bbox_lane: 6.5310, d0.loss_bbox_H: 6.5291, d1.loss_cls_lane: 0.6284, d1.loss_cls_H: 0.6284, d1.loss_bbox_lane: 6.5310, d1.loss_bbox_H: 6.5291, d2.loss_cls_lane: 0.6284, d2.loss_cls_H: 0.6284, d2.loss_bbox_lane: 6.5310, d2.loss_bbox_H: 6.5291, d3.loss_cls_lane: 0.6284, d3.loss_cls_H: 0.6284, d3.loss_bbox_lane: 6.5310, d3.loss_bbox_H: 6.5291, d4.loss_cls_lane: 0.6284, d4.loss_cls_H: 0.6284, d4.loss_bbox_lane: 6.5310, d4.loss_bbox_H: 6.5291, vlm_loss: 0.0000, loss: 130.4044, grad_norm: nan
2025-02-28 23:07:10,100 - mmdet - INFO - Iter [1500/168780]	lr: 3.999e-04, eta: 5 days, 23:39:55, time: 3.325, data_time: 0.017, memory: 22027, loss_cls: 1.5638, loss_bbox: 2.7030, d0.loss_cls: 1.5638, d0.loss_bbox: 2.7030, d1.loss_cls: 1.5638, d1.loss_bbox: 2.7030, d2.loss_cls: 1.5638, d2.loss_bbox: 2.7030, d3.loss_cls: 1.5638, d3.loss_bbox: 2.7030, d4.loss_cls: 1.5638, d4.loss_bbox: 2.7030, dn_loss_cls: 0.9337, dn_loss_bbox: 2.2965, d0.dn_loss_cls: 0.9337, d0.dn_loss_bbox: 2.2965, d1.dn_loss_cls: 0.9337, d1.dn_loss_bbox: 2.2965, d2.dn_loss_cls: 0.9337, d2.dn_loss_bbox: 2.2965, d3.dn_loss_cls: 0.9337, d3.dn_loss_bbox: 2.2965, d4.dn_loss_cls: 0.9337, d4.dn_loss_bbox: 2.2965, loss_cls_lane: 0.6489, loss_cls_H: 0.6489, loss_bbox_lane: 6.5791, loss_bbox_H: 6.5883, d0.loss_cls_lane: 0.6489, d0.loss_cls_H: 0.6489, d0.loss_bbox_lane: 6.5791, d0.loss_bbox_H: 6.5883, d1.loss_cls_lane: 0.6489, d1.loss_cls_H: 0.6489, d1.loss_bbox_lane: 6.5791, d1.loss_bbox_H: 6.5883, d2.loss_cls_lane: 0.6489, d2.loss_cls_H: 0.6489, d2.loss_bbox_lane: 6.5791, d2.loss_bbox_H: 6.5883, d3.loss_cls_lane: 0.6489, d3.loss_cls_H: 0.6489, d3.loss_bbox_lane: 6.5791, d3.loss_bbox_H: 6.5883, d4.loss_cls_lane: 0.6489, d4.loss_cls_H: 0.6489, d4.loss_bbox_lane: 6.5791, d4.loss_bbox_H: 6.5883, vlm_loss: 0.0000, loss: 131.7736, grad_norm: nan
2025-02-28 23:09:38,509 - mmdet - INFO - Iter [1550/168780]	lr: 3.999e-04, eta: 5 days, 23:26:14, time: 2.968, data_time: 0.017, memory: 22027, loss_cls: 1.4216, loss_bbox: 2.7577, d0.loss_cls: 1.4216, d0.loss_bbox: 2.7577, d1.loss_cls: 1.4216, d1.loss_bbox: 2.7577, d2.loss_cls: 1.4216, d2.loss_bbox: 2.7577, d3.loss_cls: 1.4216, d3.loss_bbox: 2.7577, d4.loss_cls: 1.4216, d4.loss_bbox: 2.7577, dn_loss_cls: 0.9799, dn_loss_bbox: 2.4240, d0.dn_loss_cls: 0.9799, d0.dn_loss_bbox: 2.4240, d1.dn_loss_cls: 0.9799, d1.dn_loss_bbox: 2.4240, d2.dn_loss_cls: 0.9799, d2.dn_loss_bbox: 2.4240, d3.dn_loss_cls: 0.9799, d3.dn_loss_bbox: 2.4240, d4.dn_loss_cls: 0.9799, d4.dn_loss_bbox: 2.4240, loss_cls_lane: 0.6326, loss_cls_H: 0.6326, loss_bbox_lane: 6.3961, loss_bbox_H: 6.3866, d0.loss_cls_lane: 0.6326, d0.loss_cls_H: 0.6326, d0.loss_bbox_lane: 6.3961, d0.loss_bbox_H: 6.3866, d1.loss_cls_lane: 0.6326, d1.loss_cls_H: 0.6326, d1.loss_bbox_lane: 6.3961, d1.loss_bbox_H: 6.3866, d2.loss_cls_lane: 0.6326, d2.loss_cls_H: 0.6326, d2.loss_bbox_lane: 6.3961, d2.loss_bbox_H: 6.3866, d3.loss_cls_lane: 0.6326, d3.loss_cls_H: 0.6326, d3.loss_bbox_lane: 6.3961, d3.loss_bbox_H: 6.3866, d4.loss_cls_lane: 0.6326, d4.loss_cls_H: 0.6326, d4.loss_bbox_lane: 6.3961, d4.loss_bbox_H: 6.3866, vlm_loss: 0.0000, loss: 129.7865, grad_norm: nan
2025-02-28 23:12:06,684 - mmdet - INFO - Iter [1600/168780]	lr: 3.999e-04, eta: 5 days, 23:12:50, time: 2.963, data_time: 0.017, memory: 22027, loss_cls: 1.9245, loss_bbox: 2.9195, d0.loss_cls: 1.9245, d0.loss_bbox: 2.9195, d1.loss_cls: 1.9245, d1.loss_bbox: 2.9195, d2.loss_cls: 1.9245, d2.loss_bbox: 2.9195, d3.loss_cls: 1.9245, d3.loss_bbox: 2.9195, d4.loss_cls: 1.9245, d4.loss_bbox: 2.9195, dn_loss_cls: 0.9802, dn_loss_bbox: 2.5599, d0.dn_loss_cls: 0.9802, d0.dn_loss_bbox: 2.5599, d1.dn_loss_cls: 0.9802, d1.dn_loss_bbox: 2.5599, d2.dn_loss_cls: 0.9802, d2.dn_loss_bbox: 2.5599, d3.dn_loss_cls: 0.9802, d3.dn_loss_bbox: 2.5599, d4.dn_loss_cls: 0.9802, d4.dn_loss_bbox: 2.5599, loss_cls_lane: 0.6464, loss_cls_H: 0.7002, loss_bbox_lane: 6.2856, loss_bbox_H: 6.2926, d0.loss_cls_lane: 0.6464, d0.loss_cls_H: 0.7002, d0.loss_bbox_lane: 6.2856, d0.loss_bbox_H: 6.2926, d1.loss_cls_lane: 0.6464, d1.loss_cls_H: 0.7002, d1.loss_bbox_lane: 6.2856, d1.loss_bbox_H: 6.2926, d2.loss_cls_lane: 0.6464, d2.loss_cls_H: 0.7002, d2.loss_bbox_lane: 6.2856, d2.loss_bbox_H: 6.2926, d3.loss_cls_lane: 0.6464, d3.loss_cls_H: 0.7002, d3.loss_bbox_lane: 6.2856, d3.loss_bbox_H: 6.2926, d4.loss_cls_lane: 0.6464, d4.loss_cls_H: 0.7002, d4.loss_bbox_lane: 6.2856, d4.loss_bbox_H: 6.2926, vlm_loss: 0.0000, loss: 133.8542, grad_norm: nan
2025-02-28 23:14:36,117 - mmdet - INFO - Iter [1650/168780]	lr: 3.999e-04, eta: 5 days, 23:02:13, time: 2.989, data_time: 0.018, memory: 22027, loss_cls: 1.4833, loss_bbox: 2.9779, d0.loss_cls: 1.4833, d0.loss_bbox: 2.9779, d1.loss_cls: 1.4833, d1.loss_bbox: 2.9779, d2.loss_cls: 1.4833, d2.loss_bbox: 2.9779, d3.loss_cls: 1.4833, d3.loss_bbox: 2.9779, d4.loss_cls: 1.4833, d4.loss_bbox: 2.9779, dn_loss_cls: 0.9338, dn_loss_bbox: 2.5935, d0.dn_loss_cls: 0.9338, d0.dn_loss_bbox: 2.5935, d1.dn_loss_cls: 0.9338, d1.dn_loss_bbox: 2.5935, d2.dn_loss_cls: 0.9338, d2.dn_loss_bbox: 2.5935, d3.dn_loss_cls: 0.9338, d3.dn_loss_bbox: 2.5935, d4.dn_loss_cls: 0.9338, d4.dn_loss_bbox: 2.5935, loss_cls_lane: 0.6499, loss_cls_H: 0.6499, loss_bbox_lane: 6.5562, loss_bbox_H: 6.5676, d0.loss_cls_lane: 0.6499, d0.loss_cls_H: 0.6499, d0.loss_bbox_lane: 6.5562, d0.loss_bbox_H: 6.5676, d1.loss_cls_lane: 0.6499, d1.loss_cls_H: 0.6499, d1.loss_bbox_lane: 6.5562, d1.loss_bbox_H: 6.5676, d2.loss_cls_lane: 0.6499, d2.loss_cls_H: 0.6499, d2.loss_bbox_lane: 6.5562, d2.loss_bbox_H: 6.5676, d3.loss_cls_lane: 0.6499, d3.loss_cls_H: 0.6499, d3.loss_bbox_lane: 6.5562, d3.loss_bbox_H: 6.5676, d4.loss_cls_lane: 0.6499, d4.loss_cls_H: 0.6499, d4.loss_bbox_lane: 6.5562, d4.loss_bbox_H: 6.5676, vlm_loss: 0.0000, loss: 134.4729, grad_norm: nan
2025-02-28 23:17:05,589 - mmdet - INFO - Iter [1700/168780]	lr: 3.999e-04, eta: 5 days, 22:52:09, time: 2.989, data_time: 0.017, memory: 22027, loss_cls: 1.9286, loss_bbox: 2.8064, d0.loss_cls: 1.9286, d0.loss_bbox: 2.8064, d1.loss_cls: 1.9286, d1.loss_bbox: 2.8064, d2.loss_cls: 1.9286, d2.loss_bbox: 2.8064, d3.loss_cls: 1.9286, d3.loss_bbox: 2.8064, d4.loss_cls: 1.9286, d4.loss_bbox: 2.8064, dn_loss_cls: 0.9790, dn_loss_bbox: 2.4738, d0.dn_loss_cls: 0.9790, d0.dn_loss_bbox: 2.4738, d1.dn_loss_cls: 0.9790, d1.dn_loss_bbox: 2.4738, d2.dn_loss_cls: 0.9790, d2.dn_loss_bbox: 2.4738, d3.dn_loss_cls: 0.9790, d3.dn_loss_bbox: 2.4738, d4.dn_loss_cls: 0.9790, d4.dn_loss_bbox: 2.4738, loss_cls_lane: 0.6448, loss_cls_H: 0.6448, loss_bbox_lane: 6.8806, loss_bbox_H: 6.8687, d0.loss_cls_lane: 0.6448, d0.loss_cls_H: 0.6448, d0.loss_bbox_lane: 6.8806, d0.loss_bbox_H: 6.8687, d1.loss_cls_lane: 0.6448, d1.loss_cls_H: 0.6448, d1.loss_bbox_lane: 6.8806, d1.loss_bbox_H: 6.8687, d2.loss_cls_lane: 0.6448, d2.loss_cls_H: 0.6448, d2.loss_bbox_lane: 6.8806, d2.loss_bbox_H: 6.8687, d3.loss_cls_lane: 0.6448, d3.loss_cls_H: 0.6448, d3.loss_bbox_lane: 6.8806, d3.loss_bbox_H: 6.8687, d4.loss_cls_lane: 0.6448, d4.loss_cls_H: 0.6448, d4.loss_bbox_lane: 6.8806, d4.loss_bbox_H: 6.8687, vlm_loss: 0.0000, loss: 139.3614, grad_norm: nan
2025-02-28 23:19:34,124 - mmdet - INFO - Iter [1750/168780]	lr: 3.999e-04, eta: 5 days, 22:41:01, time: 2.971, data_time: 0.017, memory: 22027, loss_cls: 1.7716, loss_bbox: 3.0064, d0.loss_cls: 1.7716, d0.loss_bbox: 3.0064, d1.loss_cls: 1.7716, d1.loss_bbox: 3.0064, d2.loss_cls: 1.7716, d2.loss_bbox: 3.0064, d3.loss_cls: 1.7716, d3.loss_bbox: 3.0064, d4.loss_cls: 1.7716, d4.loss_bbox: 3.0064, dn_loss_cls: 0.9590, dn_loss_bbox: 2.6329, d0.dn_loss_cls: 0.9590, d0.dn_loss_bbox: 2.6329, d1.dn_loss_cls: 0.9590, d1.dn_loss_bbox: 2.6329, d2.dn_loss_cls: 0.9590, d2.dn_loss_bbox: 2.6329, d3.dn_loss_cls: 0.9590, d3.dn_loss_bbox: 2.6329, d4.dn_loss_cls: 0.9590, d4.dn_loss_bbox: 2.6329, loss_cls_lane: 0.6422, loss_cls_H: 0.6422, loss_bbox_lane: 6.6223, loss_bbox_H: 6.6184, d0.loss_cls_lane: 0.6422, d0.loss_cls_H: 0.6422, d0.loss_bbox_lane: 6.6223, d0.loss_bbox_H: 6.6184, d1.loss_cls_lane: 0.6422, d1.loss_cls_H: 0.6422, d1.loss_bbox_lane: 6.6223, d1.loss_bbox_H: 6.6184, d2.loss_cls_lane: 0.6422, d2.loss_cls_H: 0.6422, d2.loss_bbox_lane: 6.6223, d2.loss_bbox_H: 6.6184, d3.loss_cls_lane: 0.6422, d3.loss_cls_H: 0.6422, d3.loss_bbox_lane: 6.6223, d3.loss_bbox_H: 6.6184, d4.loss_cls_lane: 0.6422, d4.loss_cls_H: 0.6422, d4.loss_bbox_lane: 6.6223, d4.loss_bbox_H: 6.6184, vlm_loss: 0.0000, loss: 137.3705, grad_norm: nan
2025-02-28 23:22:03,005 - mmdet - INFO - Iter [1800/168780]	lr: 3.999e-04, eta: 5 days, 22:30:55, time: 2.978, data_time: 0.018, memory: 22027, loss_cls: 1.4721, loss_bbox: 2.7234, d0.loss_cls: 1.4721, d0.loss_bbox: 2.7234, d1.loss_cls: 1.4721, d1.loss_bbox: 2.7234, d2.loss_cls: 1.4721, d2.loss_bbox: 2.7234, d3.loss_cls: 1.4721, d3.loss_bbox: 2.7234, d4.loss_cls: 1.4721, d4.loss_bbox: 2.7234, dn_loss_cls: 0.8914, dn_loss_bbox: 2.3580, d0.dn_loss_cls: 0.8914, d0.dn_loss_bbox: 2.3580, d1.dn_loss_cls: 0.8914, d1.dn_loss_bbox: 2.3580, d2.dn_loss_cls: 0.8914, d2.dn_loss_bbox: 2.3580, d3.dn_loss_cls: 0.8914, d3.dn_loss_bbox: 2.3580, d4.dn_loss_cls: 0.8914, d4.dn_loss_bbox: 2.3580, loss_cls_lane: 0.6537, loss_cls_H: 0.6537, loss_bbox_lane: 7.0345, loss_bbox_H: 7.0179, d0.loss_cls_lane: 0.6537, d0.loss_cls_H: 0.6537, d0.loss_bbox_lane: 7.0345, d0.loss_bbox_H: 7.0179, d1.loss_cls_lane: 0.6537, d1.loss_cls_H: 0.6537, d1.loss_bbox_lane: 7.0345, d1.loss_bbox_H: 7.0179, d2.loss_cls_lane: 0.6537, d2.loss_cls_H: 0.6537, d2.loss_bbox_lane: 7.0345, d2.loss_bbox_H: 7.0179, d3.loss_cls_lane: 0.6537, d3.loss_cls_H: 0.6537, d3.loss_bbox_lane: 7.0345, d3.loss_bbox_H: 7.0179, d4.loss_cls_lane: 0.6537, d4.loss_cls_H: 0.6537, d4.loss_bbox_lane: 7.0345, d4.loss_bbox_H: 7.0179, vlm_loss: 0.0000, loss: 136.8293, grad_norm: nan
2025-02-28 23:24:30,478 - mmdet - INFO - Iter [1850/168780]	lr: 3.999e-04, eta: 5 days, 22:19:06, time: 2.949, data_time: 0.017, memory: 22027, loss_cls: 2.0344, loss_bbox: 3.0149, d0.loss_cls: 2.0344, d0.loss_bbox: 3.0149, d1.loss_cls: 2.0344, d1.loss_bbox: 3.0149, d2.loss_cls: 2.0344, d2.loss_bbox: 3.0149, d3.loss_cls: 2.0344, d3.loss_bbox: 3.0149, d4.loss_cls: 2.0344, d4.loss_bbox: 3.0149, dn_loss_cls: 0.9391, dn_loss_bbox: 2.6714, d0.dn_loss_cls: 0.9391, d0.dn_loss_bbox: 2.6714, d1.dn_loss_cls: 0.9391, d1.dn_loss_bbox: 2.6714, d2.dn_loss_cls: 0.9391, d2.dn_loss_bbox: 2.6714, d3.dn_loss_cls: 0.9391, d3.dn_loss_bbox: 2.6714, d4.dn_loss_cls: 0.9391, d4.dn_loss_bbox: 2.6714, loss_cls_lane: 0.6339, loss_cls_H: 0.6339, loss_bbox_lane: 6.4640, loss_bbox_H: 6.4552, d0.loss_cls_lane: 0.6339, d0.loss_cls_H: 0.6339, d0.loss_bbox_lane: 6.4640, d0.loss_bbox_H: 6.4552, d1.loss_cls_lane: 0.6339, d1.loss_cls_H: 0.6339, d1.loss_bbox_lane: 6.4640, d1.loss_bbox_H: 6.4552, d2.loss_cls_lane: 0.6339, d2.loss_cls_H: 0.6339, d2.loss_bbox_lane: 6.4640, d2.loss_bbox_H: 6.4552, d3.loss_cls_lane: 0.6339, d3.loss_cls_H: 0.6339, d3.loss_bbox_lane: 6.4640, d3.loss_bbox_H: 6.4552, d4.loss_cls_lane: 0.6339, d4.loss_cls_H: 0.6339, d4.loss_bbox_lane: 6.4640, d4.loss_bbox_H: 6.4552, vlm_loss: 0.0000, loss: 137.0807, grad_norm: nan
2025-02-28 23:26:58,028 - mmdet - INFO - Iter [1900/168780]	lr: 3.999e-04, eta: 5 days, 22:07:53, time: 2.951, data_time: 0.018, memory: 22027, loss_cls: 1.3841, loss_bbox: 2.8028, d0.loss_cls: 1.3841, d0.loss_bbox: 2.8028, d1.loss_cls: 1.3841, d1.loss_bbox: 2.8028, d2.loss_cls: 1.3841, d2.loss_bbox: 2.8028, d3.loss_cls: 1.3841, d3.loss_bbox: 2.8028, d4.loss_cls: 1.3841, d4.loss_bbox: 2.8028, dn_loss_cls: 0.8508, dn_loss_bbox: 2.5002, d0.dn_loss_cls: 0.8508, d0.dn_loss_bbox: 2.5002, d1.dn_loss_cls: 0.8508, d1.dn_loss_bbox: 2.5002, d2.dn_loss_cls: 0.8508, d2.dn_loss_bbox: 2.5002, d3.dn_loss_cls: 0.8508, d3.dn_loss_bbox: 2.5002, d4.dn_loss_cls: 0.8508, d4.dn_loss_bbox: 2.5002, loss_cls_lane: 0.6494, loss_cls_H: 0.6494, loss_bbox_lane: 6.2157, loss_bbox_H: 6.2090, d0.loss_cls_lane: 0.6494, d0.loss_cls_H: 0.6494, d0.loss_bbox_lane: 6.2157, d0.loss_bbox_H: 6.2090, d1.loss_cls_lane: 0.6494, d1.loss_cls_H: 0.6494, d1.loss_bbox_lane: 6.2157, d1.loss_bbox_H: 6.2090, d2.loss_cls_lane: 0.6494, d2.loss_cls_H: 0.6494, d2.loss_bbox_lane: 6.2157, d2.loss_bbox_H: 6.2090, d3.loss_cls_lane: 0.6494, d3.loss_cls_H: 0.6494, d3.loss_bbox_lane: 6.2157, d3.loss_bbox_H: 6.2090, d4.loss_cls_lane: 0.6494, d4.loss_cls_H: 0.6494, d4.loss_bbox_lane: 6.2157, d4.loss_bbox_H: 6.2090, vlm_loss: 0.0000, loss: 127.5685, grad_norm: nan
2025-02-28 23:29:23,438 - mmdet - INFO - Iter [1950/168780]	lr: 3.999e-04, eta: 5 days, 21:54:04, time: 2.908, data_time: 0.016, memory: 22027, loss_cls: 3.1429, loss_bbox: 3.3625, d0.loss_cls: 3.1429, d0.loss_bbox: 3.3625, d1.loss_cls: 3.1429, d1.loss_bbox: 3.3625, d2.loss_cls: 3.1429, d2.loss_bbox: 3.3625, d3.loss_cls: 3.1429, d3.loss_bbox: 3.3625, d4.loss_cls: 3.1429, d4.loss_bbox: 3.3625, dn_loss_cls: 0.8791, dn_loss_bbox: 3.0670, d0.dn_loss_cls: 0.8791, d0.dn_loss_bbox: 3.0670, d1.dn_loss_cls: 0.8791, d1.dn_loss_bbox: 3.0670, d2.dn_loss_cls: 0.8791, d2.dn_loss_bbox: 3.0670, d3.dn_loss_cls: 0.8791, d3.dn_loss_bbox: 3.0670, d4.dn_loss_cls: 0.8791, d4.dn_loss_bbox: 3.0670, loss_cls_lane: 0.6842, loss_cls_H: 0.6842, loss_bbox_lane: 6.9098, loss_bbox_H: 6.8880, d0.loss_cls_lane: 0.6842, d0.loss_cls_H: 0.6842, d0.loss_bbox_lane: 6.9098, d0.loss_bbox_H: 6.8880, d1.loss_cls_lane: 0.6842, d1.loss_cls_H: 0.6842, d1.loss_bbox_lane: 6.9098, d1.loss_bbox_H: 6.8880, d2.loss_cls_lane: 0.6842, d2.loss_cls_H: 0.6842, d2.loss_bbox_lane: 6.9098, d2.loss_bbox_H: 6.8880, d3.loss_cls_lane: 0.6842, d3.loss_cls_H: 0.6842, d3.loss_bbox_lane: 6.9098, d3.loss_bbox_H: 6.8880, d4.loss_cls_lane: 0.6842, d4.loss_cls_H: 0.6842, d4.loss_bbox_lane: 6.9098, d4.loss_bbox_H: 6.8880, vlm_loss: 0.0000, loss: 153.7065, grad_norm: nan
2025-02-28 23:31:53,486 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-02-28 23:31:53,486 - mmdet - INFO - Iter [2000/168780]	lr: 3.999e-04, eta: 5 days, 21:47:16, time: 3.001, data_time: 0.018, memory: 22027, loss_cls: 1.5096, loss_bbox: 2.8820, d0.loss_cls: 1.5096, d0.loss_bbox: 2.8820, d1.loss_cls: 1.5096, d1.loss_bbox: 2.8820, d2.loss_cls: 1.5096, d2.loss_bbox: 2.8820, d3.loss_cls: 1.5096, d3.loss_bbox: 2.8820, d4.loss_cls: 1.5096, d4.loss_bbox: 2.8820, dn_loss_cls: 0.9936, dn_loss_bbox: 2.3988, d0.dn_loss_cls: 0.9936, d0.dn_loss_bbox: 2.3988, d1.dn_loss_cls: 0.9936, d1.dn_loss_bbox: 2.3988, d2.dn_loss_cls: 0.9936, d2.dn_loss_bbox: 2.3988, d3.dn_loss_cls: 0.9936, d3.dn_loss_bbox: 2.3988, d4.dn_loss_cls: 0.9936, d4.dn_loss_bbox: 2.3988, loss_cls_lane: 0.6320, loss_cls_H: 0.6320, loss_bbox_lane: 6.3105, loss_bbox_H: 6.3173, d0.loss_cls_lane: 0.6320, d0.loss_cls_H: 0.6320, d0.loss_bbox_lane: 6.3105, d0.loss_bbox_H: 6.3173, d1.loss_cls_lane: 0.6320, d1.loss_cls_H: 0.6320, d1.loss_bbox_lane: 6.3105, d1.loss_bbox_H: 6.3173, d2.loss_cls_lane: 0.6320, d2.loss_cls_H: 0.6320, d2.loss_bbox_lane: 6.3105, d2.loss_bbox_H: 6.3173, d3.loss_cls_lane: 0.6320, d3.loss_cls_H: 0.6320, d3.loss_bbox_lane: 6.3105, d3.loss_bbox_H: 6.3173, d4.loss_cls_lane: 0.6320, d4.loss_cls_H: 0.6320, d4.loss_bbox_lane: 6.3105, d4.loss_bbox_H: 6.3173, vlm_loss: 0.0000, loss: 130.0552, grad_norm: nan
2025-02-28 23:34:20,128 - mmdet - INFO - Iter [2050/168780]	lr: 3.999e-04, eta: 5 days, 21:36:04, time: 2.933, data_time: 0.018, memory: 22027, loss_cls: 1.7898, loss_bbox: 3.0719, d0.loss_cls: 1.7898, d0.loss_bbox: 3.0719, d1.loss_cls: 1.7898, d1.loss_bbox: 3.0719, d2.loss_cls: 1.7898, d2.loss_bbox: 3.0719, d3.loss_cls: 1.7898, d3.loss_bbox: 3.0719, d4.loss_cls: 1.7898, d4.loss_bbox: 3.0719, dn_loss_cls: 0.9616, dn_loss_bbox: 2.7774, d0.dn_loss_cls: 0.9616, d0.dn_loss_bbox: 2.7774, d1.dn_loss_cls: 0.9616, d1.dn_loss_bbox: 2.7774, d2.dn_loss_cls: 0.9616, d2.dn_loss_bbox: 2.7774, d3.dn_loss_cls: 0.9616, d3.dn_loss_bbox: 2.7774, d4.dn_loss_cls: 0.9616, d4.dn_loss_bbox: 2.7774, loss_cls_lane: 0.6316, loss_cls_H: 0.6316, loss_bbox_lane: 6.7556, loss_bbox_H: 6.7361, d0.loss_cls_lane: 0.6316, d0.loss_cls_H: 0.6316, d0.loss_bbox_lane: 6.7556, d0.loss_bbox_H: 6.7361, d1.loss_cls_lane: 0.6316, d1.loss_cls_H: 0.6316, d1.loss_bbox_lane: 6.7556, d1.loss_bbox_H: 6.7361, d2.loss_cls_lane: 0.6316, d2.loss_cls_H: 0.6316, d2.loss_bbox_lane: 6.7556, d2.loss_bbox_H: 6.7361, d3.loss_cls_lane: 0.6316, d3.loss_cls_H: 0.6316, d3.loss_bbox_lane: 6.7556, d3.loss_bbox_H: 6.7361, d4.loss_cls_lane: 0.6316, d4.loss_cls_H: 0.6316, d4.loss_bbox_lane: 6.7556, d4.loss_bbox_H: 6.7361, vlm_loss: 0.0000, loss: 140.1333, grad_norm: nan
2025-02-28 23:36:48,905 - mmdet - INFO - Iter [2100/168780]	lr: 3.998e-04, eta: 5 days, 21:28:06, time: 2.976, data_time: 0.017, memory: 22027, loss_cls: 1.5590, loss_bbox: 2.7729, d0.loss_cls: 1.5590, d0.loss_bbox: 2.7729, d1.loss_cls: 1.5590, d1.loss_bbox: 2.7729, d2.loss_cls: 1.5590, d2.loss_bbox: 2.7729, d3.loss_cls: 1.5590, d3.loss_bbox: 2.7729, d4.loss_cls: 1.5590, d4.loss_bbox: 2.7729, dn_loss_cls: 0.8811, dn_loss_bbox: 2.4084, d0.dn_loss_cls: 0.8811, d0.dn_loss_bbox: 2.4084, d1.dn_loss_cls: 0.8811, d1.dn_loss_bbox: 2.4084, d2.dn_loss_cls: 0.8811, d2.dn_loss_bbox: 2.4084, d3.dn_loss_cls: 0.8811, d3.dn_loss_bbox: 2.4084, d4.dn_loss_cls: 0.8811, d4.dn_loss_bbox: 2.4084, loss_cls_lane: 0.6375, loss_cls_H: 0.6375, loss_bbox_lane: 6.4384, loss_bbox_H: 6.4393, d0.loss_cls_lane: 0.6375, d0.loss_cls_H: 0.6375, d0.loss_bbox_lane: 6.4384, d0.loss_bbox_H: 6.4393, d1.loss_cls_lane: 0.6375, d1.loss_cls_H: 0.6375, d1.loss_bbox_lane: 6.4384, d1.loss_bbox_H: 6.4393, d2.loss_cls_lane: 0.6375, d2.loss_cls_H: 0.6375, d2.loss_bbox_lane: 6.4384, d2.loss_bbox_H: 6.4393, d3.loss_cls_lane: 0.6375, d3.loss_cls_H: 0.6375, d3.loss_bbox_lane: 6.4384, d3.loss_bbox_H: 6.4393, d4.loss_cls_lane: 0.6375, d4.loss_cls_H: 0.6375, d4.loss_bbox_lane: 6.4384, d4.loss_bbox_H: 6.4393, vlm_loss: 0.0000, loss: 130.6441, grad_norm: nan
2025-02-28 23:39:17,838 - mmdet - INFO - Iter [2150/168780]	lr: 3.998e-04, eta: 5 days, 21:20:36, time: 2.979, data_time: 0.018, memory: 22027, loss_cls: 1.2536, loss_bbox: 2.7286, d0.loss_cls: 1.2536, d0.loss_bbox: 2.7286, d1.loss_cls: 1.2536, d1.loss_bbox: 2.7286, d2.loss_cls: 1.2536, d2.loss_bbox: 2.7286, d3.loss_cls: 1.2536, d3.loss_bbox: 2.7286, d4.loss_cls: 1.2536, d4.loss_bbox: 2.7286, dn_loss_cls: 0.9145, dn_loss_bbox: 2.4663, d0.dn_loss_cls: 0.9145, d0.dn_loss_bbox: 2.4663, d1.dn_loss_cls: 0.9145, d1.dn_loss_bbox: 2.4663, d2.dn_loss_cls: 0.9145, d2.dn_loss_bbox: 2.4663, d3.dn_loss_cls: 0.9145, d3.dn_loss_bbox: 2.4663, d4.dn_loss_cls: 0.9145, d4.dn_loss_bbox: 2.4663, loss_cls_lane: 0.6332, loss_cls_H: 0.6332, loss_bbox_lane: 6.4104, loss_bbox_H: 6.4105, d0.loss_cls_lane: 0.6332, d0.loss_cls_H: 0.6332, d0.loss_bbox_lane: 6.4104, d0.loss_bbox_H: 6.4105, d1.loss_cls_lane: 0.6332, d1.loss_cls_H: 0.6332, d1.loss_bbox_lane: 6.4104, d1.loss_bbox_H: 6.4105, d2.loss_cls_lane: 0.6332, d2.loss_cls_H: 0.6332, d2.loss_bbox_lane: 6.4104, d2.loss_bbox_H: 6.4105, d3.loss_cls_lane: 0.6332, d3.loss_cls_H: 0.6332, d3.loss_bbox_lane: 6.4104, d3.loss_bbox_H: 6.4105, d4.loss_cls_lane: 0.6332, d4.loss_cls_H: 0.6332, d4.loss_bbox_lane: 6.4104, d4.loss_bbox_H: 6.4105, vlm_loss: 0.0000, loss: 128.7015, grad_norm: nan
2025-02-28 23:41:44,853 - mmdet - INFO - Iter [2200/168780]	lr: 3.998e-04, eta: 5 days, 21:10:54, time: 2.940, data_time: 0.018, memory: 22027, loss_cls: 2.7435, loss_bbox: 3.0150, d0.loss_cls: 2.7435, d0.loss_bbox: 3.0150, d1.loss_cls: 2.7435, d1.loss_bbox: 3.0150, d2.loss_cls: 2.7435, d2.loss_bbox: 3.0150, d3.loss_cls: 2.7435, d3.loss_bbox: 3.0150, d4.loss_cls: 2.7435, d4.loss_bbox: 3.0150, dn_loss_cls: 0.9100, dn_loss_bbox: 2.8268, d0.dn_loss_cls: 0.9100, d0.dn_loss_bbox: 2.8268, d1.dn_loss_cls: 0.9100, d1.dn_loss_bbox: 2.8268, d2.dn_loss_cls: 0.9100, d2.dn_loss_bbox: 2.8268, d3.dn_loss_cls: 0.9100, d3.dn_loss_bbox: 2.8268, d4.dn_loss_cls: 0.9100, d4.dn_loss_bbox: 2.8268, loss_cls_lane: 0.6418, loss_cls_H: 0.6418, loss_bbox_lane: 6.6794, loss_bbox_H: 6.6717, d0.loss_cls_lane: 0.6418, d0.loss_cls_H: 0.6418, d0.loss_bbox_lane: 6.6794, d0.loss_bbox_H: 6.6717, d1.loss_cls_lane: 0.6418, d1.loss_cls_H: 0.6418, d1.loss_bbox_lane: 6.6794, d1.loss_bbox_H: 6.6717, d2.loss_cls_lane: 0.6418, d2.loss_cls_H: 0.6418, d2.loss_bbox_lane: 6.6794, d2.loss_bbox_H: 6.6717, d3.loss_cls_lane: 0.6418, d3.loss_cls_H: 0.6418, d3.loss_bbox_lane: 6.6794, d3.loss_bbox_H: 6.6717, d4.loss_cls_lane: 0.6418, d4.loss_cls_H: 0.6418, d4.loss_bbox_lane: 6.6794, d4.loss_bbox_H: 6.6717, vlm_loss: 0.0000, loss: 144.7802, grad_norm: nan
2025-02-28 23:44:12,876 - mmdet - INFO - Iter [2250/168780]	lr: 3.998e-04, eta: 5 days, 21:02:46, time: 2.960, data_time: 0.018, memory: 22027, loss_cls: 1.6009, loss_bbox: 2.7719, d0.loss_cls: 1.6009, d0.loss_bbox: 2.7719, d1.loss_cls: 1.6009, d1.loss_bbox: 2.7719, d2.loss_cls: 1.6009, d2.loss_bbox: 2.7719, d3.loss_cls: 1.6009, d3.loss_bbox: 2.7719, d4.loss_cls: 1.6009, d4.loss_bbox: 2.7719, dn_loss_cls: 0.9154, dn_loss_bbox: 2.4167, d0.dn_loss_cls: 0.9154, d0.dn_loss_bbox: 2.4167, d1.dn_loss_cls: 0.9154, d1.dn_loss_bbox: 2.4167, d2.dn_loss_cls: 0.9154, d2.dn_loss_bbox: 2.4167, d3.dn_loss_cls: 0.9154, d3.dn_loss_bbox: 2.4167, d4.dn_loss_cls: 0.9154, d4.dn_loss_bbox: 2.4167, loss_cls_lane: 0.6450, loss_cls_H: 0.6450, loss_bbox_lane: 6.4250, loss_bbox_H: 6.4214, d0.loss_cls_lane: 0.6450, d0.loss_cls_H: 0.6450, d0.loss_bbox_lane: 6.4250, d0.loss_bbox_H: 6.4214, d1.loss_cls_lane: 0.6450, d1.loss_cls_H: 0.6450, d1.loss_bbox_lane: 6.4250, d1.loss_bbox_H: 6.4214, d2.loss_cls_lane: 0.6450, d2.loss_cls_H: 0.6450, d2.loss_bbox_lane: 6.4250, d2.loss_bbox_H: 6.4214, d3.loss_cls_lane: 0.6450, d3.loss_cls_H: 0.6450, d3.loss_bbox_lane: 6.4250, d3.loss_bbox_H: 6.4214, d4.loss_cls_lane: 0.6450, d4.loss_cls_H: 0.6450, d4.loss_bbox_lane: 6.4250, d4.loss_bbox_H: 6.4214, vlm_loss: 0.0000, loss: 131.0477, grad_norm: nan
2025-02-28 23:46:41,205 - mmdet - INFO - Iter [2300/168780]	lr: 3.998e-04, eta: 5 days, 20:55:15, time: 2.967, data_time: 0.017, memory: 22027, loss_cls: 2.0060, loss_bbox: 2.6436, d0.loss_cls: 2.0060, d0.loss_bbox: 2.6436, d1.loss_cls: 2.0060, d1.loss_bbox: 2.6436, d2.loss_cls: 2.0060, d2.loss_bbox: 2.6436, d3.loss_cls: 2.0060, d3.loss_bbox: 2.6436, d4.loss_cls: 2.0060, d4.loss_bbox: 2.6436, dn_loss_cls: 0.9907, dn_loss_bbox: 2.3739, d0.dn_loss_cls: 0.9907, d0.dn_loss_bbox: 2.3739, d1.dn_loss_cls: 0.9907, d1.dn_loss_bbox: 2.3739, d2.dn_loss_cls: 0.9907, d2.dn_loss_bbox: 2.3739, d3.dn_loss_cls: 0.9907, d3.dn_loss_bbox: 2.3739, d4.dn_loss_cls: 0.9907, d4.dn_loss_bbox: 2.3739, loss_cls_lane: 0.6437, loss_cls_H: 0.6437, loss_bbox_lane: 6.3888, loss_bbox_H: 6.3899, d0.loss_cls_lane: 0.6437, d0.loss_cls_H: 0.6437, d0.loss_bbox_lane: 6.3888, d0.loss_bbox_H: 6.3899, d1.loss_cls_lane: 0.6437, d1.loss_cls_H: 0.6437, d1.loss_bbox_lane: 6.3888, d1.loss_bbox_H: 6.3899, d2.loss_cls_lane: 0.6437, d2.loss_cls_H: 0.6437, d2.loss_bbox_lane: 6.3888, d2.loss_bbox_H: 6.3899, d3.loss_cls_lane: 0.6437, d3.loss_cls_H: 0.6437, d3.loss_bbox_lane: 6.3888, d3.loss_bbox_H: 6.3899, d4.loss_cls_lane: 0.6437, d4.loss_cls_H: 0.6437, d4.loss_bbox_lane: 6.3888, d4.loss_bbox_H: 6.3899, vlm_loss: 0.0000, loss: 132.4816, grad_norm: nan
2025-02-28 23:49:09,724 - mmdet - INFO - Iter [2350/168780]	lr: 3.998e-04, eta: 5 days, 20:48:10, time: 2.970, data_time: 0.019, memory: 22027, loss_cls: 1.6525, loss_bbox: 2.6165, d0.loss_cls: 1.6525, d0.loss_bbox: 2.6165, d1.loss_cls: 1.6525, d1.loss_bbox: 2.6165, d2.loss_cls: 1.6525, d2.loss_bbox: 2.6165, d3.loss_cls: 1.6525, d3.loss_bbox: 2.6165, d4.loss_cls: 1.6525, d4.loss_bbox: 2.6165, dn_loss_cls: 0.9603, dn_loss_bbox: 2.2837, d0.dn_loss_cls: 0.9603, d0.dn_loss_bbox: 2.2837, d1.dn_loss_cls: 0.9603, d1.dn_loss_bbox: 2.2837, d2.dn_loss_cls: 0.9603, d2.dn_loss_bbox: 2.2837, d3.dn_loss_cls: 0.9603, d3.dn_loss_bbox: 2.2837, d4.dn_loss_cls: 0.9603, d4.dn_loss_bbox: 2.2837, loss_cls_lane: 0.6328, loss_cls_H: 0.6328, loss_bbox_lane: 6.2754, loss_bbox_H: 6.2964, d0.loss_cls_lane: 0.6328, d0.loss_cls_H: 0.6328, d0.loss_bbox_lane: 6.2754, d0.loss_bbox_H: 6.2964, d1.loss_cls_lane: 0.6328, d1.loss_cls_H: 0.6328, d1.loss_bbox_lane: 6.2754, d1.loss_bbox_H: 6.2964, d2.loss_cls_lane: 0.6328, d2.loss_cls_H: 0.6328, d2.loss_bbox_lane: 6.2754, d2.loss_bbox_H: 6.2964, d3.loss_cls_lane: 0.6328, d3.loss_cls_H: 0.6328, d3.loss_bbox_lane: 6.2754, d3.loss_bbox_H: 6.2964, d4.loss_cls_lane: 0.6328, d4.loss_cls_H: 0.6328, d4.loss_bbox_lane: 6.2754, d4.loss_bbox_H: 6.2964, vlm_loss: 0.0000, loss: 128.1032, grad_norm: nan
2025-02-28 23:51:37,559 - mmdet - INFO - Iter [2400/168780]	lr: 3.998e-04, eta: 5 days, 20:40:29, time: 2.957, data_time: 0.019, memory: 22027, loss_cls: 1.3811, loss_bbox: 2.7806, d0.loss_cls: 1.3811, d0.loss_bbox: 2.7806, d1.loss_cls: 1.3811, d1.loss_bbox: 2.7806, d2.loss_cls: 1.3811, d2.loss_bbox: 2.7806, d3.loss_cls: 1.3811, d3.loss_bbox: 2.7806, d4.loss_cls: 1.3811, d4.loss_bbox: 2.7806, dn_loss_cls: 0.9165, dn_loss_bbox: 2.5662, d0.dn_loss_cls: 0.9165, d0.dn_loss_bbox: 2.5662, d1.dn_loss_cls: 0.9165, d1.dn_loss_bbox: 2.5662, d2.dn_loss_cls: 0.9165, d2.dn_loss_bbox: 2.5662, d3.dn_loss_cls: 0.9165, d3.dn_loss_bbox: 2.5662, d4.dn_loss_cls: 0.9165, d4.dn_loss_bbox: 2.5662, loss_cls_lane: 0.6361, loss_cls_H: 0.6361, loss_bbox_lane: 6.5380, loss_bbox_H: 6.5466, d0.loss_cls_lane: 0.6361, d0.loss_cls_H: 0.6361, d0.loss_bbox_lane: 6.5380, d0.loss_bbox_H: 6.5466, d1.loss_cls_lane: 0.6361, d1.loss_cls_H: 0.6361, d1.loss_bbox_lane: 6.5380, d1.loss_bbox_H: 6.5466, d2.loss_cls_lane: 0.6361, d2.loss_cls_H: 0.6361, d2.loss_bbox_lane: 6.5380, d2.loss_bbox_H: 6.5466, d3.loss_cls_lane: 0.6361, d3.loss_cls_H: 0.6361, d3.loss_bbox_lane: 6.5380, d3.loss_bbox_H: 6.5466, d4.loss_cls_lane: 0.6361, d4.loss_cls_H: 0.6361, d4.loss_bbox_lane: 6.5380, d4.loss_bbox_H: 6.5466, vlm_loss: 0.0000, loss: 132.0072, grad_norm: nan
2025-02-28 23:54:24,408 - mmdet - INFO - Iter [2450/168780]	lr: 3.998e-04, eta: 5 days, 20:54:32, time: 3.337, data_time: 0.019, memory: 22027, loss_cls: 1.2760, loss_bbox: 2.6169, d0.loss_cls: 1.2760, d0.loss_bbox: 2.6169, d1.loss_cls: 1.2760, d1.loss_bbox: 2.6169, d2.loss_cls: 1.2760, d2.loss_bbox: 2.6169, d3.loss_cls: 1.2760, d3.loss_bbox: 2.6169, d4.loss_cls: 1.2760, d4.loss_bbox: 2.6169, dn_loss_cls: 0.8873, dn_loss_bbox: 2.2375, d0.dn_loss_cls: 0.8873, d0.dn_loss_bbox: 2.2375, d1.dn_loss_cls: 0.8873, d1.dn_loss_bbox: 2.2375, d2.dn_loss_cls: 0.8873, d2.dn_loss_bbox: 2.2375, d3.dn_loss_cls: 0.8873, d3.dn_loss_bbox: 2.2375, d4.dn_loss_cls: 0.8873, d4.dn_loss_bbox: 2.2375, loss_cls_lane: 0.6620, loss_cls_H: 0.6620, loss_bbox_lane: 6.4490, loss_bbox_H: 6.4332, d0.loss_cls_lane: 0.6620, d0.loss_cls_H: 0.6620, d0.loss_bbox_lane: 6.4490, d0.loss_bbox_H: 6.4332, d1.loss_cls_lane: 0.6620, d1.loss_cls_H: 0.6620, d1.loss_bbox_lane: 6.4490, d1.loss_bbox_H: 6.4332, d2.loss_cls_lane: 0.6620, d2.loss_cls_H: 0.6620, d2.loss_bbox_lane: 6.4490, d2.loss_bbox_H: 6.4332, d3.loss_cls_lane: 0.6620, d3.loss_cls_H: 0.6620, d3.loss_bbox_lane: 6.4490, d3.loss_bbox_H: 6.4332, d4.loss_cls_lane: 0.6620, d4.loss_cls_H: 0.6620, d4.loss_bbox_lane: 6.4490, d4.loss_bbox_H: 6.4332, vlm_loss: 0.0000, loss: 127.3440, grad_norm: nan
2025-02-28 23:56:55,318 - mmdet - INFO - Iter [2500/168780]	lr: 3.998e-04, eta: 5 days, 20:50:15, time: 3.018, data_time: 0.019, memory: 22027, loss_cls: 1.4158, loss_bbox: 2.8497, d0.loss_cls: 1.4158, d0.loss_bbox: 2.8497, d1.loss_cls: 1.4158, d1.loss_bbox: 2.8497, d2.loss_cls: 1.4158, d2.loss_bbox: 2.8497, d3.loss_cls: 1.4158, d3.loss_bbox: 2.8497, d4.loss_cls: 1.4158, d4.loss_bbox: 2.8497, dn_loss_cls: 0.9704, dn_loss_bbox: 2.4752, d0.dn_loss_cls: 0.9704, d0.dn_loss_bbox: 2.4752, d1.dn_loss_cls: 0.9704, d1.dn_loss_bbox: 2.4752, d2.dn_loss_cls: 0.9704, d2.dn_loss_bbox: 2.4752, d3.dn_loss_cls: 0.9704, d3.dn_loss_bbox: 2.4752, d4.dn_loss_cls: 0.9704, d4.dn_loss_bbox: 2.4752, loss_cls_lane: 0.6280, loss_cls_H: 0.6280, loss_bbox_lane: 6.4737, loss_bbox_H: 6.4731, d0.loss_cls_lane: 0.6280, d0.loss_cls_H: 0.6280, d0.loss_bbox_lane: 6.4737, d0.loss_bbox_H: 6.4731, d1.loss_cls_lane: 0.6280, d1.loss_cls_H: 0.6280, d1.loss_bbox_lane: 6.4737, d1.loss_bbox_H: 6.4731, d2.loss_cls_lane: 0.6280, d2.loss_cls_H: 0.6280, d2.loss_bbox_lane: 6.4737, d2.loss_bbox_H: 6.4731, d3.loss_cls_lane: 0.6280, d3.loss_cls_H: 0.6280, d3.loss_bbox_lane: 6.4737, d3.loss_bbox_H: 6.4731, d4.loss_cls_lane: 0.6280, d4.loss_cls_H: 0.6280, d4.loss_bbox_lane: 6.4737, d4.loss_bbox_H: 6.4731, vlm_loss: 0.0000, loss: 131.4840, grad_norm: nan
2025-02-28 23:59:23,857 - mmdet - INFO - Iter [2550/168780]	lr: 3.998e-04, eta: 5 days, 20:43:27, time: 2.971, data_time: 0.018, memory: 22027, loss_cls: 1.4536, loss_bbox: 2.6117, d0.loss_cls: 1.4536, d0.loss_bbox: 2.6117, d1.loss_cls: 1.4536, d1.loss_bbox: 2.6117, d2.loss_cls: 1.4536, d2.loss_bbox: 2.6117, d3.loss_cls: 1.4536, d3.loss_bbox: 2.6117, d4.loss_cls: 1.4536, d4.loss_bbox: 2.6117, dn_loss_cls: 0.9089, dn_loss_bbox: 2.2319, d0.dn_loss_cls: 0.9089, d0.dn_loss_bbox: 2.2319, d1.dn_loss_cls: 0.9089, d1.dn_loss_bbox: 2.2319, d2.dn_loss_cls: 0.9089, d2.dn_loss_bbox: 2.2319, d3.dn_loss_cls: 0.9089, d3.dn_loss_bbox: 2.2319, d4.dn_loss_cls: 0.9089, d4.dn_loss_bbox: 2.2319, loss_cls_lane: 0.6418, loss_cls_H: 0.6418, loss_bbox_lane: 6.5367, loss_bbox_H: 6.5281, d0.loss_cls_lane: 0.6418, d0.loss_cls_H: 0.6418, d0.loss_bbox_lane: 6.5367, d0.loss_bbox_H: 6.5281, d1.loss_cls_lane: 0.6418, d1.loss_cls_H: 0.6418, d1.loss_bbox_lane: 6.5367, d1.loss_bbox_H: 6.5281, d2.loss_cls_lane: 0.6418, d2.loss_cls_H: 0.6418, d2.loss_bbox_lane: 6.5367, d2.loss_bbox_H: 6.5281, d3.loss_cls_lane: 0.6418, d3.loss_cls_H: 0.6418, d3.loss_bbox_lane: 6.5367, d3.loss_bbox_H: 6.5281, d4.loss_cls_lane: 0.6418, d4.loss_cls_H: 0.6418, d4.loss_bbox_lane: 6.5367, d4.loss_bbox_H: 6.5281, vlm_loss: 0.0000, loss: 129.3261, grad_norm: nan
2025-03-01 00:01:52,105 - mmdet - INFO - Iter [2600/168780]	lr: 3.998e-04, eta: 5 days, 20:36:30, time: 2.965, data_time: 0.019, memory: 22027, loss_cls: 1.6762, loss_bbox: 2.9735, d0.loss_cls: 1.6762, d0.loss_bbox: 2.9735, d1.loss_cls: 1.6762, d1.loss_bbox: 2.9735, d2.loss_cls: 1.6762, d2.loss_bbox: 2.9735, d3.loss_cls: 1.6762, d3.loss_bbox: 2.9735, d4.loss_cls: 1.6762, d4.loss_bbox: 2.9735, dn_loss_cls: 0.9239, dn_loss_bbox: 2.5242, d0.dn_loss_cls: 0.9239, d0.dn_loss_bbox: 2.5242, d1.dn_loss_cls: 0.9239, d1.dn_loss_bbox: 2.5242, d2.dn_loss_cls: 0.9239, d2.dn_loss_bbox: 2.5242, d3.dn_loss_cls: 0.9239, d3.dn_loss_bbox: 2.5242, d4.dn_loss_cls: 0.9239, d4.dn_loss_bbox: 2.5242, loss_cls_lane: 0.6431, loss_cls_H: 0.7506, loss_bbox_lane: 6.2634, loss_bbox_H: 6.2567, d0.loss_cls_lane: 0.6431, d0.loss_cls_H: 0.7506, d0.loss_bbox_lane: 6.2634, d0.loss_bbox_H: 6.2567, d1.loss_cls_lane: 0.6431, d1.loss_cls_H: 0.7506, d1.loss_bbox_lane: 6.2634, d1.loss_bbox_H: 6.2567, d2.loss_cls_lane: 0.6431, d2.loss_cls_H: 0.7506, d2.loss_bbox_lane: 6.2634, d2.loss_bbox_H: 6.2567, d3.loss_cls_lane: 0.6431, d3.loss_cls_H: 0.7506, d3.loss_bbox_lane: 6.2634, d3.loss_bbox_H: 6.2567, d4.loss_cls_lane: 0.6431, d4.loss_cls_H: 0.7506, d4.loss_bbox_lane: 6.2634, d4.loss_bbox_H: 6.2567, vlm_loss: 0.0000, loss: 132.0698, grad_norm: nan
2025-03-01 00:04:22,134 - mmdet - INFO - Iter [2650/168780]	lr: 3.998e-04, eta: 5 days, 20:31:35, time: 3.001, data_time: 0.017, memory: 22027, loss_cls: 1.4030, loss_bbox: 2.7784, d0.loss_cls: 1.4030, d0.loss_bbox: 2.7784, d1.loss_cls: 1.4030, d1.loss_bbox: 2.7784, d2.loss_cls: 1.4030, d2.loss_bbox: 2.7784, d3.loss_cls: 1.4030, d3.loss_bbox: 2.7784, d4.loss_cls: 1.4030, d4.loss_bbox: 2.7784, dn_loss_cls: 0.9544, dn_loss_bbox: 2.3411, d0.dn_loss_cls: 0.9544, d0.dn_loss_bbox: 2.3411, d1.dn_loss_cls: 0.9544, d1.dn_loss_bbox: 2.3411, d2.dn_loss_cls: 0.9544, d2.dn_loss_bbox: 2.3411, d3.dn_loss_cls: 0.9544, d3.dn_loss_bbox: 2.3411, d4.dn_loss_cls: 0.9544, d4.dn_loss_bbox: 2.3411, loss_cls_lane: 0.7099, loss_cls_H: 0.7099, loss_bbox_lane: 7.3460, loss_bbox_H: 7.3337, d0.loss_cls_lane: 0.7099, d0.loss_cls_H: 0.7099, d0.loss_bbox_lane: 7.3460, d0.loss_bbox_H: 7.3337, d1.loss_cls_lane: 0.7099, d1.loss_cls_H: 0.7099, d1.loss_bbox_lane: 7.3460, d1.loss_bbox_H: 7.3337, d2.loss_cls_lane: 0.7099, d2.loss_cls_H: 0.7099, d2.loss_bbox_lane: 7.3460, d2.loss_bbox_H: 7.3337, d3.loss_cls_lane: 0.7099, d3.loss_cls_H: 0.7099, d3.loss_bbox_lane: 7.3460, d3.loss_bbox_H: 7.3337, d4.loss_cls_lane: 0.7099, d4.loss_cls_H: 0.7099, d4.loss_bbox_lane: 7.3460, d4.loss_bbox_H: 7.3337, vlm_loss: 0.0000, loss: 141.4580, grad_norm: nan
2025-03-01 00:06:49,503 - mmdet - INFO - Iter [2700/168780]	lr: 3.997e-04, eta: 5 days, 20:24:02, time: 2.947, data_time: 0.018, memory: 22027, loss_cls: 1.7097, loss_bbox: 2.7854, d0.loss_cls: 1.7097, d0.loss_bbox: 2.7854, d1.loss_cls: 1.7097, d1.loss_bbox: 2.7854, d2.loss_cls: 1.7097, d2.loss_bbox: 2.7854, d3.loss_cls: 1.7097, d3.loss_bbox: 2.7854, d4.loss_cls: 1.7097, d4.loss_bbox: 2.7854, dn_loss_cls: 0.9832, dn_loss_bbox: 2.5069, d0.dn_loss_cls: 0.9832, d0.dn_loss_bbox: 2.5069, d1.dn_loss_cls: 0.9832, d1.dn_loss_bbox: 2.5069, d2.dn_loss_cls: 0.9832, d2.dn_loss_bbox: 2.5069, d3.dn_loss_cls: 0.9832, d3.dn_loss_bbox: 2.5069, d4.dn_loss_cls: 0.9832, d4.dn_loss_bbox: 2.5069, loss_cls_lane: 0.6470, loss_cls_H: 0.6470, loss_bbox_lane: 6.7490, loss_bbox_H: 6.7366, d0.loss_cls_lane: 0.6470, d0.loss_cls_H: 0.6470, d0.loss_bbox_lane: 6.7490, d0.loss_bbox_H: 6.7366, d1.loss_cls_lane: 0.6470, d1.loss_cls_H: 0.6470, d1.loss_bbox_lane: 6.7490, d1.loss_bbox_H: 6.7366, d2.loss_cls_lane: 0.6470, d2.loss_cls_H: 0.6470, d2.loss_bbox_lane: 6.7490, d2.loss_bbox_H: 6.7366, d3.loss_cls_lane: 0.6470, d3.loss_cls_H: 0.6470, d3.loss_bbox_lane: 6.7490, d3.loss_bbox_H: 6.7366, d4.loss_cls_lane: 0.6470, d4.loss_cls_H: 0.6470, d4.loss_bbox_lane: 6.7490, d4.loss_bbox_H: 6.7366, vlm_loss: 0.0000, loss: 136.5897, grad_norm: nan
2025-03-01 00:09:36,291 - mmdet - INFO - Iter [2750/168780]	lr: 3.997e-04, eta: 5 days, 20:36:13, time: 3.336, data_time: 0.018, memory: 22027, loss_cls: 1.4010, loss_bbox: 2.9219, d0.loss_cls: 1.4010, d0.loss_bbox: 2.9219, d1.loss_cls: 1.4010, d1.loss_bbox: 2.9219, d2.loss_cls: 1.4010, d2.loss_bbox: 2.9219, d3.loss_cls: 1.4010, d3.loss_bbox: 2.9219, d4.loss_cls: 1.4010, d4.loss_bbox: 2.9219, dn_loss_cls: 0.9168, dn_loss_bbox: 2.5604, d0.dn_loss_cls: 0.9168, d0.dn_loss_bbox: 2.5604, d1.dn_loss_cls: 0.9168, d1.dn_loss_bbox: 2.5604, d2.dn_loss_cls: 0.9168, d2.dn_loss_bbox: 2.5604, d3.dn_loss_cls: 0.9168, d3.dn_loss_bbox: 2.5604, d4.dn_loss_cls: 0.9168, d4.dn_loss_bbox: 2.5604, loss_cls_lane: 0.6259, loss_cls_H: 0.6259, loss_bbox_lane: 6.5004, loss_bbox_H: 6.4922, d0.loss_cls_lane: 0.6259, d0.loss_cls_H: 0.6259, d0.loss_bbox_lane: 6.5004, d0.loss_bbox_H: 6.4922, d1.loss_cls_lane: 0.6259, d1.loss_cls_H: 0.6259, d1.loss_bbox_lane: 6.5004, d1.loss_bbox_H: 6.4922, d2.loss_cls_lane: 0.6259, d2.loss_cls_H: 0.6259, d2.loss_bbox_lane: 6.5004, d2.loss_bbox_H: 6.4922, d3.loss_cls_lane: 0.6259, d3.loss_cls_H: 0.6259, d3.loss_bbox_lane: 6.5004, d3.loss_bbox_H: 6.4922, d4.loss_cls_lane: 0.6259, d4.loss_cls_H: 0.6259, d4.loss_bbox_lane: 6.5004, d4.loss_bbox_H: 6.4922, vlm_loss: 0.0000, loss: 132.2668, grad_norm: nan
2025-03-01 00:12:06,131 - mmdet - INFO - Iter [2800/168780]	lr: 3.997e-04, eta: 5 days, 20:31:06, time: 2.997, data_time: 0.019, memory: 22027, loss_cls: 1.3977, loss_bbox: 2.9692, d0.loss_cls: 1.3977, d0.loss_bbox: 2.9692, d1.loss_cls: 1.3977, d1.loss_bbox: 2.9692, d2.loss_cls: 1.3977, d2.loss_bbox: 2.9692, d3.loss_cls: 1.3977, d3.loss_bbox: 2.9692, d4.loss_cls: 1.3977, d4.loss_bbox: 2.9692, dn_loss_cls: 0.9265, dn_loss_bbox: 2.5734, d0.dn_loss_cls: 0.9265, d0.dn_loss_bbox: 2.5734, d1.dn_loss_cls: 0.9265, d1.dn_loss_bbox: 2.5734, d2.dn_loss_cls: 0.9265, d2.dn_loss_bbox: 2.5734, d3.dn_loss_cls: 0.9265, d3.dn_loss_bbox: 2.5734, d4.dn_loss_cls: 0.9265, d4.dn_loss_bbox: 2.5734, loss_cls_lane: 0.6311, loss_cls_H: 0.6311, loss_bbox_lane: 6.5472, loss_bbox_H: 6.5317, d0.loss_cls_lane: 0.6311, d0.loss_cls_H: 0.6311, d0.loss_bbox_lane: 6.5472, d0.loss_bbox_H: 6.5317, d1.loss_cls_lane: 0.6311, d1.loss_cls_H: 0.6311, d1.loss_bbox_lane: 6.5472, d1.loss_bbox_H: 6.5317, d2.loss_cls_lane: 0.6311, d2.loss_cls_H: 0.6311, d2.loss_bbox_lane: 6.5472, d2.loss_bbox_H: 6.5317, d3.loss_cls_lane: 0.6311, d3.loss_cls_H: 0.6311, d3.loss_bbox_lane: 6.5472, d3.loss_bbox_H: 6.5317, d4.loss_cls_lane: 0.6311, d4.loss_cls_H: 0.6311, d4.loss_bbox_lane: 6.5472, d4.loss_bbox_H: 6.5317, vlm_loss: 0.0000, loss: 133.2476, grad_norm: nan
2025-03-01 00:14:35,206 - mmdet - INFO - Iter [2850/168780]	lr: 3.997e-04, eta: 5 days, 20:25:21, time: 2.982, data_time: 0.019, memory: 22027, loss_cls: 1.3792, loss_bbox: 2.6085, d0.loss_cls: 1.3792, d0.loss_bbox: 2.6085, d1.loss_cls: 1.3792, d1.loss_bbox: 2.6085, d2.loss_cls: 1.3792, d2.loss_bbox: 2.6085, d3.loss_cls: 1.3792, d3.loss_bbox: 2.6085, d4.loss_cls: 1.3792, d4.loss_bbox: 2.6085, dn_loss_cls: 0.8707, dn_loss_bbox: 2.2389, d0.dn_loss_cls: 0.8707, d0.dn_loss_bbox: 2.2389, d1.dn_loss_cls: 0.8707, d1.dn_loss_bbox: 2.2389, d2.dn_loss_cls: 0.8707, d2.dn_loss_bbox: 2.2389, d3.dn_loss_cls: 0.8707, d3.dn_loss_bbox: 2.2389, d4.dn_loss_cls: 0.8707, d4.dn_loss_bbox: 2.2389, loss_cls_lane: 0.6331, loss_cls_H: 0.6331, loss_bbox_lane: 6.5878, loss_bbox_H: 6.5708, d0.loss_cls_lane: 0.6331, d0.loss_cls_H: 0.6331, d0.loss_bbox_lane: 6.5878, d0.loss_bbox_H: 6.5708, d1.loss_cls_lane: 0.6331, d1.loss_cls_H: 0.6331, d1.loss_bbox_lane: 6.5878, d1.loss_bbox_H: 6.5708, d2.loss_cls_lane: 0.6331, d2.loss_cls_H: 0.6331, d2.loss_bbox_lane: 6.5878, d2.loss_bbox_H: 6.5708, d3.loss_cls_lane: 0.6331, d3.loss_cls_H: 0.6331, d3.loss_bbox_lane: 6.5878, d3.loss_bbox_H: 6.5708, d4.loss_cls_lane: 0.6331, d4.loss_cls_H: 0.6331, d4.loss_bbox_lane: 6.5878, d4.loss_bbox_H: 6.5708, vlm_loss: 0.0000, loss: 129.1322, grad_norm: nan
2025-03-01 00:17:03,888 - mmdet - INFO - Iter [2900/168780]	lr: 3.997e-04, eta: 5 days, 20:19:20, time: 2.974, data_time: 0.018, memory: 22027, loss_cls: 1.7963, loss_bbox: 2.9434, d0.loss_cls: 1.7963, d0.loss_bbox: 2.9434, d1.loss_cls: 1.7963, d1.loss_bbox: 2.9434, d2.loss_cls: 1.7963, d2.loss_bbox: 2.9434, d3.loss_cls: 1.7963, d3.loss_bbox: 2.9434, d4.loss_cls: 1.7963, d4.loss_bbox: 2.9434, dn_loss_cls: 0.9360, dn_loss_bbox: 2.6587, d0.dn_loss_cls: 0.9360, d0.dn_loss_bbox: 2.6587, d1.dn_loss_cls: 0.9360, d1.dn_loss_bbox: 2.6587, d2.dn_loss_cls: 0.9360, d2.dn_loss_bbox: 2.6587, d3.dn_loss_cls: 0.9360, d3.dn_loss_bbox: 2.6587, d4.dn_loss_cls: 0.9360, d4.dn_loss_bbox: 2.6587, loss_cls_lane: 0.6280, loss_cls_H: 0.6280, loss_bbox_lane: 6.2715, loss_bbox_H: 6.2715, d0.loss_cls_lane: 0.6280, d0.loss_cls_H: 0.6280, d0.loss_bbox_lane: 6.2715, d0.loss_bbox_H: 6.2715, d1.loss_cls_lane: 0.6280, d1.loss_cls_H: 0.6280, d1.loss_bbox_lane: 6.2715, d1.loss_bbox_H: 6.2715, d2.loss_cls_lane: 0.6280, d2.loss_cls_H: 0.6280, d2.loss_bbox_lane: 6.2715, d2.loss_bbox_H: 6.2715, d3.loss_cls_lane: 0.6280, d3.loss_cls_H: 0.6280, d3.loss_bbox_lane: 6.2715, d3.loss_bbox_H: 6.2715, d4.loss_cls_lane: 0.6280, d4.loss_cls_H: 0.6280, d4.loss_bbox_lane: 6.2715, d4.loss_bbox_H: 6.2715, vlm_loss: 0.0000, loss: 132.8001, grad_norm: nan
2025-03-01 00:19:32,618 - mmdet - INFO - Iter [2950/168780]	lr: 3.997e-04, eta: 5 days, 20:13:29, time: 2.975, data_time: 0.018, memory: 22027, loss_cls: 1.3607, loss_bbox: 2.8777, d0.loss_cls: 1.3607, d0.loss_bbox: 2.8777, d1.loss_cls: 1.3607, d1.loss_bbox: 2.8777, d2.loss_cls: 1.3607, d2.loss_bbox: 2.8777, d3.loss_cls: 1.3607, d3.loss_bbox: 2.8777, d4.loss_cls: 1.3607, d4.loss_bbox: 2.8777, dn_loss_cls: 0.8352, dn_loss_bbox: 2.6192, d0.dn_loss_cls: 0.8352, d0.dn_loss_bbox: 2.6192, d1.dn_loss_cls: 0.8352, d1.dn_loss_bbox: 2.6192, d2.dn_loss_cls: 0.8352, d2.dn_loss_bbox: 2.6192, d3.dn_loss_cls: 0.8352, d3.dn_loss_bbox: 2.6192, d4.dn_loss_cls: 0.8352, d4.dn_loss_bbox: 2.6192, loss_cls_lane: 0.6354, loss_cls_H: 0.6354, loss_bbox_lane: 6.6637, loss_bbox_H: 6.6625, d0.loss_cls_lane: 0.6354, d0.loss_cls_H: 0.6354, d0.loss_bbox_lane: 6.6637, d0.loss_bbox_H: 6.6625, d1.loss_cls_lane: 0.6354, d1.loss_cls_H: 0.6354, d1.loss_bbox_lane: 6.6637, d1.loss_bbox_H: 6.6625, d2.loss_cls_lane: 0.6354, d2.loss_cls_H: 0.6354, d2.loss_bbox_lane: 6.6637, d2.loss_bbox_H: 6.6625, d3.loss_cls_lane: 0.6354, d3.loss_cls_H: 0.6354, d3.loss_bbox_lane: 6.6637, d3.loss_bbox_H: 6.6625, d4.loss_cls_lane: 0.6354, d4.loss_cls_H: 0.6354, d4.loss_bbox_lane: 6.6637, d4.loss_bbox_H: 6.6625, vlm_loss: 0.0000, loss: 133.7383, grad_norm: nan
2025-03-01 00:22:01,934 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 00:22:01,934 - mmdet - INFO - Iter [3000/168780]	lr: 3.997e-04, eta: 5 days, 20:08:17, time: 2.986, data_time: 0.017, memory: 22027, loss_cls: 1.6917, loss_bbox: 2.8324, d0.loss_cls: 1.6917, d0.loss_bbox: 2.8324, d1.loss_cls: 1.6917, d1.loss_bbox: 2.8324, d2.loss_cls: 1.6917, d2.loss_bbox: 2.8324, d3.loss_cls: 1.6917, d3.loss_bbox: 2.8324, d4.loss_cls: 1.6917, d4.loss_bbox: 2.8324, dn_loss_cls: 0.9946, dn_loss_bbox: 2.5390, d0.dn_loss_cls: 0.9946, d0.dn_loss_bbox: 2.5390, d1.dn_loss_cls: 0.9946, d1.dn_loss_bbox: 2.5390, d2.dn_loss_cls: 0.9946, d2.dn_loss_bbox: 2.5390, d3.dn_loss_cls: 0.9946, d3.dn_loss_bbox: 2.5390, d4.dn_loss_cls: 0.9946, d4.dn_loss_bbox: 2.5390, loss_cls_lane: 0.6479, loss_cls_H: 0.6479, loss_bbox_lane: 6.6111, loss_bbox_H: 6.6134, d0.loss_cls_lane: 0.6479, d0.loss_cls_H: 0.6479, d0.loss_bbox_lane: 6.6111, d0.loss_bbox_H: 6.6134, d1.loss_cls_lane: 0.6479, d1.loss_cls_H: 0.6479, d1.loss_bbox_lane: 6.6111, d1.loss_bbox_H: 6.6134, d2.loss_cls_lane: 0.6479, d2.loss_cls_H: 0.6479, d2.loss_bbox_lane: 6.6111, d2.loss_bbox_H: 6.6134, d3.loss_cls_lane: 0.6479, d3.loss_cls_H: 0.6479, d3.loss_bbox_lane: 6.6111, d3.loss_bbox_H: 6.6134, d4.loss_cls_lane: 0.6479, d4.loss_cls_H: 0.6479, d4.loss_bbox_lane: 6.6111, d4.loss_bbox_H: 6.6134, vlm_loss: 0.0000, loss: 135.4680, grad_norm: nan
2025-03-01 00:24:24,927 - mmdet - INFO - Iter [3050/168780]	lr: 3.997e-04, eta: 5 days, 19:57:27, time: 2.860, data_time: 0.017, memory: 22027, loss_cls: 2.4423, loss_bbox: 2.9067, d0.loss_cls: 2.4423, d0.loss_bbox: 2.9067, d1.loss_cls: 2.4423, d1.loss_bbox: 2.9067, d2.loss_cls: 2.4423, d2.loss_bbox: 2.9067, d3.loss_cls: 2.4423, d3.loss_bbox: 2.9067, d4.loss_cls: 2.4423, d4.loss_bbox: 2.9067, dn_loss_cls: 0.9397, dn_loss_bbox: 2.6107, d0.dn_loss_cls: 0.9397, d0.dn_loss_bbox: 2.6107, d1.dn_loss_cls: 0.9397, d1.dn_loss_bbox: 2.6107, d2.dn_loss_cls: 0.9397, d2.dn_loss_bbox: 2.6107, d3.dn_loss_cls: 0.9397, d3.dn_loss_bbox: 2.6107, d4.dn_loss_cls: 0.9397, d4.dn_loss_bbox: 2.6107, loss_cls_lane: 0.6472, loss_cls_H: 0.9697, loss_bbox_lane: 5.4820, loss_bbox_H: 5.4862, d0.loss_cls_lane: 0.6472, d0.loss_cls_H: 0.9697, d0.loss_bbox_lane: 5.4820, d0.loss_bbox_H: 5.4862, d1.loss_cls_lane: 0.6472, d1.loss_cls_H: 0.9697, d1.loss_bbox_lane: 5.4820, d1.loss_bbox_H: 5.4862, d2.loss_cls_lane: 0.6472, d2.loss_cls_H: 0.9697, d2.loss_bbox_lane: 5.4820, d2.loss_bbox_H: 5.4862, d3.loss_cls_lane: 0.6472, d3.loss_cls_H: 0.9697, d3.loss_bbox_lane: 5.4820, d3.loss_bbox_H: 5.4862, d4.loss_cls_lane: 0.6472, d4.loss_cls_H: 0.9697, d4.loss_bbox_lane: 5.4820, d4.loss_bbox_H: 5.4862, vlm_loss: 0.0000, loss: 128.9075, grad_norm: nan
2025-03-01 00:26:51,415 - mmdet - INFO - Iter [3100/168780]	lr: 3.997e-04, eta: 5 days, 19:50:00, time: 2.930, data_time: 0.018, memory: 22027, loss_cls: 1.9968, loss_bbox: 2.7587, d0.loss_cls: 1.9968, d0.loss_bbox: 2.7587, d1.loss_cls: 1.9968, d1.loss_bbox: 2.7587, d2.loss_cls: 1.9968, d2.loss_bbox: 2.7587, d3.loss_cls: 1.9968, d3.loss_bbox: 2.7587, d4.loss_cls: 1.9968, d4.loss_bbox: 2.7587, dn_loss_cls: 0.8702, dn_loss_bbox: 2.4722, d0.dn_loss_cls: 0.8702, d0.dn_loss_bbox: 2.4722, d1.dn_loss_cls: 0.8702, d1.dn_loss_bbox: 2.4722, d2.dn_loss_cls: 0.8702, d2.dn_loss_bbox: 2.4722, d3.dn_loss_cls: 0.8702, d3.dn_loss_bbox: 2.4722, d4.dn_loss_cls: 0.8702, d4.dn_loss_bbox: 2.4722, loss_cls_lane: 0.6376, loss_cls_H: 0.6376, loss_bbox_lane: 6.1467, loss_bbox_H: 6.1620, d0.loss_cls_lane: 0.6376, d0.loss_cls_H: 0.6376, d0.loss_bbox_lane: 6.1467, d0.loss_bbox_H: 6.1620, d1.loss_cls_lane: 0.6376, d1.loss_cls_H: 0.6376, d1.loss_bbox_lane: 6.1467, d1.loss_bbox_H: 6.1620, d2.loss_cls_lane: 0.6376, d2.loss_cls_H: 0.6376, d2.loss_bbox_lane: 6.1467, d2.loss_bbox_H: 6.1620, d3.loss_cls_lane: 0.6376, d3.loss_cls_H: 0.6376, d3.loss_bbox_lane: 6.1467, d3.loss_bbox_H: 6.1620, d4.loss_cls_lane: 0.6376, d4.loss_cls_H: 0.6376, d4.loss_bbox_lane: 6.1467, d4.loss_bbox_H: 6.1620, vlm_loss: 0.0000, loss: 130.0912, grad_norm: nan
2025-03-01 00:29:19,246 - mmdet - INFO - Iter [3150/168780]	lr: 3.997e-04, eta: 5 days, 19:43:53, time: 2.957, data_time: 0.017, memory: 22027, loss_cls: 2.2790, loss_bbox: 3.1345, d0.loss_cls: 2.2790, d0.loss_bbox: 3.1345, d1.loss_cls: 2.2790, d1.loss_bbox: 3.1345, d2.loss_cls: 2.2790, d2.loss_bbox: 3.1345, d3.loss_cls: 2.2790, d3.loss_bbox: 3.1345, d4.loss_cls: 2.2790, d4.loss_bbox: 3.1345, dn_loss_cls: 0.9653, dn_loss_bbox: 2.9151, d0.dn_loss_cls: 0.9653, d0.dn_loss_bbox: 2.9151, d1.dn_loss_cls: 0.9653, d1.dn_loss_bbox: 2.9151, d2.dn_loss_cls: 0.9653, d2.dn_loss_bbox: 2.9151, d3.dn_loss_cls: 0.9653, d3.dn_loss_bbox: 2.9151, d4.dn_loss_cls: 0.9653, d4.dn_loss_bbox: 2.9151, loss_cls_lane: 0.6320, loss_cls_H: 0.6320, loss_bbox_lane: 6.7424, loss_bbox_H: 6.7289, d0.loss_cls_lane: 0.6320, d0.loss_cls_H: 0.6320, d0.loss_bbox_lane: 6.7424, d0.loss_bbox_H: 6.7289, d1.loss_cls_lane: 0.6320, d1.loss_cls_H: 0.6320, d1.loss_bbox_lane: 6.7424, d1.loss_bbox_H: 6.7289, d2.loss_cls_lane: 0.6320, d2.loss_cls_H: 0.6320, d2.loss_bbox_lane: 6.7424, d2.loss_bbox_H: 6.7289, d3.loss_cls_lane: 0.6320, d3.loss_cls_H: 0.6320, d3.loss_bbox_lane: 6.7424, d3.loss_bbox_H: 6.7289, d4.loss_cls_lane: 0.6320, d4.loss_cls_H: 0.6320, d4.loss_bbox_lane: 6.7424, d4.loss_bbox_H: 6.7289, vlm_loss: 0.0000, loss: 144.1744, grad_norm: nan
2025-03-01 00:31:46,199 - mmdet - INFO - Iter [3200/168780]	lr: 3.996e-04, eta: 5 days, 19:37:07, time: 2.939, data_time: 0.017, memory: 22027, loss_cls: 1.4005, loss_bbox: 2.7075, d0.loss_cls: 1.4005, d0.loss_bbox: 2.7075, d1.loss_cls: 1.4005, d1.loss_bbox: 2.7075, d2.loss_cls: 1.4005, d2.loss_bbox: 2.7075, d3.loss_cls: 1.4005, d3.loss_bbox: 2.7075, d4.loss_cls: 1.4005, d4.loss_bbox: 2.7075, dn_loss_cls: 0.8930, dn_loss_bbox: 2.3787, d0.dn_loss_cls: 0.8930, d0.dn_loss_bbox: 2.3787, d1.dn_loss_cls: 0.8930, d1.dn_loss_bbox: 2.3787, d2.dn_loss_cls: 0.8930, d2.dn_loss_bbox: 2.3787, d3.dn_loss_cls: 0.8930, d3.dn_loss_bbox: 2.3787, d4.dn_loss_cls: 0.8930, d4.dn_loss_bbox: 2.3787, loss_cls_lane: 0.6390, loss_cls_H: 0.6390, loss_bbox_lane: 6.6141, loss_bbox_H: 6.6065, d0.loss_cls_lane: 0.6390, d0.loss_cls_H: 0.6390, d0.loss_bbox_lane: 6.6141, d0.loss_bbox_H: 6.6065, d1.loss_cls_lane: 0.6390, d1.loss_cls_H: 0.6390, d1.loss_bbox_lane: 6.6141, d1.loss_bbox_H: 6.6065, d2.loss_cls_lane: 0.6390, d2.loss_cls_H: 0.6390, d2.loss_bbox_lane: 6.6141, d2.loss_bbox_H: 6.6065, d3.loss_cls_lane: 0.6390, d3.loss_cls_H: 0.6390, d3.loss_bbox_lane: 6.6141, d3.loss_bbox_H: 6.6065, d4.loss_cls_lane: 0.6390, d4.loss_cls_H: 0.6390, d4.loss_bbox_lane: 6.6141, d4.loss_bbox_H: 6.6065, vlm_loss: 0.0000, loss: 131.2694, grad_norm: nan
2025-03-01 00:34:11,712 - mmdet - INFO - Iter [3250/168780]	lr: 3.996e-04, eta: 5 days, 19:29:17, time: 2.910, data_time: 0.016, memory: 22027, loss_cls: 3.1820, loss_bbox: 3.0431, d0.loss_cls: 3.1820, d0.loss_bbox: 3.0431, d1.loss_cls: 3.1820, d1.loss_bbox: 3.0431, d2.loss_cls: 3.1820, d2.loss_bbox: 3.0431, d3.loss_cls: 3.1820, d3.loss_bbox: 3.0431, d4.loss_cls: 3.1820, d4.loss_bbox: 3.0431, dn_loss_cls: 1.0384, dn_loss_bbox: 2.7104, d0.dn_loss_cls: 1.0384, d0.dn_loss_bbox: 2.7104, d1.dn_loss_cls: 1.0384, d1.dn_loss_bbox: 2.7104, d2.dn_loss_cls: 1.0384, d2.dn_loss_bbox: 2.7104, d3.dn_loss_cls: 1.0384, d3.dn_loss_bbox: 2.7104, d4.dn_loss_cls: 1.0384, d4.dn_loss_bbox: 2.7104, loss_cls_lane: 0.6863, loss_cls_H: 0.6863, loss_bbox_lane: 6.8578, loss_bbox_H: 6.8417, d0.loss_cls_lane: 0.6863, d0.loss_cls_H: 0.6863, d0.loss_bbox_lane: 6.8578, d0.loss_bbox_H: 6.8417, d1.loss_cls_lane: 0.6863, d1.loss_cls_H: 0.6863, d1.loss_bbox_lane: 6.8578, d1.loss_bbox_H: 6.8417, d2.loss_cls_lane: 0.6863, d2.loss_cls_H: 0.6863, d2.loss_bbox_lane: 6.8578, d2.loss_bbox_H: 6.8417, d3.loss_cls_lane: 0.6863, d3.loss_cls_H: 0.6863, d3.loss_bbox_lane: 6.8578, d3.loss_bbox_H: 6.8417, d4.loss_cls_lane: 0.6863, d4.loss_cls_H: 0.6863, d4.loss_bbox_lane: 6.8578, d4.loss_bbox_H: 6.8417, vlm_loss: 0.0000, loss: 150.2751, grad_norm: nan
2025-03-01 00:36:37,355 - mmdet - INFO - Iter [3300/168780]	lr: 3.996e-04, eta: 5 days, 19:21:42, time: 2.913, data_time: 0.015, memory: 22027, loss_cls: 2.2813, loss_bbox: 3.0092, d0.loss_cls: 2.2813, d0.loss_bbox: 3.0092, d1.loss_cls: 2.2813, d1.loss_bbox: 3.0092, d2.loss_cls: 2.2813, d2.loss_bbox: 3.0092, d3.loss_cls: 2.2813, d3.loss_bbox: 3.0092, d4.loss_cls: 2.2813, d4.loss_bbox: 3.0092, dn_loss_cls: 0.9301, dn_loss_bbox: 2.6696, d0.dn_loss_cls: 0.9301, d0.dn_loss_bbox: 2.6696, d1.dn_loss_cls: 0.9301, d1.dn_loss_bbox: 2.6696, d2.dn_loss_cls: 0.9301, d2.dn_loss_bbox: 2.6696, d3.dn_loss_cls: 0.9301, d3.dn_loss_bbox: 2.6696, d4.dn_loss_cls: 0.9301, d4.dn_loss_bbox: 2.6696, loss_cls_lane: 0.7221, loss_cls_H: 0.7758, loss_bbox_lane: 6.8245, loss_bbox_H: 6.8047, d0.loss_cls_lane: 0.7221, d0.loss_cls_H: 0.7758, d0.loss_bbox_lane: 6.8245, d0.loss_bbox_H: 6.8047, d1.loss_cls_lane: 0.7221, d1.loss_cls_H: 0.7758, d1.loss_bbox_lane: 6.8245, d1.loss_bbox_H: 6.8047, d2.loss_cls_lane: 0.7221, d2.loss_cls_H: 0.7758, d2.loss_bbox_lane: 6.8245, d2.loss_bbox_H: 6.8047, d3.loss_cls_lane: 0.7221, d3.loss_cls_H: 0.7758, d3.loss_bbox_lane: 6.8245, d3.loss_bbox_H: 6.8047, d4.loss_cls_lane: 0.7221, d4.loss_cls_H: 0.7758, d4.loss_bbox_lane: 6.8245, d4.loss_bbox_H: 6.8047, vlm_loss: 0.0000, loss: 144.1035, grad_norm: nan
2025-03-01 00:39:21,587 - mmdet - INFO - Iter [3350/168780]	lr: 3.996e-04, eta: 5 days, 19:29:35, time: 3.285, data_time: 0.017, memory: 22027, loss_cls: 1.9997, loss_bbox: 2.7390, d0.loss_cls: 1.9997, d0.loss_bbox: 2.7390, d1.loss_cls: 1.9997, d1.loss_bbox: 2.7390, d2.loss_cls: 1.9997, d2.loss_bbox: 2.7390, d3.loss_cls: 1.9997, d3.loss_bbox: 2.7390, d4.loss_cls: 1.9997, d4.loss_bbox: 2.7390, dn_loss_cls: 1.0296, dn_loss_bbox: 2.4322, d0.dn_loss_cls: 1.0296, d0.dn_loss_bbox: 2.4322, d1.dn_loss_cls: 1.0296, d1.dn_loss_bbox: 2.4322, d2.dn_loss_cls: 1.0296, d2.dn_loss_bbox: 2.4322, d3.dn_loss_cls: 1.0296, d3.dn_loss_bbox: 2.4322, d4.dn_loss_cls: 1.0296, d4.dn_loss_bbox: 2.4322, loss_cls_lane: 0.6395, loss_cls_H: 0.6395, loss_bbox_lane: 6.4140, loss_bbox_H: 6.4109, d0.loss_cls_lane: 0.6395, d0.loss_cls_H: 0.6395, d0.loss_bbox_lane: 6.4140, d0.loss_bbox_H: 6.4109, d1.loss_cls_lane: 0.6395, d1.loss_cls_H: 0.6395, d1.loss_bbox_lane: 6.4140, d1.loss_bbox_H: 6.4109, d2.loss_cls_lane: 0.6395, d2.loss_cls_H: 0.6395, d2.loss_bbox_lane: 6.4140, d2.loss_bbox_H: 6.4109, d3.loss_cls_lane: 0.6395, d3.loss_cls_H: 0.6395, d3.loss_bbox_lane: 6.4140, d3.loss_bbox_H: 6.4109, d4.loss_cls_lane: 0.6395, d4.loss_cls_H: 0.6395, d4.loss_bbox_lane: 6.4140, d4.loss_bbox_H: 6.4109, vlm_loss: 0.0000, loss: 133.8252, grad_norm: nan
2025-03-01 00:41:49,584 - mmdet - INFO - Iter [3400/168780]	lr: 3.996e-04, eta: 5 days, 19:23:59, time: 2.960, data_time: 0.017, memory: 22027, loss_cls: 1.6338, loss_bbox: 2.8174, d0.loss_cls: 1.6338, d0.loss_bbox: 2.8174, d1.loss_cls: 1.6338, d1.loss_bbox: 2.8174, d2.loss_cls: 1.6338, d2.loss_bbox: 2.8174, d3.loss_cls: 1.6338, d3.loss_bbox: 2.8174, d4.loss_cls: 1.6338, d4.loss_bbox: 2.8174, dn_loss_cls: 0.9572, dn_loss_bbox: 2.4957, d0.dn_loss_cls: 0.9572, d0.dn_loss_bbox: 2.4957, d1.dn_loss_cls: 0.9572, d1.dn_loss_bbox: 2.4957, d2.dn_loss_cls: 0.9572, d2.dn_loss_bbox: 2.4957, d3.dn_loss_cls: 0.9572, d3.dn_loss_bbox: 2.4957, d4.dn_loss_cls: 0.9572, d4.dn_loss_bbox: 2.4957, loss_cls_lane: 0.6315, loss_cls_H: 0.6315, loss_bbox_lane: 6.3826, loss_bbox_H: 6.3652, d0.loss_cls_lane: 0.6315, d0.loss_cls_H: 0.6315, d0.loss_bbox_lane: 6.3826, d0.loss_bbox_H: 6.3652, d1.loss_cls_lane: 0.6315, d1.loss_cls_H: 0.6315, d1.loss_bbox_lane: 6.3826, d1.loss_bbox_H: 6.3652, d2.loss_cls_lane: 0.6315, d2.loss_cls_H: 0.6315, d2.loss_bbox_lane: 6.3826, d2.loss_bbox_H: 6.3652, d3.loss_cls_lane: 0.6315, d3.loss_cls_H: 0.6315, d3.loss_bbox_lane: 6.3826, d3.loss_bbox_H: 6.3652, d4.loss_cls_lane: 0.6315, d4.loss_cls_H: 0.6315, d4.loss_bbox_lane: 6.3826, d4.loss_bbox_H: 6.3652, vlm_loss: 0.0000, loss: 131.4894, grad_norm: nan
2025-03-01 00:44:18,489 - mmdet - INFO - Iter [3450/168780]	lr: 3.996e-04, eta: 5 days, 19:19:12, time: 2.978, data_time: 0.017, memory: 22027, loss_cls: 1.5056, loss_bbox: 2.7250, d0.loss_cls: 1.5056, d0.loss_bbox: 2.7250, d1.loss_cls: 1.5056, d1.loss_bbox: 2.7250, d2.loss_cls: 1.5056, d2.loss_bbox: 2.7250, d3.loss_cls: 1.5056, d3.loss_bbox: 2.7250, d4.loss_cls: 1.5056, d4.loss_bbox: 2.7250, dn_loss_cls: 0.8968, dn_loss_bbox: 2.3935, d0.dn_loss_cls: 0.8968, d0.dn_loss_bbox: 2.3935, d1.dn_loss_cls: 0.8968, d1.dn_loss_bbox: 2.3935, d2.dn_loss_cls: 0.8968, d2.dn_loss_bbox: 2.3935, d3.dn_loss_cls: 0.8968, d3.dn_loss_bbox: 2.3935, d4.dn_loss_cls: 0.8968, d4.dn_loss_bbox: 2.3935, loss_cls_lane: 0.6555, loss_cls_H: 0.6555, loss_bbox_lane: 6.5832, loss_bbox_H: 6.5729, d0.loss_cls_lane: 0.6555, d0.loss_cls_H: 0.6555, d0.loss_bbox_lane: 6.5832, d0.loss_bbox_H: 6.5729, d1.loss_cls_lane: 0.6555, d1.loss_cls_H: 0.6555, d1.loss_bbox_lane: 6.5832, d1.loss_bbox_H: 6.5729, d2.loss_cls_lane: 0.6555, d2.loss_cls_H: 0.6555, d2.loss_bbox_lane: 6.5832, d2.loss_bbox_H: 6.5729, d3.loss_cls_lane: 0.6555, d3.loss_cls_H: 0.6555, d3.loss_bbox_lane: 6.5832, d3.loss_bbox_H: 6.5729, d4.loss_cls_lane: 0.6555, d4.loss_cls_H: 0.6555, d4.loss_bbox_lane: 6.5832, d4.loss_bbox_H: 6.5729, vlm_loss: 0.0000, loss: 131.9280, grad_norm: nan
2025-03-01 00:46:47,097 - mmdet - INFO - Iter [3500/168780]	lr: 3.996e-04, eta: 5 days, 19:14:15, time: 2.972, data_time: 0.016, memory: 22027, loss_cls: 1.7397, loss_bbox: 2.7454, d0.loss_cls: 1.7397, d0.loss_bbox: 2.7454, d1.loss_cls: 1.7397, d1.loss_bbox: 2.7454, d2.loss_cls: 1.7397, d2.loss_bbox: 2.7454, d3.loss_cls: 1.7397, d3.loss_bbox: 2.7454, d4.loss_cls: 1.7397, d4.loss_bbox: 2.7454, dn_loss_cls: 0.9843, dn_loss_bbox: 2.3075, d0.dn_loss_cls: 0.9843, d0.dn_loss_bbox: 2.3075, d1.dn_loss_cls: 0.9843, d1.dn_loss_bbox: 2.3075, d2.dn_loss_cls: 0.9843, d2.dn_loss_bbox: 2.3075, d3.dn_loss_cls: 0.9843, d3.dn_loss_bbox: 2.3075, d4.dn_loss_cls: 0.9843, d4.dn_loss_bbox: 2.3075, loss_cls_lane: 0.6652, loss_cls_H: 0.6652, loss_bbox_lane: 6.7543, loss_bbox_H: 6.7593, d0.loss_cls_lane: 0.6652, d0.loss_cls_H: 0.6652, d0.loss_bbox_lane: 6.7543, d0.loss_bbox_H: 6.7593, d1.loss_cls_lane: 0.6652, d1.loss_cls_H: 0.6652, d1.loss_bbox_lane: 6.7543, d1.loss_bbox_H: 6.7593, d2.loss_cls_lane: 0.6652, d2.loss_cls_H: 0.6652, d2.loss_bbox_lane: 6.7543, d2.loss_bbox_H: 6.7593, d3.loss_cls_lane: 0.6652, d3.loss_cls_H: 0.6652, d3.loss_bbox_lane: 6.7543, d3.loss_bbox_H: 6.7593, d4.loss_cls_lane: 0.6652, d4.loss_cls_H: 0.6652, d4.loss_bbox_lane: 6.7543, d4.loss_bbox_H: 6.7593, vlm_loss: 0.0000, loss: 135.7249, grad_norm: nan
2025-03-01 00:49:15,697 - mmdet - INFO - Iter [3550/168780]	lr: 3.996e-04, eta: 5 days, 19:09:22, time: 2.972, data_time: 0.016, memory: 22027, loss_cls: 1.7809, loss_bbox: 2.7094, d0.loss_cls: 1.7809, d0.loss_bbox: 2.7094, d1.loss_cls: 1.7809, d1.loss_bbox: 2.7094, d2.loss_cls: 1.7809, d2.loss_bbox: 2.7094, d3.loss_cls: 1.7809, d3.loss_bbox: 2.7094, d4.loss_cls: 1.7809, d4.loss_bbox: 2.7094, dn_loss_cls: 0.9569, dn_loss_bbox: 2.4108, d0.dn_loss_cls: 0.9569, d0.dn_loss_bbox: 2.4108, d1.dn_loss_cls: 0.9569, d1.dn_loss_bbox: 2.4108, d2.dn_loss_cls: 0.9569, d2.dn_loss_bbox: 2.4108, d3.dn_loss_cls: 0.9569, d3.dn_loss_bbox: 2.4108, d4.dn_loss_cls: 0.9569, d4.dn_loss_bbox: 2.4108, loss_cls_lane: 0.6906, loss_cls_H: 0.6906, loss_bbox_lane: 7.0971, loss_bbox_H: 7.0788, d0.loss_cls_lane: 0.6906, d0.loss_cls_H: 0.6906, d0.loss_bbox_lane: 7.0971, d0.loss_bbox_H: 7.0788, d1.loss_cls_lane: 0.6906, d1.loss_cls_H: 0.6906, d1.loss_bbox_lane: 7.0971, d1.loss_bbox_H: 7.0788, d2.loss_cls_lane: 0.6906, d2.loss_cls_H: 0.6906, d2.loss_bbox_lane: 7.0971, d2.loss_bbox_H: 7.0788, d3.loss_cls_lane: 0.6906, d3.loss_cls_H: 0.6906, d3.loss_bbox_lane: 7.0971, d3.loss_bbox_H: 7.0788, d4.loss_cls_lane: 0.6906, d4.loss_cls_H: 0.6906, d4.loss_bbox_lane: 7.0971, d4.loss_bbox_H: 7.0788, vlm_loss: 0.0000, loss: 140.4912, grad_norm: nan
2025-03-01 00:51:45,463 - mmdet - INFO - Iter [3600/168780]	lr: 3.996e-04, eta: 5 days, 19:05:27, time: 2.995, data_time: 0.017, memory: 22027, loss_cls: 1.3494, loss_bbox: 2.6515, d0.loss_cls: 1.3494, d0.loss_bbox: 2.6515, d1.loss_cls: 1.3494, d1.loss_bbox: 2.6515, d2.loss_cls: 1.3494, d2.loss_bbox: 2.6515, d3.loss_cls: 1.3494, d3.loss_bbox: 2.6515, d4.loss_cls: 1.3494, d4.loss_bbox: 2.6515, dn_loss_cls: 0.9557, dn_loss_bbox: 2.3104, d0.dn_loss_cls: 0.9557, d0.dn_loss_bbox: 2.3104, d1.dn_loss_cls: 0.9557, d1.dn_loss_bbox: 2.3104, d2.dn_loss_cls: 0.9557, d2.dn_loss_bbox: 2.3104, d3.dn_loss_cls: 0.9557, d3.dn_loss_bbox: 2.3104, d4.dn_loss_cls: 0.9557, d4.dn_loss_bbox: 2.3104, loss_cls_lane: 0.6481, loss_cls_H: 0.6481, loss_bbox_lane: 6.7177, loss_bbox_H: 6.7131, d0.loss_cls_lane: 0.6481, d0.loss_cls_H: 0.6481, d0.loss_bbox_lane: 6.7177, d0.loss_bbox_H: 6.7131, d1.loss_cls_lane: 0.6481, d1.loss_cls_H: 0.6481, d1.loss_bbox_lane: 6.7177, d1.loss_bbox_H: 6.7131, d2.loss_cls_lane: 0.6481, d2.loss_cls_H: 0.6481, d2.loss_bbox_lane: 6.7177, d2.loss_bbox_H: 6.7131, d3.loss_cls_lane: 0.6481, d3.loss_cls_H: 0.6481, d3.loss_bbox_lane: 6.7177, d3.loss_bbox_H: 6.7131, d4.loss_cls_lane: 0.6481, d4.loss_cls_H: 0.6481, d4.loss_bbox_lane: 6.7177, d4.loss_bbox_H: 6.7131, vlm_loss: 0.0000, loss: 131.9646, grad_norm: nan
2025-03-01 00:54:13,766 - mmdet - INFO - Iter [3650/168780]	lr: 3.995e-04, eta: 5 days, 19:00:27, time: 2.966, data_time: 0.017, memory: 22027, loss_cls: 1.6241, loss_bbox: 2.8513, d0.loss_cls: 1.6241, d0.loss_bbox: 2.8513, d1.loss_cls: 1.6241, d1.loss_bbox: 2.8513, d2.loss_cls: 1.6241, d2.loss_bbox: 2.8513, d3.loss_cls: 1.6241, d3.loss_bbox: 2.8513, d4.loss_cls: 1.6241, d4.loss_bbox: 2.8513, dn_loss_cls: 0.9778, dn_loss_bbox: 2.5908, d0.dn_loss_cls: 0.9778, d0.dn_loss_bbox: 2.5908, d1.dn_loss_cls: 0.9778, d1.dn_loss_bbox: 2.5908, d2.dn_loss_cls: 0.9778, d2.dn_loss_bbox: 2.5908, d3.dn_loss_cls: 0.9778, d3.dn_loss_bbox: 2.5908, d4.dn_loss_cls: 0.9778, d4.dn_loss_bbox: 2.5908, loss_cls_lane: 0.6536, loss_cls_H: 0.6536, loss_bbox_lane: 6.7403, loss_bbox_H: 6.7316, d0.loss_cls_lane: 0.6536, d0.loss_cls_H: 0.6536, d0.loss_bbox_lane: 6.7403, d0.loss_bbox_H: 6.7316, d1.loss_cls_lane: 0.6536, d1.loss_cls_H: 0.6536, d1.loss_bbox_lane: 6.7403, d1.loss_bbox_H: 6.7316, d2.loss_cls_lane: 0.6536, d2.loss_cls_H: 0.6536, d2.loss_bbox_lane: 6.7403, d2.loss_bbox_H: 6.7316, d3.loss_cls_lane: 0.6536, d3.loss_cls_H: 0.6536, d3.loss_bbox_lane: 6.7403, d3.loss_bbox_H: 6.7316, d4.loss_cls_lane: 0.6536, d4.loss_cls_H: 0.6536, d4.loss_bbox_lane: 6.7403, d4.loss_bbox_H: 6.7316, vlm_loss: 0.0000, loss: 136.9393, grad_norm: nan
2025-03-01 00:56:41,953 - mmdet - INFO - Iter [3700/168780]	lr: 3.995e-04, eta: 5 days, 18:55:27, time: 2.964, data_time: 0.017, memory: 22027, loss_cls: 1.8083, loss_bbox: 2.6941, d0.loss_cls: 1.8083, d0.loss_bbox: 2.6941, d1.loss_cls: 1.8083, d1.loss_bbox: 2.6941, d2.loss_cls: 1.8083, d2.loss_bbox: 2.6941, d3.loss_cls: 1.8083, d3.loss_bbox: 2.6941, d4.loss_cls: 1.8083, d4.loss_bbox: 2.6941, dn_loss_cls: 0.8980, dn_loss_bbox: 2.3072, d0.dn_loss_cls: 0.8980, d0.dn_loss_bbox: 2.3072, d1.dn_loss_cls: 0.8980, d1.dn_loss_bbox: 2.3072, d2.dn_loss_cls: 0.8980, d2.dn_loss_bbox: 2.3072, d3.dn_loss_cls: 0.8980, d3.dn_loss_bbox: 2.3072, d4.dn_loss_cls: 0.8980, d4.dn_loss_bbox: 2.3072, loss_cls_lane: 0.6251, loss_cls_H: 0.6251, loss_bbox_lane: 6.2459, loss_bbox_H: 6.2612, d0.loss_cls_lane: 0.6251, d0.loss_cls_H: 0.6251, d0.loss_bbox_lane: 6.2459, d0.loss_bbox_H: 6.2612, d1.loss_cls_lane: 0.6251, d1.loss_cls_H: 0.6251, d1.loss_bbox_lane: 6.2459, d1.loss_bbox_H: 6.2612, d2.loss_cls_lane: 0.6251, d2.loss_cls_H: 0.6251, d2.loss_bbox_lane: 6.2459, d2.loss_bbox_H: 6.2612, d3.loss_cls_lane: 0.6251, d3.loss_cls_H: 0.6251, d3.loss_bbox_lane: 6.2459, d3.loss_bbox_H: 6.2612, d4.loss_cls_lane: 0.6251, d4.loss_cls_H: 0.6251, d4.loss_bbox_lane: 6.2459, d4.loss_bbox_H: 6.2612, vlm_loss: 0.0000, loss: 128.7902, grad_norm: nan
2025-03-01 00:59:09,219 - mmdet - INFO - Iter [3750/168780]	lr: 3.995e-04, eta: 5 days, 18:49:50, time: 2.945, data_time: 0.018, memory: 22027, loss_cls: 1.5854, loss_bbox: 3.0029, d0.loss_cls: 1.5854, d0.loss_bbox: 3.0029, d1.loss_cls: 1.5854, d1.loss_bbox: 3.0029, d2.loss_cls: 1.5854, d2.loss_bbox: 3.0029, d3.loss_cls: 1.5854, d3.loss_bbox: 3.0029, d4.loss_cls: 1.5854, d4.loss_bbox: 3.0029, dn_loss_cls: 0.9129, dn_loss_bbox: 2.5759, d0.dn_loss_cls: 0.9129, d0.dn_loss_bbox: 2.5759, d1.dn_loss_cls: 0.9129, d1.dn_loss_bbox: 2.5759, d2.dn_loss_cls: 0.9129, d2.dn_loss_bbox: 2.5759, d3.dn_loss_cls: 0.9129, d3.dn_loss_bbox: 2.5759, d4.dn_loss_cls: 0.9129, d4.dn_loss_bbox: 2.5759, loss_cls_lane: 0.6385, loss_cls_H: 0.6385, loss_bbox_lane: 6.6910, loss_bbox_H: 6.6712, d0.loss_cls_lane: 0.6385, d0.loss_cls_H: 0.6385, d0.loss_bbox_lane: 6.6910, d0.loss_bbox_H: 6.6712, d1.loss_cls_lane: 0.6385, d1.loss_cls_H: 0.6385, d1.loss_bbox_lane: 6.6910, d1.loss_bbox_H: 6.6712, d2.loss_cls_lane: 0.6385, d2.loss_cls_H: 0.6385, d2.loss_bbox_lane: 6.6910, d2.loss_bbox_H: 6.6712, d3.loss_cls_lane: 0.6385, d3.loss_cls_H: 0.6385, d3.loss_bbox_lane: 6.6910, d3.loss_bbox_H: 6.6712, d4.loss_cls_lane: 0.6385, d4.loss_cls_H: 0.6385, d4.loss_bbox_lane: 6.6910, d4.loss_bbox_H: 6.6712, vlm_loss: 0.0000, loss: 136.2979, grad_norm: nan
2025-03-01 01:01:38,545 - mmdet - INFO - Iter [3800/168780]	lr: 3.995e-04, eta: 5 days, 18:45:47, time: 2.987, data_time: 0.017, memory: 22027, loss_cls: 1.3937, loss_bbox: 2.7909, d0.loss_cls: 1.3937, d0.loss_bbox: 2.7909, d1.loss_cls: 1.3937, d1.loss_bbox: 2.7909, d2.loss_cls: 1.3937, d2.loss_bbox: 2.7909, d3.loss_cls: 1.3937, d3.loss_bbox: 2.7909, d4.loss_cls: 1.3937, d4.loss_bbox: 2.7909, dn_loss_cls: 1.0212, dn_loss_bbox: 2.5455, d0.dn_loss_cls: 1.0212, d0.dn_loss_bbox: 2.5455, d1.dn_loss_cls: 1.0212, d1.dn_loss_bbox: 2.5455, d2.dn_loss_cls: 1.0212, d2.dn_loss_bbox: 2.5455, d3.dn_loss_cls: 1.0212, d3.dn_loss_bbox: 2.5455, d4.dn_loss_cls: 1.0212, d4.dn_loss_bbox: 2.5455, loss_cls_lane: 0.6398, loss_cls_H: 0.6398, loss_bbox_lane: 6.4039, loss_bbox_H: 6.4151, d0.loss_cls_lane: 0.6398, d0.loss_cls_H: 0.6398, d0.loss_bbox_lane: 6.4039, d0.loss_bbox_H: 6.4151, d1.loss_cls_lane: 0.6398, d1.loss_cls_H: 0.6398, d1.loss_bbox_lane: 6.4039, d1.loss_bbox_H: 6.4151, d2.loss_cls_lane: 0.6398, d2.loss_cls_H: 0.6398, d2.loss_bbox_lane: 6.4039, d2.loss_bbox_H: 6.4151, d3.loss_cls_lane: 0.6398, d3.loss_cls_H: 0.6398, d3.loss_bbox_lane: 6.4039, d3.loss_bbox_H: 6.4151, d4.loss_cls_lane: 0.6398, d4.loss_cls_H: 0.6398, d4.loss_bbox_lane: 6.4039, d4.loss_bbox_H: 6.4151, vlm_loss: 0.0000, loss: 131.0995, grad_norm: nan
2025-03-01 01:04:06,727 - mmdet - INFO - Iter [3850/168780]	lr: 3.995e-04, eta: 5 days, 18:40:58, time: 2.964, data_time: 0.017, memory: 22027, loss_cls: 1.5667, loss_bbox: 2.7075, d0.loss_cls: 1.5667, d0.loss_bbox: 2.7075, d1.loss_cls: 1.5667, d1.loss_bbox: 2.7075, d2.loss_cls: 1.5667, d2.loss_bbox: 2.7075, d3.loss_cls: 1.5667, d3.loss_bbox: 2.7075, d4.loss_cls: 1.5667, d4.loss_bbox: 2.7075, dn_loss_cls: 0.9106, dn_loss_bbox: 2.3205, d0.dn_loss_cls: 0.9106, d0.dn_loss_bbox: 2.3205, d1.dn_loss_cls: 0.9106, d1.dn_loss_bbox: 2.3205, d2.dn_loss_cls: 0.9106, d2.dn_loss_bbox: 2.3205, d3.dn_loss_cls: 0.9106, d3.dn_loss_bbox: 2.3205, d4.dn_loss_cls: 0.9106, d4.dn_loss_bbox: 2.3205, loss_cls_lane: 0.6324, loss_cls_H: 0.6861, loss_bbox_lane: 6.4600, loss_bbox_H: 6.4630, d0.loss_cls_lane: 0.6324, d0.loss_cls_H: 0.6861, d0.loss_bbox_lane: 6.4600, d0.loss_bbox_H: 6.4630, d1.loss_cls_lane: 0.6324, d1.loss_cls_H: 0.6861, d1.loss_bbox_lane: 6.4600, d1.loss_bbox_H: 6.4630, d2.loss_cls_lane: 0.6324, d2.loss_cls_H: 0.6861, d2.loss_bbox_lane: 6.4600, d2.loss_bbox_H: 6.4630, d3.loss_cls_lane: 0.6324, d3.loss_cls_H: 0.6861, d3.loss_bbox_lane: 6.4600, d3.loss_bbox_H: 6.4630, d4.loss_cls_lane: 0.6324, d4.loss_cls_H: 0.6861, d4.loss_bbox_lane: 6.4600, d4.loss_bbox_H: 6.4630, vlm_loss: 0.0000, loss: 130.4809, grad_norm: nan
2025-03-01 01:06:36,331 - mmdet - INFO - Iter [3900/168780]	lr: 3.995e-04, eta: 5 days, 18:37:13, time: 2.992, data_time: 0.018, memory: 22027, loss_cls: 1.5460, loss_bbox: 2.8234, d0.loss_cls: 1.5460, d0.loss_bbox: 2.8234, d1.loss_cls: 1.5460, d1.loss_bbox: 2.8234, d2.loss_cls: 1.5460, d2.loss_bbox: 2.8234, d3.loss_cls: 1.5460, d3.loss_bbox: 2.8234, d4.loss_cls: 1.5460, d4.loss_bbox: 2.8234, dn_loss_cls: 0.9641, dn_loss_bbox: 2.5233, d0.dn_loss_cls: 0.9641, d0.dn_loss_bbox: 2.5233, d1.dn_loss_cls: 0.9641, d1.dn_loss_bbox: 2.5233, d2.dn_loss_cls: 0.9641, d2.dn_loss_bbox: 2.5233, d3.dn_loss_cls: 0.9641, d3.dn_loss_bbox: 2.5233, d4.dn_loss_cls: 0.9641, d4.dn_loss_bbox: 2.5233, loss_cls_lane: 0.6266, loss_cls_H: 0.6266, loss_bbox_lane: 6.3724, loss_bbox_H: 6.3816, d0.loss_cls_lane: 0.6266, d0.loss_cls_H: 0.6266, d0.loss_bbox_lane: 6.3724, d0.loss_bbox_H: 6.3816, d1.loss_cls_lane: 0.6266, d1.loss_cls_H: 0.6266, d1.loss_bbox_lane: 6.3724, d1.loss_bbox_H: 6.3816, d2.loss_cls_lane: 0.6266, d2.loss_cls_H: 0.6266, d2.loss_bbox_lane: 6.3724, d2.loss_bbox_H: 6.3816, d3.loss_cls_lane: 0.6266, d3.loss_cls_H: 0.6266, d3.loss_bbox_lane: 6.3724, d3.loss_bbox_H: 6.3816, d4.loss_cls_lane: 0.6266, d4.loss_cls_H: 0.6266, d4.loss_bbox_lane: 6.3724, d4.loss_bbox_H: 6.3816, vlm_loss: 0.0000, loss: 131.1850, grad_norm: nan
2025-03-01 01:09:02,598 - mmdet - INFO - Iter [3950/168780]	lr: 3.995e-04, eta: 5 days, 18:31:10, time: 2.925, data_time: 0.016, memory: 22027, loss_cls: 2.1503, loss_bbox: 2.9521, d0.loss_cls: 2.1503, d0.loss_bbox: 2.9521, d1.loss_cls: 2.1503, d1.loss_bbox: 2.9521, d2.loss_cls: 2.1503, d2.loss_bbox: 2.9521, d3.loss_cls: 2.1503, d3.loss_bbox: 2.9521, d4.loss_cls: 2.1503, d4.loss_bbox: 2.9521, dn_loss_cls: 0.9234, dn_loss_bbox: 2.5080, d0.dn_loss_cls: 0.9234, d0.dn_loss_bbox: 2.5080, d1.dn_loss_cls: 0.9234, d1.dn_loss_bbox: 2.5080, d2.dn_loss_cls: 0.9234, d2.dn_loss_bbox: 2.5080, d3.dn_loss_cls: 0.9234, d3.dn_loss_bbox: 2.5080, d4.dn_loss_cls: 0.9234, d4.dn_loss_bbox: 2.5080, loss_cls_lane: 0.6585, loss_cls_H: 0.6585, loss_bbox_lane: 6.4455, loss_bbox_H: 6.4513, d0.loss_cls_lane: 0.6585, d0.loss_cls_H: 0.6585, d0.loss_bbox_lane: 6.4455, d0.loss_bbox_H: 6.4513, d1.loss_cls_lane: 0.6585, d1.loss_cls_H: 0.6585, d1.loss_bbox_lane: 6.4455, d1.loss_bbox_H: 6.4513, d2.loss_cls_lane: 0.6585, d2.loss_cls_H: 0.6585, d2.loss_bbox_lane: 6.4455, d2.loss_bbox_H: 6.4513, d3.loss_cls_lane: 0.6585, d3.loss_cls_H: 0.6585, d3.loss_bbox_lane: 6.4455, d3.loss_bbox_H: 6.4513, d4.loss_cls_lane: 0.6585, d4.loss_cls_H: 0.6585, d4.loss_bbox_lane: 6.4455, d4.loss_bbox_H: 6.4513, vlm_loss: 0.0000, loss: 136.4845, grad_norm: nan
2025-03-01 01:11:30,401 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 01:11:30,402 - mmdet - INFO - Iter [4000/168780]	lr: 3.994e-04, eta: 5 days, 18:26:16, time: 2.956, data_time: 0.017, memory: 22027, loss_cls: 1.6493, loss_bbox: 2.9737, d0.loss_cls: 1.6493, d0.loss_bbox: 2.9737, d1.loss_cls: 1.6493, d1.loss_bbox: 2.9737, d2.loss_cls: 1.6493, d2.loss_bbox: 2.9737, d3.loss_cls: 1.6493, d3.loss_bbox: 2.9737, d4.loss_cls: 1.6493, d4.loss_bbox: 2.9737, dn_loss_cls: 0.9129, dn_loss_bbox: 2.6126, d0.dn_loss_cls: 0.9129, d0.dn_loss_bbox: 2.6126, d1.dn_loss_cls: 0.9129, d1.dn_loss_bbox: 2.6126, d2.dn_loss_cls: 0.9129, d2.dn_loss_bbox: 2.6126, d3.dn_loss_cls: 0.9129, d3.dn_loss_bbox: 2.6126, d4.dn_loss_cls: 0.9129, d4.dn_loss_bbox: 2.6126, loss_cls_lane: 0.6835, loss_cls_H: 0.6835, loss_bbox_lane: 7.1409, loss_bbox_H: 7.1310, d0.loss_cls_lane: 0.6835, d0.loss_cls_H: 0.6835, d0.loss_bbox_lane: 7.1409, d0.loss_bbox_H: 7.1310, d1.loss_cls_lane: 0.6835, d1.loss_cls_H: 0.6835, d1.loss_bbox_lane: 7.1409, d1.loss_bbox_H: 7.1310, d2.loss_cls_lane: 0.6835, d2.loss_cls_H: 0.6835, d2.loss_bbox_lane: 7.1409, d2.loss_bbox_H: 7.1310, d3.loss_cls_lane: 0.6835, d3.loss_cls_H: 0.6835, d3.loss_bbox_lane: 7.1409, d3.loss_bbox_H: 7.1310, d4.loss_cls_lane: 0.6835, d4.loss_cls_H: 0.6835, d4.loss_bbox_lane: 7.1409, d4.loss_bbox_H: 7.1310, vlm_loss: 0.0000, loss: 142.7236, grad_norm: nan
2025-03-01 01:14:00,488 - mmdet - INFO - Iter [4050/168780]	lr: 3.994e-04, eta: 5 days, 18:22:59, time: 3.002, data_time: 0.018, memory: 22027, loss_cls: 1.2383, loss_bbox: 2.7359, d0.loss_cls: 1.2383, d0.loss_bbox: 2.7359, d1.loss_cls: 1.2383, d1.loss_bbox: 2.7359, d2.loss_cls: 1.2383, d2.loss_bbox: 2.7359, d3.loss_cls: 1.2383, d3.loss_bbox: 2.7359, d4.loss_cls: 1.2383, d4.loss_bbox: 2.7359, dn_loss_cls: 0.8851, dn_loss_bbox: 2.5130, d0.dn_loss_cls: 0.8851, d0.dn_loss_bbox: 2.5130, d1.dn_loss_cls: 0.8851, d1.dn_loss_bbox: 2.5130, d2.dn_loss_cls: 0.8851, d2.dn_loss_bbox: 2.5130, d3.dn_loss_cls: 0.8851, d3.dn_loss_bbox: 2.5130, d4.dn_loss_cls: 0.8851, d4.dn_loss_bbox: 2.5130, loss_cls_lane: 0.6581, loss_cls_H: 0.6581, loss_bbox_lane: 7.1831, loss_bbox_H: 7.1695, d0.loss_cls_lane: 0.6581, d0.loss_cls_H: 0.6581, d0.loss_bbox_lane: 7.1831, d0.loss_bbox_H: 7.1695, d1.loss_cls_lane: 0.6581, d1.loss_cls_H: 0.6581, d1.loss_bbox_lane: 7.1831, d1.loss_bbox_H: 7.1695, d2.loss_cls_lane: 0.6581, d2.loss_cls_H: 0.6581, d2.loss_bbox_lane: 7.1831, d2.loss_bbox_H: 7.1695, d3.loss_cls_lane: 0.6581, d3.loss_cls_H: 0.6581, d3.loss_bbox_lane: 7.1831, d3.loss_bbox_H: 7.1695, d4.loss_cls_lane: 0.6581, d4.loss_cls_H: 0.6581, d4.loss_bbox_lane: 7.1831, d4.loss_bbox_H: 7.1695, vlm_loss: 0.0000, loss: 138.2468, grad_norm: nan
2025-03-01 01:16:30,101 - mmdet - INFO - Iter [4100/168780]	lr: 3.994e-04, eta: 5 days, 18:19:23, time: 2.992, data_time: 0.017, memory: 22027, loss_cls: 1.2965, loss_bbox: 2.8300, d0.loss_cls: 1.2965, d0.loss_bbox: 2.8300, d1.loss_cls: 1.2965, d1.loss_bbox: 2.8300, d2.loss_cls: 1.2965, d2.loss_bbox: 2.8300, d3.loss_cls: 1.2965, d3.loss_bbox: 2.8300, d4.loss_cls: 1.2965, d4.loss_bbox: 2.8300, dn_loss_cls: 0.9045, dn_loss_bbox: 2.5044, d0.dn_loss_cls: 0.9045, d0.dn_loss_bbox: 2.5044, d1.dn_loss_cls: 0.9045, d1.dn_loss_bbox: 2.5044, d2.dn_loss_cls: 0.9045, d2.dn_loss_bbox: 2.5044, d3.dn_loss_cls: 0.9045, d3.dn_loss_bbox: 2.5044, d4.dn_loss_cls: 0.9045, d4.dn_loss_bbox: 2.5044, loss_cls_lane: 0.6503, loss_cls_H: 0.6503, loss_bbox_lane: 6.7040, loss_bbox_H: 6.6876, d0.loss_cls_lane: 0.6503, d0.loss_cls_H: 0.6503, d0.loss_bbox_lane: 6.7040, d0.loss_bbox_H: 6.6876, d1.loss_cls_lane: 0.6503, d1.loss_cls_H: 0.6503, d1.loss_bbox_lane: 6.7040, d1.loss_bbox_H: 6.6876, d2.loss_cls_lane: 0.6503, d2.loss_cls_H: 0.6503, d2.loss_bbox_lane: 6.7040, d2.loss_bbox_H: 6.6876, d3.loss_cls_lane: 0.6503, d3.loss_cls_H: 0.6503, d3.loss_bbox_lane: 6.7040, d3.loss_bbox_H: 6.6876, d4.loss_cls_lane: 0.6503, d4.loss_cls_H: 0.6503, d4.loss_bbox_lane: 6.7040, d4.loss_bbox_H: 6.6876, vlm_loss: 0.0000, loss: 133.3651, grad_norm: nan
2025-03-01 01:18:58,122 - mmdet - INFO - Iter [4150/168780]	lr: 3.994e-04, eta: 5 days, 18:14:46, time: 2.960, data_time: 0.017, memory: 22027, loss_cls: 2.0099, loss_bbox: 2.8055, d0.loss_cls: 2.0099, d0.loss_bbox: 2.8055, d1.loss_cls: 2.0099, d1.loss_bbox: 2.8055, d2.loss_cls: 2.0099, d2.loss_bbox: 2.8055, d3.loss_cls: 2.0099, d3.loss_bbox: 2.8055, d4.loss_cls: 2.0099, d4.loss_bbox: 2.8055, dn_loss_cls: 0.9402, dn_loss_bbox: 2.4980, d0.dn_loss_cls: 0.9402, d0.dn_loss_bbox: 2.4980, d1.dn_loss_cls: 0.9402, d1.dn_loss_bbox: 2.4980, d2.dn_loss_cls: 0.9402, d2.dn_loss_bbox: 2.4980, d3.dn_loss_cls: 0.9402, d3.dn_loss_bbox: 2.4980, d4.dn_loss_cls: 0.9402, d4.dn_loss_bbox: 2.4980, loss_cls_lane: 0.6328, loss_cls_H: 0.6328, loss_bbox_lane: 6.2631, loss_bbox_H: 6.2578, d0.loss_cls_lane: 0.6328, d0.loss_cls_H: 0.6328, d0.loss_bbox_lane: 6.2631, d0.loss_bbox_H: 6.2578, d1.loss_cls_lane: 0.6328, d1.loss_cls_H: 0.6328, d1.loss_bbox_lane: 6.2631, d1.loss_bbox_H: 6.2578, d2.loss_cls_lane: 0.6328, d2.loss_cls_H: 0.6328, d2.loss_bbox_lane: 6.2631, d2.loss_bbox_H: 6.2578, d3.loss_cls_lane: 0.6328, d3.loss_cls_H: 0.6328, d3.loss_bbox_lane: 6.2631, d3.loss_bbox_H: 6.2578, d4.loss_cls_lane: 0.6328, d4.loss_cls_H: 0.6328, d4.loss_bbox_lane: 6.2631, d4.loss_bbox_H: 6.2578, vlm_loss: 0.0000, loss: 132.2408, grad_norm: nan
2025-03-01 01:21:45,567 - mmdet - INFO - Iter [4200/168780]	lr: 3.994e-04, eta: 5 days, 18:22:54, time: 3.349, data_time: 0.017, memory: 22027, loss_cls: 1.2941, loss_bbox: 2.7820, d0.loss_cls: 1.2941, d0.loss_bbox: 2.7820, d1.loss_cls: 1.2941, d1.loss_bbox: 2.7820, d2.loss_cls: 1.2941, d2.loss_bbox: 2.7820, d3.loss_cls: 1.2941, d3.loss_bbox: 2.7820, d4.loss_cls: 1.2941, d4.loss_bbox: 2.7820, dn_loss_cls: 0.9332, dn_loss_bbox: 2.4413, d0.dn_loss_cls: 0.9332, d0.dn_loss_bbox: 2.4413, d1.dn_loss_cls: 0.9332, d1.dn_loss_bbox: 2.4413, d2.dn_loss_cls: 0.9332, d2.dn_loss_bbox: 2.4413, d3.dn_loss_cls: 0.9332, d3.dn_loss_bbox: 2.4413, d4.dn_loss_cls: 0.9332, d4.dn_loss_bbox: 2.4413, loss_cls_lane: 0.6277, loss_cls_H: 0.6277, loss_bbox_lane: 6.4862, loss_bbox_H: 6.4826, d0.loss_cls_lane: 0.6277, d0.loss_cls_H: 0.6277, d0.loss_bbox_lane: 6.4862, d0.loss_bbox_H: 6.4826, d1.loss_cls_lane: 0.6277, d1.loss_cls_H: 0.6277, d1.loss_bbox_lane: 6.4862, d1.loss_bbox_H: 6.4826, d2.loss_cls_lane: 0.6277, d2.loss_cls_H: 0.6277, d2.loss_bbox_lane: 6.4862, d2.loss_bbox_H: 6.4826, d3.loss_cls_lane: 0.6277, d3.loss_cls_H: 0.6277, d3.loss_bbox_lane: 6.4862, d3.loss_bbox_H: 6.4826, d4.loss_cls_lane: 0.6277, d4.loss_cls_H: 0.6277, d4.loss_bbox_lane: 6.4862, d4.loss_bbox_H: 6.4826, vlm_loss: 0.0000, loss: 130.0478, grad_norm: nan
2025-03-01 01:24:13,716 - mmdet - INFO - Iter [4250/168780]	lr: 3.994e-04, eta: 5 days, 18:18:18, time: 2.963, data_time: 0.017, memory: 22027, loss_cls: 1.9553, loss_bbox: 2.8108, d0.loss_cls: 1.9553, d0.loss_bbox: 2.8108, d1.loss_cls: 1.9553, d1.loss_bbox: 2.8108, d2.loss_cls: 1.9553, d2.loss_bbox: 2.8108, d3.loss_cls: 1.9553, d3.loss_bbox: 2.8108, d4.loss_cls: 1.9553, d4.loss_bbox: 2.8108, dn_loss_cls: 0.9926, dn_loss_bbox: 2.4907, d0.dn_loss_cls: 0.9926, d0.dn_loss_bbox: 2.4907, d1.dn_loss_cls: 0.9926, d1.dn_loss_bbox: 2.4907, d2.dn_loss_cls: 0.9926, d2.dn_loss_bbox: 2.4907, d3.dn_loss_cls: 0.9926, d3.dn_loss_bbox: 2.4907, d4.dn_loss_cls: 0.9926, d4.dn_loss_bbox: 2.4907, loss_cls_lane: 0.6337, loss_cls_H: 0.6337, loss_bbox_lane: 6.3187, loss_bbox_H: 6.3428, d0.loss_cls_lane: 0.6337, d0.loss_cls_H: 0.6337, d0.loss_bbox_lane: 6.3187, d0.loss_bbox_H: 6.3428, d1.loss_cls_lane: 0.6337, d1.loss_cls_H: 0.6337, d1.loss_bbox_lane: 6.3187, d1.loss_bbox_H: 6.3428, d2.loss_cls_lane: 0.6337, d2.loss_cls_H: 0.6337, d2.loss_bbox_lane: 6.3187, d2.loss_bbox_H: 6.3428, d3.loss_cls_lane: 0.6337, d3.loss_cls_H: 0.6337, d3.loss_bbox_lane: 6.3187, d3.loss_bbox_H: 6.3428, d4.loss_cls_lane: 0.6337, d4.loss_cls_H: 0.6337, d4.loss_bbox_lane: 6.3187, d4.loss_bbox_H: 6.3428, vlm_loss: 0.0000, loss: 133.0706, grad_norm: nan
2025-03-01 01:26:42,535 - mmdet - INFO - Iter [4300/168780]	lr: 3.994e-04, eta: 5 days, 18:14:12, time: 2.976, data_time: 0.017, memory: 22027, loss_cls: 1.3021, loss_bbox: 2.7034, d0.loss_cls: 1.3021, d0.loss_bbox: 2.7034, d1.loss_cls: 1.3021, d1.loss_bbox: 2.7034, d2.loss_cls: 1.3021, d2.loss_bbox: 2.7034, d3.loss_cls: 1.3021, d3.loss_bbox: 2.7034, d4.loss_cls: 1.3021, d4.loss_bbox: 2.7034, dn_loss_cls: 0.8631, dn_loss_bbox: 2.3593, d0.dn_loss_cls: 0.8631, d0.dn_loss_bbox: 2.3593, d1.dn_loss_cls: 0.8631, d1.dn_loss_bbox: 2.3593, d2.dn_loss_cls: 0.8631, d2.dn_loss_bbox: 2.3593, d3.dn_loss_cls: 0.8631, d3.dn_loss_bbox: 2.3593, d4.dn_loss_cls: 0.8631, d4.dn_loss_bbox: 2.3593, loss_cls_lane: 0.6359, loss_cls_H: 0.6359, loss_bbox_lane: 6.5510, loss_bbox_H: 6.5599, d0.loss_cls_lane: 0.6359, d0.loss_cls_H: 0.6359, d0.loss_bbox_lane: 6.5510, d0.loss_bbox_H: 6.5599, d1.loss_cls_lane: 0.6359, d1.loss_cls_H: 0.6359, d1.loss_bbox_lane: 6.5510, d1.loss_bbox_H: 6.5599, d2.loss_cls_lane: 0.6359, d2.loss_cls_H: 0.6359, d2.loss_bbox_lane: 6.5510, d2.loss_bbox_H: 6.5599, d3.loss_cls_lane: 0.6359, d3.loss_cls_H: 0.6359, d3.loss_bbox_lane: 6.5510, d3.loss_bbox_H: 6.5599, d4.loss_cls_lane: 0.6359, d4.loss_cls_H: 0.6359, d4.loss_bbox_lane: 6.5510, d4.loss_bbox_H: 6.5599, vlm_loss: 0.0000, loss: 129.6635, grad_norm: nan
2025-03-01 01:29:04,242 - mmdet - INFO - Iter [4350/168780]	lr: 3.993e-04, eta: 5 days, 18:05:39, time: 2.834, data_time: 0.017, memory: 22027, loss_cls: 1.3704, loss_bbox: 2.8910, d0.loss_cls: 1.3704, d0.loss_bbox: 2.8910, d1.loss_cls: 1.3704, d1.loss_bbox: 2.8910, d2.loss_cls: 1.3704, d2.loss_bbox: 2.8910, d3.loss_cls: 1.3704, d3.loss_bbox: 2.8910, d4.loss_cls: 1.3704, d4.loss_bbox: 2.8910, dn_loss_cls: 0.8927, dn_loss_bbox: 2.5827, d0.dn_loss_cls: 0.8927, d0.dn_loss_bbox: 2.5827, d1.dn_loss_cls: 0.8927, d1.dn_loss_bbox: 2.5827, d2.dn_loss_cls: 0.8927, d2.dn_loss_bbox: 2.5827, d3.dn_loss_cls: 0.8927, d3.dn_loss_bbox: 2.5827, d4.dn_loss_cls: 0.8927, d4.dn_loss_bbox: 2.5827, loss_cls_lane: 0.6694, loss_cls_H: 1.3144, loss_bbox_lane: 5.4710, loss_bbox_H: 5.4786, d0.loss_cls_lane: 0.6694, d0.loss_cls_H: 1.3144, d0.loss_bbox_lane: 5.4710, d0.loss_bbox_H: 5.4786, d1.loss_cls_lane: 0.6694, d1.loss_cls_H: 1.3144, d1.loss_bbox_lane: 5.4710, d1.loss_bbox_H: 5.4786, d2.loss_cls_lane: 0.6694, d2.loss_cls_H: 1.3144, d2.loss_bbox_lane: 5.4710, d2.loss_bbox_H: 5.4786, d3.loss_cls_lane: 0.6694, d3.loss_cls_H: 1.3144, d3.loss_bbox_lane: 5.4710, d3.loss_bbox_H: 5.4786, d4.loss_cls_lane: 0.6694, d4.loss_cls_H: 1.3144, d4.loss_bbox_lane: 5.4710, d4.loss_bbox_H: 5.4786, vlm_loss: 0.0000, loss: 124.0209, grad_norm: nan
2025-03-01 01:31:32,627 - mmdet - INFO - Iter [4400/168780]	lr: 3.993e-04, eta: 5 days, 18:01:23, time: 2.968, data_time: 0.017, memory: 22027, loss_cls: 1.6388, loss_bbox: 3.1724, d0.loss_cls: 1.6388, d0.loss_bbox: 3.1724, d1.loss_cls: 1.6388, d1.loss_bbox: 3.1724, d2.loss_cls: 1.6388, d2.loss_bbox: 3.1724, d3.loss_cls: 1.6388, d3.loss_bbox: 3.1724, d4.loss_cls: 1.6388, d4.loss_bbox: 3.1724, dn_loss_cls: 0.8941, dn_loss_bbox: 2.9728, d0.dn_loss_cls: 0.8941, d0.dn_loss_bbox: 2.9728, d1.dn_loss_cls: 0.8941, d1.dn_loss_bbox: 2.9728, d2.dn_loss_cls: 0.8941, d2.dn_loss_bbox: 2.9728, d3.dn_loss_cls: 0.8941, d3.dn_loss_bbox: 2.9728, d4.dn_loss_cls: 0.8941, d4.dn_loss_bbox: 2.9728, loss_cls_lane: 0.6311, loss_cls_H: 0.6311, loss_bbox_lane: 6.6271, loss_bbox_H: 6.6297, d0.loss_cls_lane: 0.6311, d0.loss_cls_H: 0.6311, d0.loss_bbox_lane: 6.6271, d0.loss_bbox_H: 6.6297, d1.loss_cls_lane: 0.6311, d1.loss_cls_H: 0.6311, d1.loss_bbox_lane: 6.6271, d1.loss_bbox_H: 6.6297, d2.loss_cls_lane: 0.6311, d2.loss_cls_H: 0.6311, d2.loss_bbox_lane: 6.6271, d2.loss_bbox_H: 6.6297, d3.loss_cls_lane: 0.6311, d3.loss_cls_H: 0.6311, d3.loss_bbox_lane: 6.6271, d3.loss_bbox_H: 6.6297, d4.loss_cls_lane: 0.6311, d4.loss_cls_H: 0.6311, d4.loss_bbox_lane: 6.6271, d4.loss_bbox_H: 6.6297, vlm_loss: 0.0000, loss: 139.1820, grad_norm: nan
2025-03-01 01:34:03,188 - mmdet - INFO - Iter [4450/168780]	lr: 3.993e-04, eta: 5 days, 17:58:31, time: 3.011, data_time: 0.018, memory: 22027, loss_cls: 1.4794, loss_bbox: 2.7089, d0.loss_cls: 1.4794, d0.loss_bbox: 2.7089, d1.loss_cls: 1.4794, d1.loss_bbox: 2.7089, d2.loss_cls: 1.4794, d2.loss_bbox: 2.7089, d3.loss_cls: 1.4794, d3.loss_bbox: 2.7089, d4.loss_cls: 1.4794, d4.loss_bbox: 2.7089, dn_loss_cls: 0.9670, dn_loss_bbox: 2.3198, d0.dn_loss_cls: 0.9670, d0.dn_loss_bbox: 2.3198, d1.dn_loss_cls: 0.9670, d1.dn_loss_bbox: 2.3198, d2.dn_loss_cls: 0.9670, d2.dn_loss_bbox: 2.3198, d3.dn_loss_cls: 0.9670, d3.dn_loss_bbox: 2.3198, d4.dn_loss_cls: 0.9670, d4.dn_loss_bbox: 2.3198, loss_cls_lane: 0.6241, loss_cls_H: 0.6241, loss_bbox_lane: 6.2449, loss_bbox_H: 6.2542, d0.loss_cls_lane: 0.6241, d0.loss_cls_H: 0.6241, d0.loss_bbox_lane: 6.2449, d0.loss_bbox_H: 6.2542, d1.loss_cls_lane: 0.6241, d1.loss_cls_H: 0.6241, d1.loss_bbox_lane: 6.2449, d1.loss_bbox_H: 6.2542, d2.loss_cls_lane: 0.6241, d2.loss_cls_H: 0.6241, d2.loss_bbox_lane: 6.2449, d2.loss_bbox_H: 6.2542, d3.loss_cls_lane: 0.6241, d3.loss_cls_H: 0.6241, d3.loss_bbox_lane: 6.2449, d3.loss_bbox_H: 6.2542, d4.loss_cls_lane: 0.6241, d4.loss_cls_H: 0.6241, d4.loss_bbox_lane: 6.2449, d4.loss_bbox_H: 6.2542, vlm_loss: 0.0000, loss: 127.3333, grad_norm: nan
2025-03-01 01:36:31,936 - mmdet - INFO - Iter [4500/168780]	lr: 3.993e-04, eta: 5 days, 17:54:33, time: 2.975, data_time: 0.018, memory: 22027, loss_cls: 1.4227, loss_bbox: 2.9371, d0.loss_cls: 1.4227, d0.loss_bbox: 2.9371, d1.loss_cls: 1.4227, d1.loss_bbox: 2.9371, d2.loss_cls: 1.4227, d2.loss_bbox: 2.9371, d3.loss_cls: 1.4227, d3.loss_bbox: 2.9371, d4.loss_cls: 1.4227, d4.loss_bbox: 2.9371, dn_loss_cls: 0.9701, dn_loss_bbox: 2.5169, d0.dn_loss_cls: 0.9701, d0.dn_loss_bbox: 2.5169, d1.dn_loss_cls: 0.9701, d1.dn_loss_bbox: 2.5169, d2.dn_loss_cls: 0.9701, d2.dn_loss_bbox: 2.5169, d3.dn_loss_cls: 0.9701, d3.dn_loss_bbox: 2.5169, d4.dn_loss_cls: 0.9701, d4.dn_loss_bbox: 2.5169, loss_cls_lane: 0.6275, loss_cls_H: 0.6275, loss_bbox_lane: 6.2497, loss_bbox_H: 6.2468, d0.loss_cls_lane: 0.6275, d0.loss_cls_H: 0.6275, d0.loss_bbox_lane: 6.2497, d0.loss_bbox_H: 6.2468, d1.loss_cls_lane: 0.6275, d1.loss_cls_H: 0.6275, d1.loss_bbox_lane: 6.2497, d1.loss_bbox_H: 6.2468, d2.loss_cls_lane: 0.6275, d2.loss_cls_H: 0.6275, d2.loss_bbox_lane: 6.2497, d2.loss_bbox_H: 6.2468, d3.loss_cls_lane: 0.6275, d3.loss_cls_H: 0.6275, d3.loss_bbox_lane: 6.2497, d3.loss_bbox_H: 6.2468, d4.loss_cls_lane: 0.6275, d4.loss_cls_H: 0.6275, d4.loss_bbox_lane: 6.2497, d4.loss_bbox_H: 6.2468, vlm_loss: 0.0000, loss: 129.5892, grad_norm: nan
2025-03-01 01:39:20,798 - mmdet - INFO - Iter [4550/168780]	lr: 3.993e-04, eta: 5 days, 18:02:43, time: 3.377, data_time: 0.018, memory: 22027, loss_cls: 1.4241, loss_bbox: 2.8973, d0.loss_cls: 1.4241, d0.loss_bbox: 2.8973, d1.loss_cls: 1.4241, d1.loss_bbox: 2.8973, d2.loss_cls: 1.4241, d2.loss_bbox: 2.8973, d3.loss_cls: 1.4241, d3.loss_bbox: 2.8973, d4.loss_cls: 1.4241, d4.loss_bbox: 2.8973, dn_loss_cls: 0.9858, dn_loss_bbox: 2.5580, d0.dn_loss_cls: 0.9858, d0.dn_loss_bbox: 2.5580, d1.dn_loss_cls: 0.9858, d1.dn_loss_bbox: 2.5580, d2.dn_loss_cls: 0.9858, d2.dn_loss_bbox: 2.5580, d3.dn_loss_cls: 0.9858, d3.dn_loss_bbox: 2.5580, d4.dn_loss_cls: 0.9858, d4.dn_loss_bbox: 2.5580, loss_cls_lane: 0.6363, loss_cls_H: 0.6363, loss_bbox_lane: 6.5427, loss_bbox_H: 6.5435, d0.loss_cls_lane: 0.6363, d0.loss_cls_H: 0.6363, d0.loss_bbox_lane: 6.5427, d0.loss_bbox_H: 6.5435, d1.loss_cls_lane: 0.6363, d1.loss_cls_H: 0.6363, d1.loss_bbox_lane: 6.5427, d1.loss_bbox_H: 6.5435, d2.loss_cls_lane: 0.6363, d2.loss_cls_H: 0.6363, d2.loss_bbox_lane: 6.5427, d2.loss_bbox_H: 6.5435, d3.loss_cls_lane: 0.6363, d3.loss_cls_H: 0.6363, d3.loss_bbox_lane: 6.5427, d3.loss_bbox_H: 6.5435, d4.loss_cls_lane: 0.6363, d4.loss_cls_H: 0.6363, d4.loss_bbox_lane: 6.5427, d4.loss_bbox_H: 6.5435, vlm_loss: 0.0000, loss: 133.3438, grad_norm: nan
2025-03-01 01:41:50,886 - mmdet - INFO - Iter [4600/168780]	lr: 3.993e-04, eta: 5 days, 17:59:28, time: 3.002, data_time: 0.017, memory: 22027, loss_cls: 1.3355, loss_bbox: 2.8025, d0.loss_cls: 1.3355, d0.loss_bbox: 2.8025, d1.loss_cls: 1.3355, d1.loss_bbox: 2.8025, d2.loss_cls: 1.3355, d2.loss_bbox: 2.8025, d3.loss_cls: 1.3355, d3.loss_bbox: 2.8025, d4.loss_cls: 1.3355, d4.loss_bbox: 2.8025, dn_loss_cls: 0.9483, dn_loss_bbox: 2.2939, d0.dn_loss_cls: 0.9483, d0.dn_loss_bbox: 2.2939, d1.dn_loss_cls: 0.9483, d1.dn_loss_bbox: 2.2939, d2.dn_loss_cls: 0.9483, d2.dn_loss_bbox: 2.2939, d3.dn_loss_cls: 0.9483, d3.dn_loss_bbox: 2.2939, d4.dn_loss_cls: 0.9483, d4.dn_loss_bbox: 2.2939, loss_cls_lane: 0.6337, loss_cls_H: 0.6337, loss_bbox_lane: 6.2118, loss_bbox_H: 6.2111, d0.loss_cls_lane: 0.6337, d0.loss_cls_H: 0.6337, d0.loss_bbox_lane: 6.2118, d0.loss_bbox_H: 6.2111, d1.loss_cls_lane: 0.6337, d1.loss_cls_H: 0.6337, d1.loss_bbox_lane: 6.2118, d1.loss_bbox_H: 6.2111, d2.loss_cls_lane: 0.6337, d2.loss_cls_H: 0.6337, d2.loss_bbox_lane: 6.2118, d2.loss_bbox_H: 6.2111, d3.loss_cls_lane: 0.6337, d3.loss_cls_H: 0.6337, d3.loss_bbox_lane: 6.2118, d3.loss_bbox_H: 6.2111, d4.loss_cls_lane: 0.6337, d4.loss_cls_H: 0.6337, d4.loss_bbox_lane: 6.2118, d4.loss_bbox_H: 6.2111, vlm_loss: 0.0000, loss: 126.4241, grad_norm: nan
2025-03-01 01:44:23,585 - mmdet - INFO - Iter [4650/168780]	lr: 3.993e-04, eta: 5 days, 17:57:46, time: 3.054, data_time: 0.017, memory: 22027, loss_cls: 1.3686, loss_bbox: 2.8964, d0.loss_cls: 1.3686, d0.loss_bbox: 2.8964, d1.loss_cls: 1.3686, d1.loss_bbox: 2.8964, d2.loss_cls: 1.3686, d2.loss_bbox: 2.8964, d3.loss_cls: 1.3686, d3.loss_bbox: 2.8964, d4.loss_cls: 1.3686, d4.loss_bbox: 2.8964, dn_loss_cls: 0.9097, dn_loss_bbox: 2.5898, d0.dn_loss_cls: 0.9097, d0.dn_loss_bbox: 2.5898, d1.dn_loss_cls: 0.9097, d1.dn_loss_bbox: 2.5898, d2.dn_loss_cls: 0.9097, d2.dn_loss_bbox: 2.5898, d3.dn_loss_cls: 0.9097, d3.dn_loss_bbox: 2.5898, d4.dn_loss_cls: 0.9097, d4.dn_loss_bbox: 2.5898, loss_cls_lane: 0.6461, loss_cls_H: 0.6461, loss_bbox_lane: 6.7806, loss_bbox_H: 6.7798, d0.loss_cls_lane: 0.6461, d0.loss_cls_H: 0.6461, d0.loss_bbox_lane: 6.7806, d0.loss_bbox_H: 6.7798, d1.loss_cls_lane: 0.6461, d1.loss_cls_H: 0.6461, d1.loss_bbox_lane: 6.7806, d1.loss_bbox_H: 6.7798, d2.loss_cls_lane: 0.6461, d2.loss_cls_H: 0.6461, d2.loss_bbox_lane: 6.7806, d2.loss_bbox_H: 6.7798, d3.loss_cls_lane: 0.6461, d3.loss_cls_H: 0.6461, d3.loss_bbox_lane: 6.7806, d3.loss_bbox_H: 6.7798, d4.loss_cls_lane: 0.6461, d4.loss_cls_H: 0.6461, d4.loss_bbox_lane: 6.7806, d4.loss_bbox_H: 6.7798, vlm_loss: 0.0000, loss: 135.7028, grad_norm: nan
2025-03-01 01:46:52,624 - mmdet - INFO - Iter [4700/168780]	lr: 3.992e-04, eta: 5 days, 17:53:56, time: 2.981, data_time: 0.018, memory: 22027, loss_cls: 1.4139, loss_bbox: 2.7666, d0.loss_cls: 1.4139, d0.loss_bbox: 2.7666, d1.loss_cls: 1.4139, d1.loss_bbox: 2.7666, d2.loss_cls: 1.4139, d2.loss_bbox: 2.7666, d3.loss_cls: 1.4139, d3.loss_bbox: 2.7666, d4.loss_cls: 1.4139, d4.loss_bbox: 2.7666, dn_loss_cls: 0.9328, dn_loss_bbox: 2.4647, d0.dn_loss_cls: 0.9328, d0.dn_loss_bbox: 2.4647, d1.dn_loss_cls: 0.9328, d1.dn_loss_bbox: 2.4647, d2.dn_loss_cls: 0.9328, d2.dn_loss_bbox: 2.4647, d3.dn_loss_cls: 0.9328, d3.dn_loss_bbox: 2.4647, d4.dn_loss_cls: 0.9328, d4.dn_loss_bbox: 2.4647, loss_cls_lane: 0.6312, loss_cls_H: 0.6312, loss_bbox_lane: 6.7340, loss_bbox_H: 6.7262, d0.loss_cls_lane: 0.6312, d0.loss_cls_H: 0.6312, d0.loss_bbox_lane: 6.7340, d0.loss_bbox_H: 6.7262, d1.loss_cls_lane: 0.6312, d1.loss_cls_H: 0.6312, d1.loss_bbox_lane: 6.7340, d1.loss_bbox_H: 6.7262, d2.loss_cls_lane: 0.6312, d2.loss_cls_H: 0.6312, d2.loss_bbox_lane: 6.7340, d2.loss_bbox_H: 6.7262, d3.loss_cls_lane: 0.6312, d3.loss_cls_H: 0.6312, d3.loss_bbox_lane: 6.7340, d3.loss_bbox_H: 6.7262, d4.loss_cls_lane: 0.6312, d4.loss_cls_H: 0.6312, d4.loss_bbox_lane: 6.7340, d4.loss_bbox_H: 6.7262, vlm_loss: 0.0000, loss: 133.8032, grad_norm: nan
2025-03-01 01:49:42,473 - mmdet - INFO - Iter [4750/168780]	lr: 3.992e-04, eta: 5 days, 18:02:06, time: 3.397, data_time: 0.017, memory: 22027, loss_cls: 1.6446, loss_bbox: 2.8797, d0.loss_cls: 1.6446, d0.loss_bbox: 2.8797, d1.loss_cls: 1.6446, d1.loss_bbox: 2.8797, d2.loss_cls: 1.6446, d2.loss_bbox: 2.8797, d3.loss_cls: 1.6446, d3.loss_bbox: 2.8797, d4.loss_cls: 1.6446, d4.loss_bbox: 2.8797, dn_loss_cls: 0.9849, dn_loss_bbox: 2.5306, d0.dn_loss_cls: 0.9849, d0.dn_loss_bbox: 2.5306, d1.dn_loss_cls: 0.9849, d1.dn_loss_bbox: 2.5306, d2.dn_loss_cls: 0.9849, d2.dn_loss_bbox: 2.5306, d3.dn_loss_cls: 0.9849, d3.dn_loss_bbox: 2.5306, d4.dn_loss_cls: 0.9849, d4.dn_loss_bbox: 2.5306, loss_cls_lane: 0.6477, loss_cls_H: 0.6477, loss_bbox_lane: 6.6593, loss_bbox_H: 6.6561, d0.loss_cls_lane: 0.6477, d0.loss_cls_H: 0.6477, d0.loss_bbox_lane: 6.6593, d0.loss_bbox_H: 6.6561, d1.loss_cls_lane: 0.6477, d1.loss_cls_H: 0.6477, d1.loss_bbox_lane: 6.6593, d1.loss_bbox_H: 6.6561, d2.loss_cls_lane: 0.6477, d2.loss_cls_H: 0.6477, d2.loss_bbox_lane: 6.6593, d2.loss_bbox_H: 6.6561, d3.loss_cls_lane: 0.6477, d3.loss_cls_H: 0.6477, d3.loss_bbox_lane: 6.6593, d3.loss_bbox_H: 6.6561, d4.loss_cls_lane: 0.6477, d4.loss_cls_H: 0.6477, d4.loss_bbox_lane: 6.6593, d4.loss_bbox_H: 6.6561, vlm_loss: 0.0000, loss: 135.9044, grad_norm: nan
2025-03-01 01:52:09,831 - mmdet - INFO - Iter [4800/168780]	lr: 3.992e-04, eta: 5 days, 17:57:14, time: 2.947, data_time: 0.016, memory: 22027, loss_cls: 2.0247, loss_bbox: 2.9143, d0.loss_cls: 2.0247, d0.loss_bbox: 2.9143, d1.loss_cls: 2.0247, d1.loss_bbox: 2.9143, d2.loss_cls: 2.0247, d2.loss_bbox: 2.9143, d3.loss_cls: 2.0247, d3.loss_bbox: 2.9143, d4.loss_cls: 2.0247, d4.loss_bbox: 2.9143, dn_loss_cls: 0.8582, dn_loss_bbox: 2.4895, d0.dn_loss_cls: 0.8582, d0.dn_loss_bbox: 2.4895, d1.dn_loss_cls: 0.8582, d1.dn_loss_bbox: 2.4895, d2.dn_loss_cls: 0.8582, d2.dn_loss_bbox: 2.4895, d3.dn_loss_cls: 0.8582, d3.dn_loss_bbox: 2.4895, d4.dn_loss_cls: 0.8582, d4.dn_loss_bbox: 2.4895, loss_cls_lane: 0.6389, loss_cls_H: 0.6389, loss_bbox_lane: 6.3648, loss_bbox_H: 6.3621, d0.loss_cls_lane: 0.6389, d0.loss_cls_H: 0.6389, d0.loss_bbox_lane: 6.3648, d0.loss_bbox_H: 6.3621, d1.loss_cls_lane: 0.6389, d1.loss_cls_H: 0.6389, d1.loss_bbox_lane: 6.3648, d1.loss_bbox_H: 6.3621, d2.loss_cls_lane: 0.6389, d2.loss_cls_H: 0.6389, d2.loss_bbox_lane: 6.3648, d2.loss_bbox_H: 6.3621, d3.loss_cls_lane: 0.6389, d3.loss_cls_H: 0.6389, d3.loss_bbox_lane: 6.3648, d3.loss_bbox_H: 6.3621, d4.loss_cls_lane: 0.6389, d4.loss_cls_H: 0.6389, d4.loss_bbox_lane: 6.3648, d4.loss_bbox_H: 6.3621, vlm_loss: 0.0000, loss: 133.7479, grad_norm: nan
2025-03-01 01:54:40,293 - mmdet - INFO - Iter [4850/168780]	lr: 3.992e-04, eta: 5 days, 17:54:10, time: 3.009, data_time: 0.018, memory: 22027, loss_cls: 1.2627, loss_bbox: 2.7317, d0.loss_cls: 1.2627, d0.loss_bbox: 2.7317, d1.loss_cls: 1.2627, d1.loss_bbox: 2.7317, d2.loss_cls: 1.2627, d2.loss_bbox: 2.7317, d3.loss_cls: 1.2627, d3.loss_bbox: 2.7317, d4.loss_cls: 1.2627, d4.loss_bbox: 2.7317, dn_loss_cls: 0.9241, dn_loss_bbox: 2.4210, d0.dn_loss_cls: 0.9241, d0.dn_loss_bbox: 2.4210, d1.dn_loss_cls: 0.9241, d1.dn_loss_bbox: 2.4210, d2.dn_loss_cls: 0.9241, d2.dn_loss_bbox: 2.4210, d3.dn_loss_cls: 0.9241, d3.dn_loss_bbox: 2.4210, d4.dn_loss_cls: 0.9241, d4.dn_loss_bbox: 2.4210, loss_cls_lane: 0.6381, loss_cls_H: 0.6381, loss_bbox_lane: 6.4804, loss_bbox_H: 6.4856, d0.loss_cls_lane: 0.6381, d0.loss_cls_H: 0.6381, d0.loss_bbox_lane: 6.4804, d0.loss_bbox_H: 6.4856, d1.loss_cls_lane: 0.6381, d1.loss_cls_H: 0.6381, d1.loss_bbox_lane: 6.4804, d1.loss_bbox_H: 6.4856, d2.loss_cls_lane: 0.6381, d2.loss_cls_H: 0.6381, d2.loss_bbox_lane: 6.4804, d2.loss_bbox_H: 6.4856, d3.loss_cls_lane: 0.6381, d3.loss_cls_H: 0.6381, d3.loss_bbox_lane: 6.4804, d3.loss_bbox_H: 6.4856, d4.loss_cls_lane: 0.6381, d4.loss_cls_H: 0.6381, d4.loss_bbox_lane: 6.4804, d4.loss_bbox_H: 6.4856, vlm_loss: 0.0000, loss: 129.4894, grad_norm: nan
2025-03-01 01:57:10,139 - mmdet - INFO - Iter [4900/168780]	lr: 3.992e-04, eta: 5 days, 17:50:46, time: 2.997, data_time: 0.018, memory: 22027, loss_cls: 1.4416, loss_bbox: 2.7263, d0.loss_cls: 1.4416, d0.loss_bbox: 2.7263, d1.loss_cls: 1.4416, d1.loss_bbox: 2.7263, d2.loss_cls: 1.4416, d2.loss_bbox: 2.7263, d3.loss_cls: 1.4416, d3.loss_bbox: 2.7263, d4.loss_cls: 1.4416, d4.loss_bbox: 2.7263, dn_loss_cls: 0.9830, dn_loss_bbox: 2.3204, d0.dn_loss_cls: 0.9830, d0.dn_loss_bbox: 2.3204, d1.dn_loss_cls: 0.9830, d1.dn_loss_bbox: 2.3204, d2.dn_loss_cls: 0.9830, d2.dn_loss_bbox: 2.3204, d3.dn_loss_cls: 0.9830, d3.dn_loss_bbox: 2.3204, d4.dn_loss_cls: 0.9830, d4.dn_loss_bbox: 2.3204, loss_cls_lane: 0.6629, loss_cls_H: 0.6629, loss_bbox_lane: 6.9403, loss_bbox_H: 6.9525, d0.loss_cls_lane: 0.6629, d0.loss_cls_H: 0.6629, d0.loss_bbox_lane: 6.9403, d0.loss_bbox_H: 6.9525, d1.loss_cls_lane: 0.6629, d1.loss_cls_H: 0.6629, d1.loss_bbox_lane: 6.9403, d1.loss_bbox_H: 6.9525, d2.loss_cls_lane: 0.6629, d2.loss_cls_H: 0.6629, d2.loss_bbox_lane: 6.9403, d2.loss_bbox_H: 6.9525, d3.loss_cls_lane: 0.6629, d3.loss_cls_H: 0.6629, d3.loss_bbox_lane: 6.9403, d3.loss_bbox_H: 6.9525, d4.loss_cls_lane: 0.6629, d4.loss_cls_H: 0.6629, d4.loss_bbox_lane: 6.9403, d4.loss_bbox_H: 6.9525, vlm_loss: 0.0000, loss: 136.1395, grad_norm: nan
2025-03-01 01:59:38,541 - mmdet - INFO - Iter [4950/168780]	lr: 3.992e-04, eta: 5 days, 17:46:35, time: 2.968, data_time: 0.017, memory: 22027, loss_cls: 1.5453, loss_bbox: 2.8470, d0.loss_cls: 1.5453, d0.loss_bbox: 2.8470, d1.loss_cls: 1.5453, d1.loss_bbox: 2.8470, d2.loss_cls: 1.5453, d2.loss_bbox: 2.8470, d3.loss_cls: 1.5453, d3.loss_bbox: 2.8470, d4.loss_cls: 1.5453, d4.loss_bbox: 2.8470, dn_loss_cls: 0.8683, dn_loss_bbox: 2.4034, d0.dn_loss_cls: 0.8683, d0.dn_loss_bbox: 2.4034, d1.dn_loss_cls: 0.8683, d1.dn_loss_bbox: 2.4034, d2.dn_loss_cls: 0.8683, d2.dn_loss_bbox: 2.4034, d3.dn_loss_cls: 0.8683, d3.dn_loss_bbox: 2.4034, d4.dn_loss_cls: 0.8683, d4.dn_loss_bbox: 2.4034, loss_cls_lane: 0.6489, loss_cls_H: 0.6489, loss_bbox_lane: 6.5667, loss_bbox_H: 6.5548, d0.loss_cls_lane: 0.6489, d0.loss_cls_H: 0.6489, d0.loss_bbox_lane: 6.5667, d0.loss_bbox_H: 6.5548, d1.loss_cls_lane: 0.6489, d1.loss_cls_H: 0.6489, d1.loss_bbox_lane: 6.5667, d1.loss_bbox_H: 6.5548, d2.loss_cls_lane: 0.6489, d2.loss_cls_H: 0.6489, d2.loss_bbox_lane: 6.5667, d2.loss_bbox_H: 6.5548, d3.loss_cls_lane: 0.6489, d3.loss_cls_H: 0.6489, d3.loss_bbox_lane: 6.5667, d3.loss_bbox_H: 6.5548, d4.loss_cls_lane: 0.6489, d4.loss_cls_H: 0.6489, d4.loss_bbox_lane: 6.5667, d4.loss_bbox_H: 6.5548, vlm_loss: 0.0000, loss: 132.4998, grad_norm: nan
2025-03-01 02:02:07,319 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 02:02:07,319 - mmdet - INFO - Iter [5000/168780]	lr: 3.991e-04, eta: 5 days, 17:42:38, time: 2.976, data_time: 0.018, memory: 22027, loss_cls: 1.4958, loss_bbox: 2.6581, d0.loss_cls: 1.4958, d0.loss_bbox: 2.6581, d1.loss_cls: 1.4958, d1.loss_bbox: 2.6581, d2.loss_cls: 1.4958, d2.loss_bbox: 2.6581, d3.loss_cls: 1.4958, d3.loss_bbox: 2.6581, d4.loss_cls: 1.4958, d4.loss_bbox: 2.6581, dn_loss_cls: 0.9099, dn_loss_bbox: 2.3634, d0.dn_loss_cls: 0.9099, d0.dn_loss_bbox: 2.3634, d1.dn_loss_cls: 0.9099, d1.dn_loss_bbox: 2.3634, d2.dn_loss_cls: 0.9099, d2.dn_loss_bbox: 2.3634, d3.dn_loss_cls: 0.9099, d3.dn_loss_bbox: 2.3634, d4.dn_loss_cls: 0.9099, d4.dn_loss_bbox: 2.3634, loss_cls_lane: 0.6438, loss_cls_H: 0.6438, loss_bbox_lane: 6.4859, loss_bbox_H: 6.4896, d0.loss_cls_lane: 0.6438, d0.loss_cls_H: 0.6438, d0.loss_bbox_lane: 6.4859, d0.loss_bbox_H: 6.4896, d1.loss_cls_lane: 0.6438, d1.loss_cls_H: 0.6438, d1.loss_bbox_lane: 6.4859, d1.loss_bbox_H: 6.4896, d2.loss_cls_lane: 0.6438, d2.loss_cls_H: 0.6438, d2.loss_bbox_lane: 6.4859, d2.loss_bbox_H: 6.4896, d3.loss_cls_lane: 0.6438, d3.loss_cls_H: 0.6438, d3.loss_bbox_lane: 6.4859, d3.loss_bbox_H: 6.4896, d4.loss_cls_lane: 0.6438, d4.loss_cls_H: 0.6438, d4.loss_bbox_lane: 6.4859, d4.loss_bbox_H: 6.4896, vlm_loss: 0.0000, loss: 130.1414, grad_norm: nan
2025-03-01 02:04:36,773 - mmdet - INFO - Iter [5050/168780]	lr: 3.991e-04, eta: 5 days, 17:39:06, time: 2.989, data_time: 0.018, memory: 22027, loss_cls: 1.3477, loss_bbox: 2.6480, d0.loss_cls: 1.3477, d0.loss_bbox: 2.6480, d1.loss_cls: 1.3477, d1.loss_bbox: 2.6480, d2.loss_cls: 1.3477, d2.loss_bbox: 2.6480, d3.loss_cls: 1.3477, d3.loss_bbox: 2.6480, d4.loss_cls: 1.3477, d4.loss_bbox: 2.6480, dn_loss_cls: 0.9311, dn_loss_bbox: 2.3344, d0.dn_loss_cls: 0.9311, d0.dn_loss_bbox: 2.3344, d1.dn_loss_cls: 0.9311, d1.dn_loss_bbox: 2.3344, d2.dn_loss_cls: 0.9311, d2.dn_loss_bbox: 2.3344, d3.dn_loss_cls: 0.9311, d3.dn_loss_bbox: 2.3344, d4.dn_loss_cls: 0.9311, d4.dn_loss_bbox: 2.3344, loss_cls_lane: 0.6364, loss_cls_H: 0.6364, loss_bbox_lane: 6.4242, loss_bbox_H: 6.4179, d0.loss_cls_lane: 0.6364, d0.loss_cls_H: 0.6364, d0.loss_bbox_lane: 6.4242, d0.loss_bbox_H: 6.4179, d1.loss_cls_lane: 0.6364, d1.loss_cls_H: 0.6364, d1.loss_bbox_lane: 6.4242, d1.loss_bbox_H: 6.4179, d2.loss_cls_lane: 0.6364, d2.loss_cls_H: 0.6364, d2.loss_bbox_lane: 6.4242, d2.loss_bbox_H: 6.4179, d3.loss_cls_lane: 0.6364, d3.loss_cls_H: 0.6364, d3.loss_bbox_lane: 6.4242, d3.loss_bbox_H: 6.4179, d4.loss_cls_lane: 0.6364, d4.loss_cls_H: 0.6364, d4.loss_bbox_lane: 6.4242, d4.loss_bbox_H: 6.4179, vlm_loss: 0.0000, loss: 128.2568, grad_norm: nan
2025-03-01 02:07:05,525 - mmdet - INFO - Iter [5100/168780]	lr: 3.991e-04, eta: 5 days, 17:35:12, time: 2.975, data_time: 0.017, memory: 22027, loss_cls: 1.4654, loss_bbox: 2.7992, d0.loss_cls: 1.4654, d0.loss_bbox: 2.7992, d1.loss_cls: 1.4654, d1.loss_bbox: 2.7992, d2.loss_cls: 1.4654, d2.loss_bbox: 2.7992, d3.loss_cls: 1.4654, d3.loss_bbox: 2.7992, d4.loss_cls: 1.4654, d4.loss_bbox: 2.7992, dn_loss_cls: 0.9485, dn_loss_bbox: 2.3852, d0.dn_loss_cls: 0.9485, d0.dn_loss_bbox: 2.3852, d1.dn_loss_cls: 0.9485, d1.dn_loss_bbox: 2.3852, d2.dn_loss_cls: 0.9485, d2.dn_loss_bbox: 2.3852, d3.dn_loss_cls: 0.9485, d3.dn_loss_bbox: 2.3852, d4.dn_loss_cls: 0.9485, d4.dn_loss_bbox: 2.3852, loss_cls_lane: 0.6435, loss_cls_H: 0.6435, loss_bbox_lane: 6.5901, loss_bbox_H: 6.5760, d0.loss_cls_lane: 0.6435, d0.loss_cls_H: 0.6435, d0.loss_bbox_lane: 6.5901, d0.loss_bbox_H: 6.5760, d1.loss_cls_lane: 0.6435, d1.loss_cls_H: 0.6435, d1.loss_bbox_lane: 6.5901, d1.loss_bbox_H: 6.5760, d2.loss_cls_lane: 0.6435, d2.loss_cls_H: 0.6435, d2.loss_bbox_lane: 6.5901, d2.loss_bbox_H: 6.5760, d3.loss_cls_lane: 0.6435, d3.loss_cls_H: 0.6435, d3.loss_bbox_lane: 6.5901, d3.loss_bbox_H: 6.5760, d4.loss_cls_lane: 0.6435, d4.loss_cls_H: 0.6435, d4.loss_bbox_lane: 6.5901, d4.loss_bbox_H: 6.5760, vlm_loss: 0.0000, loss: 132.3081, grad_norm: nan
2025-03-01 02:09:33,728 - mmdet - INFO - Iter [5150/168780]	lr: 3.991e-04, eta: 5 days, 17:31:02, time: 2.964, data_time: 0.018, memory: 22027, loss_cls: 1.8791, loss_bbox: 2.8602, d0.loss_cls: 1.8791, d0.loss_bbox: 2.8602, d1.loss_cls: 1.8791, d1.loss_bbox: 2.8602, d2.loss_cls: 1.8791, d2.loss_bbox: 2.8602, d3.loss_cls: 1.8791, d3.loss_bbox: 2.8602, d4.loss_cls: 1.8791, d4.loss_bbox: 2.8602, dn_loss_cls: 0.9033, dn_loss_bbox: 2.5980, d0.dn_loss_cls: 0.9033, d0.dn_loss_bbox: 2.5980, d1.dn_loss_cls: 0.9033, d1.dn_loss_bbox: 2.5980, d2.dn_loss_cls: 0.9033, d2.dn_loss_bbox: 2.5980, d3.dn_loss_cls: 0.9033, d3.dn_loss_bbox: 2.5980, d4.dn_loss_cls: 0.9033, d4.dn_loss_bbox: 2.5980, loss_cls_lane: 0.6460, loss_cls_H: 0.6460, loss_bbox_lane: 6.5283, loss_bbox_H: 6.5363, d0.loss_cls_lane: 0.6460, d0.loss_cls_H: 0.6460, d0.loss_bbox_lane: 6.5283, d0.loss_bbox_H: 6.5363, d1.loss_cls_lane: 0.6460, d1.loss_cls_H: 0.6460, d1.loss_bbox_lane: 6.5283, d1.loss_bbox_H: 6.5363, d2.loss_cls_lane: 0.6460, d2.loss_cls_H: 0.6460, d2.loss_bbox_lane: 6.5283, d2.loss_bbox_H: 6.5363, d3.loss_cls_lane: 0.6460, d3.loss_cls_H: 0.6460, d3.loss_bbox_lane: 6.5283, d3.loss_bbox_H: 6.5363, d4.loss_cls_lane: 0.6460, d4.loss_cls_H: 0.6460, d4.loss_bbox_lane: 6.5283, d4.loss_bbox_H: 6.5363, vlm_loss: 0.0000, loss: 135.5830, grad_norm: nan
2025-03-01 02:12:02,448 - mmdet - INFO - Iter [5200/168780]	lr: 3.991e-04, eta: 5 days, 17:27:10, time: 2.974, data_time: 0.017, memory: 22027, loss_cls: 1.4860, loss_bbox: 2.6400, d0.loss_cls: 1.4860, d0.loss_bbox: 2.6400, d1.loss_cls: 1.4860, d1.loss_bbox: 2.6400, d2.loss_cls: 1.4860, d2.loss_bbox: 2.6400, d3.loss_cls: 1.4860, d3.loss_bbox: 2.6400, d4.loss_cls: 1.4860, d4.loss_bbox: 2.6400, dn_loss_cls: 0.8637, dn_loss_bbox: 2.3678, d0.dn_loss_cls: 0.8637, d0.dn_loss_bbox: 2.3678, d1.dn_loss_cls: 0.8637, d1.dn_loss_bbox: 2.3678, d2.dn_loss_cls: 0.8637, d2.dn_loss_bbox: 2.3678, d3.dn_loss_cls: 0.8637, d3.dn_loss_bbox: 2.3678, d4.dn_loss_cls: 0.8637, d4.dn_loss_bbox: 2.3678, loss_cls_lane: 0.6489, loss_cls_H: 0.6489, loss_bbox_lane: 6.3709, loss_bbox_H: 6.3800, d0.loss_cls_lane: 0.6489, d0.loss_cls_H: 0.6489, d0.loss_bbox_lane: 6.3709, d0.loss_bbox_H: 6.3800, d1.loss_cls_lane: 0.6489, d1.loss_cls_H: 0.6489, d1.loss_bbox_lane: 6.3709, d1.loss_bbox_H: 6.3800, d2.loss_cls_lane: 0.6489, d2.loss_cls_H: 0.6489, d2.loss_bbox_lane: 6.3709, d2.loss_bbox_H: 6.3800, d3.loss_cls_lane: 0.6489, d3.loss_cls_H: 0.6489, d3.loss_bbox_lane: 6.3709, d3.loss_bbox_H: 6.3800, d4.loss_cls_lane: 0.6489, d4.loss_cls_H: 0.6489, d4.loss_bbox_lane: 6.3709, d4.loss_bbox_H: 6.3800, vlm_loss: 0.0000, loss: 128.4357, grad_norm: nan
2025-03-01 02:14:33,727 - mmdet - INFO - Iter [5250/168780]	lr: 3.990e-04, eta: 5 days, 17:24:40, time: 3.026, data_time: 0.017, memory: 22027, loss_cls: 1.2307, loss_bbox: 2.7923, d0.loss_cls: 1.2307, d0.loss_bbox: 2.7923, d1.loss_cls: 1.2307, d1.loss_bbox: 2.7923, d2.loss_cls: 1.2307, d2.loss_bbox: 2.7923, d3.loss_cls: 1.2307, d3.loss_bbox: 2.7923, d4.loss_cls: 1.2307, d4.loss_bbox: 2.7923, dn_loss_cls: 0.9516, dn_loss_bbox: 2.3441, d0.dn_loss_cls: 0.9516, d0.dn_loss_bbox: 2.3441, d1.dn_loss_cls: 0.9516, d1.dn_loss_bbox: 2.3441, d2.dn_loss_cls: 0.9516, d2.dn_loss_bbox: 2.3441, d3.dn_loss_cls: 0.9516, d3.dn_loss_bbox: 2.3441, d4.dn_loss_cls: 0.9516, d4.dn_loss_bbox: 2.3441, loss_cls_lane: 0.6233, loss_cls_H: 0.6233, loss_bbox_lane: 6.5731, loss_bbox_H: 6.5625, d0.loss_cls_lane: 0.6233, d0.loss_cls_H: 0.6233, d0.loss_bbox_lane: 6.5731, d0.loss_bbox_H: 6.5625, d1.loss_cls_lane: 0.6233, d1.loss_cls_H: 0.6233, d1.loss_bbox_lane: 6.5731, d1.loss_bbox_H: 6.5625, d2.loss_cls_lane: 0.6233, d2.loss_cls_H: 0.6233, d2.loss_bbox_lane: 6.5731, d2.loss_bbox_H: 6.5625, d3.loss_cls_lane: 0.6233, d3.loss_cls_H: 0.6233, d3.loss_bbox_lane: 6.5731, d3.loss_bbox_H: 6.5625, d4.loss_cls_lane: 0.6233, d4.loss_cls_H: 0.6233, d4.loss_bbox_lane: 6.5731, d4.loss_bbox_H: 6.5625, vlm_loss: 0.0000, loss: 130.2043, grad_norm: nan
2025-03-01 02:17:01,219 - mmdet - INFO - Iter [5300/168780]	lr: 3.990e-04, eta: 5 days, 17:20:12, time: 2.950, data_time: 0.017, memory: 22027, loss_cls: 1.7590, loss_bbox: 2.8509, d0.loss_cls: 1.7590, d0.loss_bbox: 2.8509, d1.loss_cls: 1.7590, d1.loss_bbox: 2.8509, d2.loss_cls: 1.7590, d2.loss_bbox: 2.8509, d3.loss_cls: 1.7590, d3.loss_bbox: 2.8509, d4.loss_cls: 1.7590, d4.loss_bbox: 2.8509, dn_loss_cls: 0.9301, dn_loss_bbox: 2.5959, d0.dn_loss_cls: 0.9301, d0.dn_loss_bbox: 2.5959, d1.dn_loss_cls: 0.9301, d1.dn_loss_bbox: 2.5959, d2.dn_loss_cls: 0.9301, d2.dn_loss_bbox: 2.5959, d3.dn_loss_cls: 0.9301, d3.dn_loss_bbox: 2.5959, d4.dn_loss_cls: 0.9301, d4.dn_loss_bbox: 2.5959, loss_cls_lane: 0.6331, loss_cls_H: 0.6331, loss_bbox_lane: 6.2823, loss_bbox_H: 6.2809, d0.loss_cls_lane: 0.6331, d0.loss_cls_H: 0.6331, d0.loss_bbox_lane: 6.2823, d0.loss_bbox_H: 6.2809, d1.loss_cls_lane: 0.6331, d1.loss_cls_H: 0.6331, d1.loss_bbox_lane: 6.2823, d1.loss_bbox_H: 6.2809, d2.loss_cls_lane: 0.6331, d2.loss_cls_H: 0.6331, d2.loss_bbox_lane: 6.2823, d2.loss_bbox_H: 6.2809, d3.loss_cls_lane: 0.6331, d3.loss_cls_H: 0.6331, d3.loss_bbox_lane: 6.2823, d3.loss_bbox_H: 6.2809, d4.loss_cls_lane: 0.6331, d4.loss_cls_H: 0.6331, d4.loss_bbox_lane: 6.2823, d4.loss_bbox_H: 6.2809, vlm_loss: 0.0000, loss: 131.7922, grad_norm: nan
2025-03-01 02:19:30,005 - mmdet - INFO - Iter [5350/168780]	lr: 3.990e-04, eta: 5 days, 17:16:27, time: 2.976, data_time: 0.017, memory: 22027, loss_cls: 1.5084, loss_bbox: 3.0380, d0.loss_cls: 1.5084, d0.loss_bbox: 3.0380, d1.loss_cls: 1.5084, d1.loss_bbox: 3.0380, d2.loss_cls: 1.5084, d2.loss_bbox: 3.0380, d3.loss_cls: 1.5084, d3.loss_bbox: 3.0380, d4.loss_cls: 1.5084, d4.loss_bbox: 3.0380, dn_loss_cls: 0.9574, dn_loss_bbox: 2.8607, d0.dn_loss_cls: 0.9574, d0.dn_loss_bbox: 2.8607, d1.dn_loss_cls: 0.9574, d1.dn_loss_bbox: 2.8607, d2.dn_loss_cls: 0.9574, d2.dn_loss_bbox: 2.8607, d3.dn_loss_cls: 0.9574, d3.dn_loss_bbox: 2.8607, d4.dn_loss_cls: 0.9574, d4.dn_loss_bbox: 2.8607, loss_cls_lane: 0.6538, loss_cls_H: 0.6538, loss_bbox_lane: 6.3020, loss_bbox_H: 6.3094, d0.loss_cls_lane: 0.6538, d0.loss_cls_H: 0.6538, d0.loss_bbox_lane: 6.3020, d0.loss_bbox_H: 6.3094, d1.loss_cls_lane: 0.6538, d1.loss_cls_H: 0.6538, d1.loss_bbox_lane: 6.3020, d1.loss_bbox_H: 6.3094, d2.loss_cls_lane: 0.6538, d2.loss_cls_H: 0.6538, d2.loss_bbox_lane: 6.3020, d2.loss_bbox_H: 6.3094, d3.loss_cls_lane: 0.6538, d3.loss_cls_H: 0.6538, d3.loss_bbox_lane: 6.3020, d3.loss_bbox_H: 6.3094, d4.loss_cls_lane: 0.6538, d4.loss_cls_H: 0.6538, d4.loss_bbox_lane: 6.3020, d4.loss_bbox_H: 6.3094, vlm_loss: 0.0000, loss: 133.7023, grad_norm: nan
2025-03-01 02:21:58,888 - mmdet - INFO - Iter [5400/168780]	lr: 3.990e-04, eta: 5 days, 17:12:46, time: 2.978, data_time: 0.018, memory: 22027, loss_cls: 1.3578, loss_bbox: 2.7360, d0.loss_cls: 1.3578, d0.loss_bbox: 2.7360, d1.loss_cls: 1.3578, d1.loss_bbox: 2.7360, d2.loss_cls: 1.3578, d2.loss_bbox: 2.7360, d3.loss_cls: 1.3578, d3.loss_bbox: 2.7360, d4.loss_cls: 1.3578, d4.loss_bbox: 2.7360, dn_loss_cls: 0.9433, dn_loss_bbox: 2.4489, d0.dn_loss_cls: 0.9433, d0.dn_loss_bbox: 2.4489, d1.dn_loss_cls: 0.9433, d1.dn_loss_bbox: 2.4489, d2.dn_loss_cls: 0.9433, d2.dn_loss_bbox: 2.4489, d3.dn_loss_cls: 0.9433, d3.dn_loss_bbox: 2.4489, d4.dn_loss_cls: 0.9433, d4.dn_loss_bbox: 2.4489, loss_cls_lane: 0.6354, loss_cls_H: 0.6354, loss_bbox_lane: 6.3917, loss_bbox_H: 6.3970, d0.loss_cls_lane: 0.6354, d0.loss_cls_H: 0.6354, d0.loss_bbox_lane: 6.3917, d0.loss_bbox_H: 6.3970, d1.loss_cls_lane: 0.6354, d1.loss_cls_H: 0.6354, d1.loss_bbox_lane: 6.3917, d1.loss_bbox_H: 6.3970, d2.loss_cls_lane: 0.6354, d2.loss_cls_H: 0.6354, d2.loss_bbox_lane: 6.3917, d2.loss_bbox_H: 6.3970, d3.loss_cls_lane: 0.6354, d3.loss_cls_H: 0.6354, d3.loss_bbox_lane: 6.3917, d3.loss_bbox_H: 6.3970, d4.loss_cls_lane: 0.6354, d4.loss_cls_H: 0.6354, d4.loss_bbox_lane: 6.3917, d4.loss_bbox_H: 6.3970, vlm_loss: 0.0000, loss: 129.2728, grad_norm: nan
2025-03-01 02:24:30,364 - mmdet - INFO - Iter [5450/168780]	lr: 3.990e-04, eta: 5 days, 17:10:24, time: 3.030, data_time: 0.019, memory: 22027, loss_cls: 1.4587, loss_bbox: 2.6779, d0.loss_cls: 1.4587, d0.loss_bbox: 2.6779, d1.loss_cls: 1.4587, d1.loss_bbox: 2.6779, d2.loss_cls: 1.4587, d2.loss_bbox: 2.6779, d3.loss_cls: 1.4587, d3.loss_bbox: 2.6779, d4.loss_cls: 1.4587, d4.loss_bbox: 2.6779, dn_loss_cls: 0.9830, dn_loss_bbox: 2.3458, d0.dn_loss_cls: 0.9830, d0.dn_loss_bbox: 2.3458, d1.dn_loss_cls: 0.9830, d1.dn_loss_bbox: 2.3458, d2.dn_loss_cls: 0.9830, d2.dn_loss_bbox: 2.3458, d3.dn_loss_cls: 0.9830, d3.dn_loss_bbox: 2.3458, d4.dn_loss_cls: 0.9830, d4.dn_loss_bbox: 2.3458, loss_cls_lane: 0.6289, loss_cls_H: 0.6289, loss_bbox_lane: 6.2645, loss_bbox_H: 6.2862, d0.loss_cls_lane: 0.6289, d0.loss_cls_H: 0.6289, d0.loss_bbox_lane: 6.2645, d0.loss_bbox_H: 6.2862, d1.loss_cls_lane: 0.6289, d1.loss_cls_H: 0.6289, d1.loss_bbox_lane: 6.2645, d1.loss_bbox_H: 6.2862, d2.loss_cls_lane: 0.6289, d2.loss_cls_H: 0.6289, d2.loss_bbox_lane: 6.2645, d2.loss_bbox_H: 6.2862, d3.loss_cls_lane: 0.6289, d3.loss_cls_H: 0.6289, d3.loss_bbox_lane: 6.2645, d3.loss_bbox_H: 6.2862, d4.loss_cls_lane: 0.6289, d4.loss_cls_H: 0.6289, d4.loss_bbox_lane: 6.2645, d4.loss_bbox_H: 6.2862, vlm_loss: 0.0000, loss: 127.6428, grad_norm: nan
2025-03-01 02:27:00,302 - mmdet - INFO - Iter [5500/168780]	lr: 3.990e-04, eta: 5 days, 17:07:16, time: 2.999, data_time: 0.018, memory: 22027, loss_cls: 1.4792, loss_bbox: 2.9653, d0.loss_cls: 1.4792, d0.loss_bbox: 2.9653, d1.loss_cls: 1.4792, d1.loss_bbox: 2.9653, d2.loss_cls: 1.4792, d2.loss_bbox: 2.9653, d3.loss_cls: 1.4792, d3.loss_bbox: 2.9653, d4.loss_cls: 1.4792, d4.loss_bbox: 2.9653, dn_loss_cls: 0.9176, dn_loss_bbox: 2.5867, d0.dn_loss_cls: 0.9176, d0.dn_loss_bbox: 2.5867, d1.dn_loss_cls: 0.9176, d1.dn_loss_bbox: 2.5867, d2.dn_loss_cls: 0.9176, d2.dn_loss_bbox: 2.5867, d3.dn_loss_cls: 0.9176, d3.dn_loss_bbox: 2.5867, d4.dn_loss_cls: 0.9176, d4.dn_loss_bbox: 2.5867, loss_cls_lane: 0.6351, loss_cls_H: 0.6351, loss_bbox_lane: 6.5130, loss_bbox_H: 6.4987, d0.loss_cls_lane: 0.6351, d0.loss_cls_H: 0.6351, d0.loss_bbox_lane: 6.5130, d0.loss_bbox_H: 6.4987, d1.loss_cls_lane: 0.6351, d1.loss_cls_H: 0.6351, d1.loss_bbox_lane: 6.5130, d1.loss_bbox_H: 6.4987, d2.loss_cls_lane: 0.6351, d2.loss_cls_H: 0.6351, d2.loss_bbox_lane: 6.5130, d2.loss_bbox_H: 6.4987, d3.loss_cls_lane: 0.6351, d3.loss_cls_H: 0.6351, d3.loss_bbox_lane: 6.5130, d3.loss_bbox_H: 6.4987, d4.loss_cls_lane: 0.6351, d4.loss_cls_H: 0.6351, d4.loss_bbox_lane: 6.5130, d4.loss_bbox_H: 6.4987, vlm_loss: 0.0000, loss: 133.3840, grad_norm: nan
2025-03-01 02:29:29,618 - mmdet - INFO - Iter [5550/168780]	lr: 3.989e-04, eta: 5 days, 17:03:51, time: 2.986, data_time: 0.018, memory: 22027, loss_cls: 1.3352, loss_bbox: 2.7101, d0.loss_cls: 1.3352, d0.loss_bbox: 2.7101, d1.loss_cls: 1.3352, d1.loss_bbox: 2.7101, d2.loss_cls: 1.3352, d2.loss_bbox: 2.7101, d3.loss_cls: 1.3352, d3.loss_bbox: 2.7101, d4.loss_cls: 1.3352, d4.loss_bbox: 2.7101, dn_loss_cls: 0.9542, dn_loss_bbox: 2.4067, d0.dn_loss_cls: 0.9542, d0.dn_loss_bbox: 2.4067, d1.dn_loss_cls: 0.9542, d1.dn_loss_bbox: 2.4067, d2.dn_loss_cls: 0.9542, d2.dn_loss_bbox: 2.4067, d3.dn_loss_cls: 0.9542, d3.dn_loss_bbox: 2.4067, d4.dn_loss_cls: 0.9542, d4.dn_loss_bbox: 2.4067, loss_cls_lane: 0.6288, loss_cls_H: 0.6288, loss_bbox_lane: 6.4594, loss_bbox_H: 6.4592, d0.loss_cls_lane: 0.6288, d0.loss_cls_H: 0.6288, d0.loss_bbox_lane: 6.4594, d0.loss_bbox_H: 6.4592, d1.loss_cls_lane: 0.6288, d1.loss_cls_H: 0.6288, d1.loss_bbox_lane: 6.4594, d1.loss_bbox_H: 6.4592, d2.loss_cls_lane: 0.6288, d2.loss_cls_H: 0.6288, d2.loss_bbox_lane: 6.4594, d2.loss_bbox_H: 6.4592, d3.loss_cls_lane: 0.6288, d3.loss_cls_H: 0.6288, d3.loss_bbox_lane: 6.4594, d3.loss_bbox_H: 6.4592, d4.loss_cls_lane: 0.6288, d4.loss_cls_H: 0.6288, d4.loss_bbox_lane: 6.4594, d4.loss_bbox_H: 6.4592, vlm_loss: 0.0000, loss: 129.4931, grad_norm: nan
2025-03-01 02:31:57,575 - mmdet - INFO - Iter [5600/168780]	lr: 3.989e-04, eta: 5 days, 16:59:47, time: 2.959, data_time: 0.017, memory: 22027, loss_cls: 1.5304, loss_bbox: 2.8080, d0.loss_cls: 1.5304, d0.loss_bbox: 2.8080, d1.loss_cls: 1.5304, d1.loss_bbox: 2.8080, d2.loss_cls: 1.5304, d2.loss_bbox: 2.8080, d3.loss_cls: 1.5304, d3.loss_bbox: 2.8080, d4.loss_cls: 1.5304, d4.loss_bbox: 2.8080, dn_loss_cls: 0.9236, dn_loss_bbox: 2.5493, d0.dn_loss_cls: 0.9236, d0.dn_loss_bbox: 2.5493, d1.dn_loss_cls: 0.9236, d1.dn_loss_bbox: 2.5493, d2.dn_loss_cls: 0.9236, d2.dn_loss_bbox: 2.5493, d3.dn_loss_cls: 0.9236, d3.dn_loss_bbox: 2.5493, d4.dn_loss_cls: 0.9236, d4.dn_loss_bbox: 2.5493, loss_cls_lane: 0.6629, loss_cls_H: 0.6629, loss_bbox_lane: 6.6196, loss_bbox_H: 6.6190, d0.loss_cls_lane: 0.6629, d0.loss_cls_H: 0.6629, d0.loss_bbox_lane: 6.6196, d0.loss_bbox_H: 6.6190, d1.loss_cls_lane: 0.6629, d1.loss_cls_H: 0.6629, d1.loss_bbox_lane: 6.6196, d1.loss_bbox_H: 6.6190, d2.loss_cls_lane: 0.6629, d2.loss_cls_H: 0.6629, d2.loss_bbox_lane: 6.6196, d2.loss_bbox_H: 6.6190, d3.loss_cls_lane: 0.6629, d3.loss_cls_H: 0.6629, d3.loss_bbox_lane: 6.6196, d3.loss_bbox_H: 6.6190, d4.loss_cls_lane: 0.6629, d4.loss_cls_H: 0.6629, d4.loss_bbox_lane: 6.6196, d4.loss_bbox_H: 6.6190, vlm_loss: 0.0000, loss: 134.2539, grad_norm: nan
2025-03-01 02:34:26,984 - mmdet - INFO - Iter [5650/168780]	lr: 3.989e-04, eta: 5 days, 16:56:26, time: 2.988, data_time: 0.016, memory: 22027, loss_cls: 1.4412, loss_bbox: 2.7963, d0.loss_cls: 1.4412, d0.loss_bbox: 2.7963, d1.loss_cls: 1.4412, d1.loss_bbox: 2.7963, d2.loss_cls: 1.4412, d2.loss_bbox: 2.7963, d3.loss_cls: 1.4412, d3.loss_bbox: 2.7963, d4.loss_cls: 1.4412, d4.loss_bbox: 2.7963, dn_loss_cls: 0.8456, dn_loss_bbox: 2.4681, d0.dn_loss_cls: 0.8456, d0.dn_loss_bbox: 2.4681, d1.dn_loss_cls: 0.8456, d1.dn_loss_bbox: 2.4681, d2.dn_loss_cls: 0.8456, d2.dn_loss_bbox: 2.4681, d3.dn_loss_cls: 0.8456, d3.dn_loss_bbox: 2.4681, d4.dn_loss_cls: 0.8456, d4.dn_loss_bbox: 2.4681, loss_cls_lane: 0.6458, loss_cls_H: 0.6458, loss_bbox_lane: 6.6536, loss_bbox_H: 6.6399, d0.loss_cls_lane: 0.6458, d0.loss_cls_H: 0.6458, d0.loss_bbox_lane: 6.6536, d0.loss_bbox_H: 6.6399, d1.loss_cls_lane: 0.6458, d1.loss_cls_H: 0.6458, d1.loss_bbox_lane: 6.6536, d1.loss_bbox_H: 6.6399, d2.loss_cls_lane: 0.6458, d2.loss_cls_H: 0.6458, d2.loss_bbox_lane: 6.6536, d2.loss_bbox_H: 6.6399, d3.loss_cls_lane: 0.6458, d3.loss_cls_H: 0.6458, d3.loss_bbox_lane: 6.6536, d3.loss_bbox_H: 6.6399, d4.loss_cls_lane: 0.6458, d4.loss_cls_H: 0.6458, d4.loss_bbox_lane: 6.6536, d4.loss_bbox_H: 6.6399, vlm_loss: 0.0000, loss: 132.8167, grad_norm: nan
2025-03-01 02:36:54,422 - mmdet - INFO - Iter [5700/168780]	lr: 3.989e-04, eta: 5 days, 16:52:10, time: 2.949, data_time: 0.017, memory: 22027, loss_cls: 2.2669, loss_bbox: 2.7503, d0.loss_cls: 2.2669, d0.loss_bbox: 2.7503, d1.loss_cls: 2.2669, d1.loss_bbox: 2.7503, d2.loss_cls: 2.2669, d2.loss_bbox: 2.7503, d3.loss_cls: 2.2669, d3.loss_bbox: 2.7503, d4.loss_cls: 2.2669, d4.loss_bbox: 2.7503, dn_loss_cls: 1.5544, dn_loss_bbox: 2.4397, d0.dn_loss_cls: 1.5544, d0.dn_loss_bbox: 2.4397, d1.dn_loss_cls: 1.5544, d1.dn_loss_bbox: 2.4397, d2.dn_loss_cls: 1.5544, d2.dn_loss_bbox: 2.4397, d3.dn_loss_cls: 1.5544, d3.dn_loss_bbox: 2.4397, d4.dn_loss_cls: 1.5544, d4.dn_loss_bbox: 2.4397, loss_cls_lane: 0.6316, loss_cls_H: 0.6316, loss_bbox_lane: 6.3702, loss_bbox_H: 6.3634, d0.loss_cls_lane: 0.6316, d0.loss_cls_H: 0.6316, d0.loss_bbox_lane: 6.3702, d0.loss_bbox_H: 6.3634, d1.loss_cls_lane: 0.6316, d1.loss_cls_H: 0.6316, d1.loss_bbox_lane: 6.3702, d1.loss_bbox_H: 6.3634, d2.loss_cls_lane: 0.6316, d2.loss_cls_H: 0.6316, d2.loss_bbox_lane: 6.3702, d2.loss_bbox_H: 6.3634, d3.loss_cls_lane: 0.6316, d3.loss_cls_H: 0.6316, d3.loss_bbox_lane: 6.3702, d3.loss_bbox_H: 6.3634, d4.loss_cls_lane: 0.6316, d4.loss_cls_H: 0.6316, d4.loss_bbox_lane: 6.3702, d4.loss_bbox_H: 6.3634, vlm_loss: 0.0000, loss: 138.0492, grad_norm: nan
2025-03-01 02:39:22,633 - mmdet - INFO - Iter [5750/168780]	lr: 3.989e-04, eta: 5 days, 16:48:18, time: 2.964, data_time: 0.017, memory: 22041, loss_cls: 1.5020, loss_bbox: 2.7535, d0.loss_cls: 1.5020, d0.loss_bbox: 2.7535, d1.loss_cls: 1.5020, d1.loss_bbox: 2.7535, d2.loss_cls: 1.5020, d2.loss_bbox: 2.7535, d3.loss_cls: 1.5020, d3.loss_bbox: 2.7535, d4.loss_cls: 1.5020, d4.loss_bbox: 2.7535, dn_loss_cls: 0.9407, dn_loss_bbox: 2.4558, d0.dn_loss_cls: 0.9407, d0.dn_loss_bbox: 2.4558, d1.dn_loss_cls: 0.9407, d1.dn_loss_bbox: 2.4558, d2.dn_loss_cls: 0.9407, d2.dn_loss_bbox: 2.4558, d3.dn_loss_cls: 0.9407, d3.dn_loss_bbox: 2.4558, d4.dn_loss_cls: 0.9407, d4.dn_loss_bbox: 2.4558, loss_cls_lane: 0.6434, loss_cls_H: 0.6434, loss_bbox_lane: 6.4398, loss_bbox_H: 6.4511, d0.loss_cls_lane: 0.6434, d0.loss_cls_H: 0.6434, d0.loss_bbox_lane: 6.4398, d0.loss_bbox_H: 6.4511, d1.loss_cls_lane: 0.6434, d1.loss_cls_H: 0.6434, d1.loss_bbox_lane: 6.4398, d1.loss_bbox_H: 6.4511, d2.loss_cls_lane: 0.6434, d2.loss_cls_H: 0.6434, d2.loss_bbox_lane: 6.4398, d2.loss_bbox_H: 6.4511, d3.loss_cls_lane: 0.6434, d3.loss_cls_H: 0.6434, d3.loss_bbox_lane: 6.4398, d3.loss_bbox_H: 6.4511, d4.loss_cls_lane: 0.6434, d4.loss_cls_H: 0.6434, d4.loss_bbox_lane: 6.4398, d4.loss_bbox_H: 6.4511, vlm_loss: 0.0000, loss: 130.9790, grad_norm: nan
2025-03-01 02:41:50,586 - mmdet - INFO - Iter [5800/168780]	lr: 3.988e-04, eta: 5 days, 16:44:20, time: 2.959, data_time: 0.017, memory: 22041, loss_cls: 1.7500, loss_bbox: 2.7659, d0.loss_cls: 1.7500, d0.loss_bbox: 2.7659, d1.loss_cls: 1.7500, d1.loss_bbox: 2.7659, d2.loss_cls: 1.7500, d2.loss_bbox: 2.7659, d3.loss_cls: 1.7500, d3.loss_bbox: 2.7659, d4.loss_cls: 1.7500, d4.loss_bbox: 2.7659, dn_loss_cls: 1.0042, dn_loss_bbox: 2.5264, d0.dn_loss_cls: 1.0042, d0.dn_loss_bbox: 2.5264, d1.dn_loss_cls: 1.0042, d1.dn_loss_bbox: 2.5264, d2.dn_loss_cls: 1.0042, d2.dn_loss_bbox: 2.5264, d3.dn_loss_cls: 1.0042, d3.dn_loss_bbox: 2.5264, d4.dn_loss_cls: 1.0042, d4.dn_loss_bbox: 2.5264, loss_cls_lane: 0.6421, loss_cls_H: 0.6421, loss_bbox_lane: 6.3445, loss_bbox_H: 6.3359, d0.loss_cls_lane: 0.6421, d0.loss_cls_H: 0.6421, d0.loss_bbox_lane: 6.3445, d0.loss_bbox_H: 6.3359, d1.loss_cls_lane: 0.6421, d1.loss_cls_H: 0.6421, d1.loss_bbox_lane: 6.3445, d1.loss_bbox_H: 6.3359, d2.loss_cls_lane: 0.6421, d2.loss_cls_H: 0.6421, d2.loss_bbox_lane: 6.3445, d2.loss_bbox_H: 6.3359, d3.loss_cls_lane: 0.6421, d3.loss_cls_H: 0.6421, d3.loss_bbox_lane: 6.3445, d3.loss_bbox_H: 6.3359, d4.loss_cls_lane: 0.6421, d4.loss_cls_H: 0.6421, d4.loss_bbox_lane: 6.3445, d4.loss_bbox_H: 6.3359, vlm_loss: 0.0000, loss: 132.0678, grad_norm: nan
2025-03-01 02:44:18,253 - mmdet - INFO - Iter [5850/168780]	lr: 3.988e-04, eta: 5 days, 16:40:16, time: 2.953, data_time: 0.017, memory: 22041, loss_cls: 1.7914, loss_bbox: 2.7470, d0.loss_cls: 1.7914, d0.loss_bbox: 2.7470, d1.loss_cls: 1.7914, d1.loss_bbox: 2.7470, d2.loss_cls: 1.7914, d2.loss_bbox: 2.7470, d3.loss_cls: 1.7914, d3.loss_bbox: 2.7470, d4.loss_cls: 1.7914, d4.loss_bbox: 2.7470, dn_loss_cls: 0.9027, dn_loss_bbox: 2.4298, d0.dn_loss_cls: 0.9027, d0.dn_loss_bbox: 2.4298, d1.dn_loss_cls: 0.9027, d1.dn_loss_bbox: 2.4298, d2.dn_loss_cls: 0.9027, d2.dn_loss_bbox: 2.4298, d3.dn_loss_cls: 0.9027, d3.dn_loss_bbox: 2.4298, d4.dn_loss_cls: 0.9027, d4.dn_loss_bbox: 2.4298, loss_cls_lane: 0.6350, loss_cls_H: 0.6350, loss_bbox_lane: 6.4439, loss_bbox_H: 6.4302, d0.loss_cls_lane: 0.6350, d0.loss_cls_H: 0.6350, d0.loss_bbox_lane: 6.4439, d0.loss_bbox_H: 6.4302, d1.loss_cls_lane: 0.6350, d1.loss_cls_H: 0.6350, d1.loss_bbox_lane: 6.4439, d1.loss_bbox_H: 6.4302, d2.loss_cls_lane: 0.6350, d2.loss_cls_H: 0.6350, d2.loss_bbox_lane: 6.4439, d2.loss_bbox_H: 6.4302, d3.loss_cls_lane: 0.6350, d3.loss_cls_H: 0.6350, d3.loss_bbox_lane: 6.4439, d3.loss_bbox_H: 6.4302, d4.loss_cls_lane: 0.6350, d4.loss_cls_H: 0.6350, d4.loss_bbox_lane: 6.4439, d4.loss_bbox_H: 6.4302, vlm_loss: 0.0000, loss: 132.0906, grad_norm: nan
2025-03-01 02:46:45,534 - mmdet - INFO - Iter [5900/168780]	lr: 3.988e-04, eta: 5 days, 16:36:02, time: 2.946, data_time: 0.016, memory: 22041, loss_cls: 1.9541, loss_bbox: 2.7435, d0.loss_cls: 1.9541, d0.loss_bbox: 2.7435, d1.loss_cls: 1.9541, d1.loss_bbox: 2.7435, d2.loss_cls: 1.9541, d2.loss_bbox: 2.7435, d3.loss_cls: 1.9541, d3.loss_bbox: 2.7435, d4.loss_cls: 1.9541, d4.loss_bbox: 2.7435, dn_loss_cls: 1.0858, dn_loss_bbox: 2.4458, d0.dn_loss_cls: 1.0858, d0.dn_loss_bbox: 2.4458, d1.dn_loss_cls: 1.0858, d1.dn_loss_bbox: 2.4458, d2.dn_loss_cls: 1.0858, d2.dn_loss_bbox: 2.4458, d3.dn_loss_cls: 1.0858, d3.dn_loss_bbox: 2.4458, d4.dn_loss_cls: 1.0858, d4.dn_loss_bbox: 2.4458, loss_cls_lane: 0.6492, loss_cls_H: 0.6492, loss_bbox_lane: 6.6678, loss_bbox_H: 6.6556, d0.loss_cls_lane: 0.6492, d0.loss_cls_H: 0.6492, d0.loss_bbox_lane: 6.6678, d0.loss_bbox_H: 6.6556, d1.loss_cls_lane: 0.6492, d1.loss_cls_H: 0.6492, d1.loss_bbox_lane: 6.6678, d1.loss_bbox_H: 6.6556, d2.loss_cls_lane: 0.6492, d2.loss_cls_H: 0.6492, d2.loss_bbox_lane: 6.6678, d2.loss_bbox_H: 6.6556, d3.loss_cls_lane: 0.6492, d3.loss_cls_H: 0.6492, d3.loss_bbox_lane: 6.6678, d3.loss_bbox_H: 6.6556, d4.loss_cls_lane: 0.6492, d4.loss_cls_H: 0.6492, d4.loss_bbox_lane: 6.6678, d4.loss_bbox_H: 6.6556, vlm_loss: 0.0000, loss: 137.1064, grad_norm: nan
2025-03-01 02:49:13,200 - mmdet - INFO - Iter [5950/168780]	lr: 3.988e-04, eta: 5 days, 16:32:01, time: 2.953, data_time: 0.017, memory: 22041, loss_cls: 1.5591, loss_bbox: 2.7517, d0.loss_cls: 1.5591, d0.loss_bbox: 2.7517, d1.loss_cls: 1.5591, d1.loss_bbox: 2.7517, d2.loss_cls: 1.5591, d2.loss_bbox: 2.7517, d3.loss_cls: 1.5591, d3.loss_bbox: 2.7517, d4.loss_cls: 1.5591, d4.loss_bbox: 2.7517, dn_loss_cls: 0.9486, dn_loss_bbox: 2.4231, d0.dn_loss_cls: 0.9486, d0.dn_loss_bbox: 2.4231, d1.dn_loss_cls: 0.9486, d1.dn_loss_bbox: 2.4231, d2.dn_loss_cls: 0.9486, d2.dn_loss_bbox: 2.4231, d3.dn_loss_cls: 0.9486, d3.dn_loss_bbox: 2.4231, d4.dn_loss_cls: 0.9486, d4.dn_loss_bbox: 2.4231, loss_cls_lane: 0.6387, loss_cls_H: 0.6387, loss_bbox_lane: 6.7279, loss_bbox_H: 6.7201, d0.loss_cls_lane: 0.6387, d0.loss_cls_H: 0.6387, d0.loss_bbox_lane: 6.7279, d0.loss_bbox_H: 6.7201, d1.loss_cls_lane: 0.6387, d1.loss_cls_H: 0.6387, d1.loss_bbox_lane: 6.7279, d1.loss_bbox_H: 6.7201, d2.loss_cls_lane: 0.6387, d2.loss_cls_H: 0.6387, d2.loss_bbox_lane: 6.7279, d2.loss_bbox_H: 6.7201, d3.loss_cls_lane: 0.6387, d3.loss_cls_H: 0.6387, d3.loss_bbox_lane: 6.7279, d3.loss_bbox_H: 6.7201, d4.loss_cls_lane: 0.6387, d4.loss_cls_H: 0.6387, d4.loss_bbox_lane: 6.7279, d4.loss_bbox_H: 6.7201, vlm_loss: 0.0000, loss: 134.4467, grad_norm: nan
2025-03-01 02:51:40,973 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 02:51:40,973 - mmdet - INFO - Iter [6000/168780]	lr: 3.988e-04, eta: 5 days, 16:28:05, time: 2.955, data_time: 0.018, memory: 22041, loss_cls: 1.4561, loss_bbox: 2.7599, d0.loss_cls: 1.4561, d0.loss_bbox: 2.7599, d1.loss_cls: 1.4561, d1.loss_bbox: 2.7599, d2.loss_cls: 1.4561, d2.loss_bbox: 2.7599, d3.loss_cls: 1.4561, d3.loss_bbox: 2.7599, d4.loss_cls: 1.4561, d4.loss_bbox: 2.7599, dn_loss_cls: 0.9427, dn_loss_bbox: 2.3274, d0.dn_loss_cls: 0.9427, d0.dn_loss_bbox: 2.3274, d1.dn_loss_cls: 0.9427, d1.dn_loss_bbox: 2.3274, d2.dn_loss_cls: 0.9427, d2.dn_loss_bbox: 2.3274, d3.dn_loss_cls: 0.9427, d3.dn_loss_bbox: 2.3274, d4.dn_loss_cls: 0.9427, d4.dn_loss_bbox: 2.3274, loss_cls_lane: 0.6484, loss_cls_H: 0.8097, loss_bbox_lane: 6.2577, loss_bbox_H: 6.2540, d0.loss_cls_lane: 0.6484, d0.loss_cls_H: 0.8097, d0.loss_bbox_lane: 6.2577, d0.loss_bbox_H: 6.2540, d1.loss_cls_lane: 0.6484, d1.loss_cls_H: 0.8097, d1.loss_bbox_lane: 6.2577, d1.loss_bbox_H: 6.2540, d2.loss_cls_lane: 0.6484, d2.loss_cls_H: 0.8097, d2.loss_bbox_lane: 6.2577, d2.loss_bbox_H: 6.2540, d3.loss_cls_lane: 0.6484, d3.loss_cls_H: 0.8097, d3.loss_bbox_lane: 6.2577, d3.loss_bbox_H: 6.2540, d4.loss_cls_lane: 0.6484, d4.loss_cls_H: 0.8097, d4.loss_bbox_lane: 6.2577, d4.loss_bbox_H: 6.2540, vlm_loss: 0.0000, loss: 128.7352, grad_norm: nan
2025-03-01 02:54:10,408 - mmdet - INFO - Iter [6050/168780]	lr: 3.987e-04, eta: 5 days, 16:24:54, time: 2.989, data_time: 0.017, memory: 22041, loss_cls: 1.4466, loss_bbox: 2.6706, d0.loss_cls: 1.4466, d0.loss_bbox: 2.6706, d1.loss_cls: 1.4466, d1.loss_bbox: 2.6706, d2.loss_cls: 1.4466, d2.loss_bbox: 2.6706, d3.loss_cls: 1.4466, d3.loss_bbox: 2.6706, d4.loss_cls: 1.4466, d4.loss_bbox: 2.6706, dn_loss_cls: 1.0225, dn_loss_bbox: 2.2611, d0.dn_loss_cls: 1.0225, d0.dn_loss_bbox: 2.2611, d1.dn_loss_cls: 1.0225, d1.dn_loss_bbox: 2.2611, d2.dn_loss_cls: 1.0225, d2.dn_loss_bbox: 2.2611, d3.dn_loss_cls: 1.0225, d3.dn_loss_bbox: 2.2611, d4.dn_loss_cls: 1.0225, d4.dn_loss_bbox: 2.2611, loss_cls_lane: 0.6353, loss_cls_H: 0.6353, loss_bbox_lane: 6.4031, loss_bbox_H: 6.4112, d0.loss_cls_lane: 0.6353, d0.loss_cls_H: 0.6353, d0.loss_bbox_lane: 6.4031, d0.loss_bbox_H: 6.4112, d1.loss_cls_lane: 0.6353, d1.loss_cls_H: 0.6353, d1.loss_bbox_lane: 6.4031, d1.loss_bbox_H: 6.4112, d2.loss_cls_lane: 0.6353, d2.loss_cls_H: 0.6353, d2.loss_bbox_lane: 6.4031, d2.loss_bbox_H: 6.4112, d3.loss_cls_lane: 0.6353, d3.loss_cls_H: 0.6353, d3.loss_bbox_lane: 6.4031, d3.loss_bbox_H: 6.4112, d4.loss_cls_lane: 0.6353, d4.loss_cls_H: 0.6353, d4.loss_bbox_lane: 6.4031, d4.loss_bbox_H: 6.4112, vlm_loss: 0.0000, loss: 128.9143, grad_norm: nan
2025-03-01 02:56:39,500 - mmdet - INFO - Iter [6100/168780]	lr: 3.987e-04, eta: 5 days, 16:21:35, time: 2.982, data_time: 0.018, memory: 22041, loss_cls: 1.4612, loss_bbox: 2.7006, d0.loss_cls: 1.4612, d0.loss_bbox: 2.7006, d1.loss_cls: 1.4612, d1.loss_bbox: 2.7006, d2.loss_cls: 1.4612, d2.loss_bbox: 2.7006, d3.loss_cls: 1.4612, d3.loss_bbox: 2.7006, d4.loss_cls: 1.4612, d4.loss_bbox: 2.7006, dn_loss_cls: 0.9227, dn_loss_bbox: 2.3103, d0.dn_loss_cls: 0.9227, d0.dn_loss_bbox: 2.3103, d1.dn_loss_cls: 0.9227, d1.dn_loss_bbox: 2.3103, d2.dn_loss_cls: 0.9227, d2.dn_loss_bbox: 2.3103, d3.dn_loss_cls: 0.9227, d3.dn_loss_bbox: 2.3103, d4.dn_loss_cls: 0.9227, d4.dn_loss_bbox: 2.3103, loss_cls_lane: 0.6297, loss_cls_H: 0.6297, loss_bbox_lane: 6.4380, loss_bbox_H: 6.4423, d0.loss_cls_lane: 0.6297, d0.loss_cls_H: 0.6297, d0.loss_bbox_lane: 6.4380, d0.loss_bbox_H: 6.4423, d1.loss_cls_lane: 0.6297, d1.loss_cls_H: 0.6297, d1.loss_bbox_lane: 6.4380, d1.loss_bbox_H: 6.4423, d2.loss_cls_lane: 0.6297, d2.loss_cls_H: 0.6297, d2.loss_bbox_lane: 6.4380, d2.loss_bbox_H: 6.4423, d3.loss_cls_lane: 0.6297, d3.loss_cls_H: 0.6297, d3.loss_bbox_lane: 6.4380, d3.loss_bbox_H: 6.4423, d4.loss_cls_lane: 0.6297, d4.loss_cls_H: 0.6297, d4.loss_bbox_lane: 6.4380, d4.loss_bbox_H: 6.4423, vlm_loss: 0.0000, loss: 129.2074, grad_norm: nan
2025-03-01 02:59:10,008 - mmdet - INFO - Iter [6150/168780]	lr: 3.987e-04, eta: 5 days, 16:18:55, time: 3.010, data_time: 0.018, memory: 22041, loss_cls: 1.2914, loss_bbox: 2.5997, d0.loss_cls: 1.2914, d0.loss_bbox: 2.5997, d1.loss_cls: 1.2914, d1.loss_bbox: 2.5997, d2.loss_cls: 1.2914, d2.loss_bbox: 2.5997, d3.loss_cls: 1.2914, d3.loss_bbox: 2.5997, d4.loss_cls: 1.2914, d4.loss_bbox: 2.5997, dn_loss_cls: 1.0215, dn_loss_bbox: 2.3398, d0.dn_loss_cls: 1.0215, d0.dn_loss_bbox: 2.3398, d1.dn_loss_cls: 1.0215, d1.dn_loss_bbox: 2.3398, d2.dn_loss_cls: 1.0215, d2.dn_loss_bbox: 2.3398, d3.dn_loss_cls: 1.0215, d3.dn_loss_bbox: 2.3398, d4.dn_loss_cls: 1.0215, d4.dn_loss_bbox: 2.3398, loss_cls_lane: 0.6399, loss_cls_H: 0.6399, loss_bbox_lane: 6.5188, loss_bbox_H: 6.5205, d0.loss_cls_lane: 0.6399, d0.loss_cls_H: 0.6399, d0.loss_bbox_lane: 6.5188, d0.loss_bbox_H: 6.5205, d1.loss_cls_lane: 0.6399, d1.loss_cls_H: 0.6399, d1.loss_bbox_lane: 6.5188, d1.loss_bbox_H: 6.5205, d2.loss_cls_lane: 0.6399, d2.loss_cls_H: 0.6399, d2.loss_bbox_lane: 6.5188, d2.loss_bbox_H: 6.5205, d3.loss_cls_lane: 0.6399, d3.loss_cls_H: 0.6399, d3.loss_bbox_lane: 6.5188, d3.loss_bbox_H: 6.5205, d4.loss_cls_lane: 0.6399, d4.loss_cls_H: 0.6399, d4.loss_bbox_lane: 6.5188, d4.loss_bbox_H: 6.5205, vlm_loss: 0.0000, loss: 129.4288, grad_norm: nan
2025-03-01 03:01:38,350 - mmdet - INFO - Iter [6200/168780]	lr: 3.987e-04, eta: 5 days, 16:15:17, time: 2.967, data_time: 0.017, memory: 22041, loss_cls: 1.7435, loss_bbox: 2.9669, d0.loss_cls: 1.7435, d0.loss_bbox: 2.9669, d1.loss_cls: 1.7435, d1.loss_bbox: 2.9669, d2.loss_cls: 1.7435, d2.loss_bbox: 2.9669, d3.loss_cls: 1.7435, d3.loss_bbox: 2.9669, d4.loss_cls: 1.7435, d4.loss_bbox: 2.9669, dn_loss_cls: 0.9369, dn_loss_bbox: 2.5707, d0.dn_loss_cls: 0.9369, d0.dn_loss_bbox: 2.5707, d1.dn_loss_cls: 0.9369, d1.dn_loss_bbox: 2.5707, d2.dn_loss_cls: 0.9369, d2.dn_loss_bbox: 2.5707, d3.dn_loss_cls: 0.9369, d3.dn_loss_bbox: 2.5707, d4.dn_loss_cls: 0.9369, d4.dn_loss_bbox: 2.5707, loss_cls_lane: 0.6361, loss_cls_H: 0.6361, loss_bbox_lane: 6.4880, loss_bbox_H: 6.4908, d0.loss_cls_lane: 0.6361, d0.loss_cls_H: 0.6361, d0.loss_bbox_lane: 6.4880, d0.loss_bbox_H: 6.4908, d1.loss_cls_lane: 0.6361, d1.loss_cls_H: 0.6361, d1.loss_bbox_lane: 6.4880, d1.loss_bbox_H: 6.4908, d2.loss_cls_lane: 0.6361, d2.loss_cls_H: 0.6361, d2.loss_bbox_lane: 6.4880, d2.loss_bbox_H: 6.4908, d3.loss_cls_lane: 0.6361, d3.loss_cls_H: 0.6361, d3.loss_bbox_lane: 6.4880, d3.loss_bbox_H: 6.4908, d4.loss_cls_lane: 0.6361, d4.loss_cls_H: 0.6361, d4.loss_bbox_lane: 6.4880, d4.loss_bbox_H: 6.4908, vlm_loss: 0.0000, loss: 134.8140, grad_norm: nan
2025-03-01 03:04:08,070 - mmdet - INFO - Iter [6250/168780]	lr: 3.986e-04, eta: 5 days, 16:12:17, time: 2.994, data_time: 0.017, memory: 22041, loss_cls: 1.4778, loss_bbox: 2.8906, d0.loss_cls: 1.4778, d0.loss_bbox: 2.8906, d1.loss_cls: 1.4778, d1.loss_bbox: 2.8906, d2.loss_cls: 1.4778, d2.loss_bbox: 2.8906, d3.loss_cls: 1.4778, d3.loss_bbox: 2.8906, d4.loss_cls: 1.4778, d4.loss_bbox: 2.8906, dn_loss_cls: 0.9156, dn_loss_bbox: 2.5417, d0.dn_loss_cls: 0.9156, d0.dn_loss_bbox: 2.5417, d1.dn_loss_cls: 0.9156, d1.dn_loss_bbox: 2.5417, d2.dn_loss_cls: 0.9156, d2.dn_loss_bbox: 2.5417, d3.dn_loss_cls: 0.9156, d3.dn_loss_bbox: 2.5417, d4.dn_loss_cls: 0.9156, d4.dn_loss_bbox: 2.5417, loss_cls_lane: 0.6306, loss_cls_H: 0.6306, loss_bbox_lane: 6.2253, loss_bbox_H: 6.2463, d0.loss_cls_lane: 0.6306, d0.loss_cls_H: 0.6306, d0.loss_bbox_lane: 6.2253, d0.loss_bbox_H: 6.2463, d1.loss_cls_lane: 0.6306, d1.loss_cls_H: 0.6306, d1.loss_bbox_lane: 6.2253, d1.loss_bbox_H: 6.2463, d2.loss_cls_lane: 0.6306, d2.loss_cls_H: 0.6306, d2.loss_bbox_lane: 6.2253, d2.loss_bbox_H: 6.2463, d3.loss_cls_lane: 0.6306, d3.loss_cls_H: 0.6306, d3.loss_bbox_lane: 6.2253, d3.loss_bbox_H: 6.2463, d4.loss_cls_lane: 0.6306, d4.loss_cls_H: 0.6306, d4.loss_bbox_lane: 6.2253, d4.loss_bbox_H: 6.2463, vlm_loss: 0.0000, loss: 129.3517, grad_norm: nan
2025-03-01 03:06:36,966 - mmdet - INFO - Iter [6300/168780]	lr: 3.986e-04, eta: 5 days, 16:08:56, time: 2.978, data_time: 0.017, memory: 22041, loss_cls: 1.6979, loss_bbox: 2.7724, d0.loss_cls: 1.6979, d0.loss_bbox: 2.7724, d1.loss_cls: 1.6979, d1.loss_bbox: 2.7724, d2.loss_cls: 1.6979, d2.loss_bbox: 2.7724, d3.loss_cls: 1.6979, d3.loss_bbox: 2.7724, d4.loss_cls: 1.6979, d4.loss_bbox: 2.7724, dn_loss_cls: 0.9415, dn_loss_bbox: 2.3537, d0.dn_loss_cls: 0.9415, d0.dn_loss_bbox: 2.3537, d1.dn_loss_cls: 0.9415, d1.dn_loss_bbox: 2.3537, d2.dn_loss_cls: 0.9415, d2.dn_loss_bbox: 2.3537, d3.dn_loss_cls: 0.9415, d3.dn_loss_bbox: 2.3537, d4.dn_loss_cls: 0.9415, d4.dn_loss_bbox: 2.3537, loss_cls_lane: 0.6332, loss_cls_H: 0.6332, loss_bbox_lane: 6.6314, loss_bbox_H: 6.6231, d0.loss_cls_lane: 0.6332, d0.loss_cls_H: 0.6332, d0.loss_bbox_lane: 6.6314, d0.loss_bbox_H: 6.6231, d1.loss_cls_lane: 0.6332, d1.loss_cls_H: 0.6332, d1.loss_bbox_lane: 6.6314, d1.loss_bbox_H: 6.6231, d2.loss_cls_lane: 0.6332, d2.loss_cls_H: 0.6332, d2.loss_bbox_lane: 6.6314, d2.loss_bbox_H: 6.6231, d3.loss_cls_lane: 0.6332, d3.loss_cls_H: 0.6332, d3.loss_bbox_lane: 6.6314, d3.loss_bbox_H: 6.6231, d4.loss_cls_lane: 0.6332, d4.loss_cls_H: 0.6332, d4.loss_bbox_lane: 6.6314, d4.loss_bbox_H: 6.6231, vlm_loss: 0.0000, loss: 133.7190, grad_norm: nan
2025-03-01 03:09:04,385 - mmdet - INFO - Iter [6350/168780]	lr: 3.986e-04, eta: 5 days, 16:04:58, time: 2.948, data_time: 0.016, memory: 22041, loss_cls: 1.7303, loss_bbox: 2.9172, d0.loss_cls: 1.7303, d0.loss_bbox: 2.9172, d1.loss_cls: 1.7303, d1.loss_bbox: 2.9172, d2.loss_cls: 1.7303, d2.loss_bbox: 2.9172, d3.loss_cls: 1.7303, d3.loss_bbox: 2.9172, d4.loss_cls: 1.7303, d4.loss_bbox: 2.9172, dn_loss_cls: 0.8846, dn_loss_bbox: 2.5526, d0.dn_loss_cls: 0.8846, d0.dn_loss_bbox: 2.5526, d1.dn_loss_cls: 0.8846, d1.dn_loss_bbox: 2.5526, d2.dn_loss_cls: 0.8846, d2.dn_loss_bbox: 2.5526, d3.dn_loss_cls: 0.8846, d3.dn_loss_bbox: 2.5526, d4.dn_loss_cls: 0.8846, d4.dn_loss_bbox: 2.5526, loss_cls_lane: 0.6398, loss_cls_H: 0.6398, loss_bbox_lane: 6.5615, loss_bbox_H: 6.5528, d0.loss_cls_lane: 0.6398, d0.loss_cls_H: 0.6398, d0.loss_bbox_lane: 6.5615, d0.loss_bbox_H: 6.5528, d1.loss_cls_lane: 0.6398, d1.loss_cls_H: 0.6398, d1.loss_bbox_lane: 6.5615, d1.loss_bbox_H: 6.5528, d2.loss_cls_lane: 0.6398, d2.loss_cls_H: 0.6398, d2.loss_bbox_lane: 6.5615, d2.loss_bbox_H: 6.5528, d3.loss_cls_lane: 0.6398, d3.loss_cls_H: 0.6398, d3.loss_bbox_lane: 6.5615, d3.loss_bbox_H: 6.5528, d4.loss_cls_lane: 0.6398, d4.loss_cls_H: 0.6398, d4.loss_bbox_lane: 6.5615, d4.loss_bbox_H: 6.5528, vlm_loss: 0.0000, loss: 134.8711, grad_norm: nan
2025-03-01 03:11:33,423 - mmdet - INFO - Iter [6400/168780]	lr: 3.986e-04, eta: 5 days, 16:01:42, time: 2.981, data_time: 0.016, memory: 22041, loss_cls: 1.6677, loss_bbox: 2.8896, d0.loss_cls: 1.6677, d0.loss_bbox: 2.8896, d1.loss_cls: 1.6677, d1.loss_bbox: 2.8896, d2.loss_cls: 1.6677, d2.loss_bbox: 2.8896, d3.loss_cls: 1.6677, d3.loss_bbox: 2.8896, d4.loss_cls: 1.6677, d4.loss_bbox: 2.8896, dn_loss_cls: 0.8800, dn_loss_bbox: 2.6265, d0.dn_loss_cls: 0.8800, d0.dn_loss_bbox: 2.6265, d1.dn_loss_cls: 0.8800, d1.dn_loss_bbox: 2.6265, d2.dn_loss_cls: 0.8800, d2.dn_loss_bbox: 2.6265, d3.dn_loss_cls: 0.8800, d3.dn_loss_bbox: 2.6265, d4.dn_loss_cls: 0.8800, d4.dn_loss_bbox: 2.6265, loss_cls_lane: 0.6399, loss_cls_H: 0.6399, loss_bbox_lane: 6.5494, loss_bbox_H: 6.5446, d0.loss_cls_lane: 0.6399, d0.loss_cls_H: 0.6399, d0.loss_bbox_lane: 6.5494, d0.loss_bbox_H: 6.5446, d1.loss_cls_lane: 0.6399, d1.loss_cls_H: 0.6399, d1.loss_bbox_lane: 6.5494, d1.loss_bbox_H: 6.5446, d2.loss_cls_lane: 0.6399, d2.loss_cls_H: 0.6399, d2.loss_bbox_lane: 6.5494, d2.loss_bbox_H: 6.5446, d3.loss_cls_lane: 0.6399, d3.loss_cls_H: 0.6399, d3.loss_bbox_lane: 6.5494, d3.loss_bbox_H: 6.5446, d4.loss_cls_lane: 0.6399, d4.loss_cls_H: 0.6399, d4.loss_bbox_lane: 6.5494, d4.loss_bbox_H: 6.5446, vlm_loss: 0.0000, loss: 134.6244, grad_norm: nan
2025-03-01 03:13:59,964 - mmdet - INFO - Iter [6450/168780]	lr: 3.986e-04, eta: 5 days, 15:57:24, time: 2.931, data_time: 0.017, memory: 22041, loss_cls: 3.4452, loss_bbox: 3.2786, d0.loss_cls: 3.4452, d0.loss_bbox: 3.2786, d1.loss_cls: 3.4452, d1.loss_bbox: 3.2786, d2.loss_cls: 3.4452, d2.loss_bbox: 3.2786, d3.loss_cls: 3.4452, d3.loss_bbox: 3.2786, d4.loss_cls: 3.4452, d4.loss_bbox: 3.2786, dn_loss_cls: 0.8621, dn_loss_bbox: 2.9938, d0.dn_loss_cls: 0.8621, d0.dn_loss_bbox: 2.9938, d1.dn_loss_cls: 0.8621, d1.dn_loss_bbox: 2.9938, d2.dn_loss_cls: 0.8621, d2.dn_loss_bbox: 2.9938, d3.dn_loss_cls: 0.8621, d3.dn_loss_bbox: 2.9938, d4.dn_loss_cls: 0.8621, d4.dn_loss_bbox: 2.9938, loss_cls_lane: 0.6760, loss_cls_H: 0.6760, loss_bbox_lane: 6.8545, loss_bbox_H: 6.8574, d0.loss_cls_lane: 0.6760, d0.loss_cls_H: 0.6760, d0.loss_bbox_lane: 6.8545, d0.loss_bbox_H: 6.8574, d1.loss_cls_lane: 0.6760, d1.loss_cls_H: 0.6760, d1.loss_bbox_lane: 6.8545, d1.loss_bbox_H: 6.8574, d2.loss_cls_lane: 0.6760, d2.loss_cls_H: 0.6760, d2.loss_bbox_lane: 6.8545, d2.loss_bbox_H: 6.8574, d3.loss_cls_lane: 0.6760, d3.loss_cls_H: 0.6760, d3.loss_bbox_lane: 6.8545, d3.loss_bbox_H: 6.8574, d4.loss_cls_lane: 0.6760, d4.loss_cls_H: 0.6760, d4.loss_bbox_lane: 6.8545, d4.loss_bbox_H: 6.8574, vlm_loss: 0.0000, loss: 153.8616, grad_norm: nan
2025-03-01 03:16:24,810 - mmdet - INFO - Iter [6500/168780]	lr: 3.985e-04, eta: 5 days, 15:52:26, time: 2.897, data_time: 0.018, memory: 22041, loss_cls: 1.3720, loss_bbox: 2.5853, d0.loss_cls: 1.3720, d0.loss_bbox: 2.5853, d1.loss_cls: 1.3720, d1.loss_bbox: 2.5853, d2.loss_cls: 1.3720, d2.loss_bbox: 2.5853, d3.loss_cls: 1.3720, d3.loss_bbox: 2.5853, d4.loss_cls: 1.3720, d4.loss_bbox: 2.5853, dn_loss_cls: 0.9196, dn_loss_bbox: 2.3509, d0.dn_loss_cls: 0.9196, d0.dn_loss_bbox: 2.3509, d1.dn_loss_cls: 0.9196, d1.dn_loss_bbox: 2.3509, d2.dn_loss_cls: 0.9196, d2.dn_loss_bbox: 2.3509, d3.dn_loss_cls: 0.9196, d3.dn_loss_bbox: 2.3509, d4.dn_loss_cls: 0.9196, d4.dn_loss_bbox: 2.3509, loss_cls_lane: 0.6505, loss_cls_H: 0.9193, loss_bbox_lane: 6.0447, loss_bbox_H: 6.0415, d0.loss_cls_lane: 0.6505, d0.loss_cls_H: 0.9193, d0.loss_bbox_lane: 6.0447, d0.loss_bbox_H: 6.0415, d1.loss_cls_lane: 0.6505, d1.loss_cls_H: 0.9193, d1.loss_bbox_lane: 6.0447, d1.loss_bbox_H: 6.0415, d2.loss_cls_lane: 0.6505, d2.loss_cls_H: 0.9193, d2.loss_bbox_lane: 6.0447, d2.loss_bbox_H: 6.0415, d3.loss_cls_lane: 0.6505, d3.loss_cls_H: 0.9193, d3.loss_bbox_lane: 6.0447, d3.loss_bbox_H: 6.0415, d4.loss_cls_lane: 0.6505, d4.loss_cls_H: 0.9193, d4.loss_bbox_lane: 6.0447, d4.loss_bbox_H: 6.0415, vlm_loss: 0.0000, loss: 125.3042, grad_norm: nan
2025-03-01 03:18:53,247 - mmdet - INFO - Iter [6550/168780]	lr: 3.985e-04, eta: 5 days, 15:48:59, time: 2.969, data_time: 0.018, memory: 22041, loss_cls: 1.5846, loss_bbox: 2.7972, d0.loss_cls: 1.5846, d0.loss_bbox: 2.7972, d1.loss_cls: 1.5846, d1.loss_bbox: 2.7972, d2.loss_cls: 1.5846, d2.loss_bbox: 2.7972, d3.loss_cls: 1.5846, d3.loss_bbox: 2.7972, d4.loss_cls: 1.5846, d4.loss_bbox: 2.7972, dn_loss_cls: 0.8816, dn_loss_bbox: 2.4758, d0.dn_loss_cls: 0.8816, d0.dn_loss_bbox: 2.4758, d1.dn_loss_cls: 0.8816, d1.dn_loss_bbox: 2.4758, d2.dn_loss_cls: 0.8816, d2.dn_loss_bbox: 2.4758, d3.dn_loss_cls: 0.8816, d3.dn_loss_bbox: 2.4758, d4.dn_loss_cls: 0.8816, d4.dn_loss_bbox: 2.4758, loss_cls_lane: 0.6621, loss_cls_H: 0.6621, loss_bbox_lane: 6.4428, loss_bbox_H: 6.4431, d0.loss_cls_lane: 0.6621, d0.loss_cls_H: 0.6621, d0.loss_bbox_lane: 6.4428, d0.loss_bbox_H: 6.4431, d1.loss_cls_lane: 0.6621, d1.loss_cls_H: 0.6621, d1.loss_bbox_lane: 6.4428, d1.loss_bbox_H: 6.4431, d2.loss_cls_lane: 0.6621, d2.loss_cls_H: 0.6621, d2.loss_bbox_lane: 6.4428, d2.loss_bbox_H: 6.4431, d3.loss_cls_lane: 0.6621, d3.loss_cls_H: 0.6621, d3.loss_bbox_lane: 6.4428, d3.loss_bbox_H: 6.4431, d4.loss_cls_lane: 0.6621, d4.loss_cls_H: 0.6621, d4.loss_bbox_lane: 6.4428, d4.loss_bbox_H: 6.4431, vlm_loss: 0.0000, loss: 131.6953, grad_norm: nan
2025-03-01 03:21:21,390 - mmdet - INFO - Iter [6600/168780]	lr: 3.985e-04, eta: 5 days, 15:45:26, time: 2.963, data_time: 0.017, memory: 22041, loss_cls: 1.5019, loss_bbox: 2.7121, d0.loss_cls: 1.5019, d0.loss_bbox: 2.7121, d1.loss_cls: 1.5019, d1.loss_bbox: 2.7121, d2.loss_cls: 1.5019, d2.loss_bbox: 2.7121, d3.loss_cls: 1.5019, d3.loss_bbox: 2.7121, d4.loss_cls: 1.5019, d4.loss_bbox: 2.7121, dn_loss_cls: 0.8626, dn_loss_bbox: 2.3471, d0.dn_loss_cls: 0.8626, d0.dn_loss_bbox: 2.3471, d1.dn_loss_cls: 0.8626, d1.dn_loss_bbox: 2.3471, d2.dn_loss_cls: 0.8626, d2.dn_loss_bbox: 2.3471, d3.dn_loss_cls: 0.8626, d3.dn_loss_bbox: 2.3471, d4.dn_loss_cls: 0.8626, d4.dn_loss_bbox: 2.3471, loss_cls_lane: 0.6300, loss_cls_H: 0.6300, loss_bbox_lane: 6.6968, loss_bbox_H: 6.6855, d0.loss_cls_lane: 0.6300, d0.loss_cls_H: 0.6300, d0.loss_bbox_lane: 6.6968, d0.loss_bbox_H: 6.6855, d1.loss_cls_lane: 0.6300, d1.loss_cls_H: 0.6300, d1.loss_bbox_lane: 6.6968, d1.loss_bbox_H: 6.6855, d2.loss_cls_lane: 0.6300, d2.loss_cls_H: 0.6300, d2.loss_bbox_lane: 6.6968, d2.loss_bbox_H: 6.6855, d3.loss_cls_lane: 0.6300, d3.loss_cls_H: 0.6300, d3.loss_bbox_lane: 6.6968, d3.loss_bbox_H: 6.6855, d4.loss_cls_lane: 0.6300, d4.loss_cls_H: 0.6300, d4.loss_bbox_lane: 6.6968, d4.loss_bbox_H: 6.6855, vlm_loss: 0.0000, loss: 132.3959, grad_norm: nan
2025-03-01 03:23:51,747 - mmdet - INFO - Iter [6650/168780]	lr: 3.985e-04, eta: 5 days, 15:42:47, time: 3.007, data_time: 0.018, memory: 22041, loss_cls: 1.2419, loss_bbox: 2.7170, d0.loss_cls: 1.2419, d0.loss_bbox: 2.7170, d1.loss_cls: 1.2419, d1.loss_bbox: 2.7170, d2.loss_cls: 1.2419, d2.loss_bbox: 2.7170, d3.loss_cls: 1.2419, d3.loss_bbox: 2.7170, d4.loss_cls: 1.2419, d4.loss_bbox: 2.7170, dn_loss_cls: 0.8746, dn_loss_bbox: 2.2595, d0.dn_loss_cls: 0.8746, d0.dn_loss_bbox: 2.2595, d1.dn_loss_cls: 0.8746, d1.dn_loss_bbox: 2.2595, d2.dn_loss_cls: 0.8746, d2.dn_loss_bbox: 2.2595, d3.dn_loss_cls: 0.8746, d3.dn_loss_bbox: 2.2595, d4.dn_loss_cls: 0.8746, d4.dn_loss_bbox: 2.2595, loss_cls_lane: 0.6250, loss_cls_H: 0.6250, loss_bbox_lane: 6.4681, loss_bbox_H: 6.4723, d0.loss_cls_lane: 0.6250, d0.loss_cls_H: 0.6250, d0.loss_bbox_lane: 6.4681, d0.loss_bbox_H: 6.4723, d1.loss_cls_lane: 0.6250, d1.loss_cls_H: 0.6250, d1.loss_bbox_lane: 6.4681, d1.loss_bbox_H: 6.4723, d2.loss_cls_lane: 0.6250, d2.loss_cls_H: 0.6250, d2.loss_bbox_lane: 6.4681, d2.loss_bbox_H: 6.4723, d3.loss_cls_lane: 0.6250, d3.loss_cls_H: 0.6250, d3.loss_bbox_lane: 6.4681, d3.loss_bbox_H: 6.4723, d4.loss_cls_lane: 0.6250, d4.loss_cls_H: 0.6250, d4.loss_bbox_lane: 6.4681, d4.loss_bbox_H: 6.4723, vlm_loss: 0.0000, loss: 127.7002, grad_norm: nan
2025-03-01 03:26:20,415 - mmdet - INFO - Iter [6700/168780]	lr: 3.984e-04, eta: 5 days, 15:39:28, time: 2.973, data_time: 0.017, memory: 22041, loss_cls: 1.5490, loss_bbox: 2.7423, d0.loss_cls: 1.5490, d0.loss_bbox: 2.7423, d1.loss_cls: 1.5490, d1.loss_bbox: 2.7423, d2.loss_cls: 1.5490, d2.loss_bbox: 2.7423, d3.loss_cls: 1.5490, d3.loss_bbox: 2.7423, d4.loss_cls: 1.5490, d4.loss_bbox: 2.7423, dn_loss_cls: 0.9026, dn_loss_bbox: 2.4692, d0.dn_loss_cls: 0.9026, d0.dn_loss_bbox: 2.4692, d1.dn_loss_cls: 0.9026, d1.dn_loss_bbox: 2.4692, d2.dn_loss_cls: 0.9026, d2.dn_loss_bbox: 2.4692, d3.dn_loss_cls: 0.9026, d3.dn_loss_bbox: 2.4692, d4.dn_loss_cls: 0.9026, d4.dn_loss_bbox: 2.4692, loss_cls_lane: 0.6332, loss_cls_H: 0.6332, loss_bbox_lane: 6.2522, loss_bbox_H: 6.2517, d0.loss_cls_lane: 0.6332, d0.loss_cls_H: 0.6332, d0.loss_bbox_lane: 6.2522, d0.loss_bbox_H: 6.2517, d1.loss_cls_lane: 0.6332, d1.loss_cls_H: 0.6332, d1.loss_bbox_lane: 6.2522, d1.loss_bbox_H: 6.2517, d2.loss_cls_lane: 0.6332, d2.loss_cls_H: 0.6332, d2.loss_bbox_lane: 6.2522, d2.loss_bbox_H: 6.2517, d3.loss_cls_lane: 0.6332, d3.loss_cls_H: 0.6332, d3.loss_bbox_lane: 6.2522, d3.loss_bbox_H: 6.2517, d4.loss_cls_lane: 0.6332, d4.loss_cls_H: 0.6332, d4.loss_bbox_lane: 6.2522, d4.loss_bbox_H: 6.2517, vlm_loss: 0.0000, loss: 128.6008, grad_norm: nan
2025-03-01 03:28:51,213 - mmdet - INFO - Iter [6750/168780]	lr: 3.984e-04, eta: 5 days, 15:37:01, time: 3.016, data_time: 0.017, memory: 22041, loss_cls: 1.3281, loss_bbox: 2.7526, d0.loss_cls: 1.3281, d0.loss_bbox: 2.7526, d1.loss_cls: 1.3281, d1.loss_bbox: 2.7526, d2.loss_cls: 1.3281, d2.loss_bbox: 2.7526, d3.loss_cls: 1.3281, d3.loss_bbox: 2.7526, d4.loss_cls: 1.3281, d4.loss_bbox: 2.7526, dn_loss_cls: 0.8784, dn_loss_bbox: 2.3671, d0.dn_loss_cls: 0.8784, d0.dn_loss_bbox: 2.3671, d1.dn_loss_cls: 0.8784, d1.dn_loss_bbox: 2.3671, d2.dn_loss_cls: 0.8784, d2.dn_loss_bbox: 2.3671, d3.dn_loss_cls: 0.8784, d3.dn_loss_bbox: 2.3671, d4.dn_loss_cls: 0.8784, d4.dn_loss_bbox: 2.3671, loss_cls_lane: 0.6922, loss_cls_H: 0.6922, loss_bbox_lane: 6.8138, loss_bbox_H: 6.8150, d0.loss_cls_lane: 0.6922, d0.loss_cls_H: 0.6922, d0.loss_bbox_lane: 6.8138, d0.loss_bbox_H: 6.8150, d1.loss_cls_lane: 0.6922, d1.loss_cls_H: 0.6922, d1.loss_bbox_lane: 6.8138, d1.loss_bbox_H: 6.8150, d2.loss_cls_lane: 0.6922, d2.loss_cls_H: 0.6922, d2.loss_bbox_lane: 6.8138, d2.loss_bbox_H: 6.8150, d3.loss_cls_lane: 0.6922, d3.loss_cls_H: 0.6922, d3.loss_bbox_lane: 6.8138, d3.loss_bbox_H: 6.8150, d4.loss_cls_lane: 0.6922, d4.loss_cls_H: 0.6922, d4.loss_bbox_lane: 6.8138, d4.loss_bbox_H: 6.8150, vlm_loss: 0.0000, loss: 134.0362, grad_norm: nan
2025-03-01 03:31:22,453 - mmdet - INFO - Iter [6800/168780]	lr: 3.984e-04, eta: 5 days, 15:34:44, time: 3.025, data_time: 0.017, memory: 22041, loss_cls: 1.4695, loss_bbox: 2.8624, d0.loss_cls: 1.4695, d0.loss_bbox: 2.8624, d1.loss_cls: 1.4695, d1.loss_bbox: 2.8624, d2.loss_cls: 1.4695, d2.loss_bbox: 2.8624, d3.loss_cls: 1.4695, d3.loss_bbox: 2.8624, d4.loss_cls: 1.4695, d4.loss_bbox: 2.8624, dn_loss_cls: 0.9140, dn_loss_bbox: 2.5371, d0.dn_loss_cls: 0.9140, d0.dn_loss_bbox: 2.5371, d1.dn_loss_cls: 0.9140, d1.dn_loss_bbox: 2.5371, d2.dn_loss_cls: 0.9140, d2.dn_loss_bbox: 2.5371, d3.dn_loss_cls: 0.9140, d3.dn_loss_bbox: 2.5371, d4.dn_loss_cls: 0.9140, d4.dn_loss_bbox: 2.5371, loss_cls_lane: 0.6558, loss_cls_H: 0.6558, loss_bbox_lane: 7.1847, loss_bbox_H: 7.1924, d0.loss_cls_lane: 0.6558, d0.loss_cls_H: 0.6558, d0.loss_bbox_lane: 7.1847, d0.loss_bbox_H: 7.1924, d1.loss_cls_lane: 0.6558, d1.loss_cls_H: 0.6558, d1.loss_bbox_lane: 7.1847, d1.loss_bbox_H: 7.1924, d2.loss_cls_lane: 0.6558, d2.loss_cls_H: 0.6558, d2.loss_bbox_lane: 7.1847, d2.loss_bbox_H: 7.1924, d3.loss_cls_lane: 0.6558, d3.loss_cls_H: 0.6558, d3.loss_bbox_lane: 7.1847, d3.loss_bbox_H: 7.1924, d4.loss_cls_lane: 0.6558, d4.loss_cls_H: 0.6558, d4.loss_bbox_lane: 7.1847, d4.loss_bbox_H: 7.1924, vlm_loss: 0.0000, loss: 140.8294, grad_norm: nan
2025-03-01 03:33:51,021 - mmdet - INFO - Iter [6850/168780]	lr: 3.984e-04, eta: 5 days, 15:31:24, time: 2.971, data_time: 0.017, memory: 22041, loss_cls: 1.3910, loss_bbox: 2.7516, d0.loss_cls: 1.3910, d0.loss_bbox: 2.7516, d1.loss_cls: 1.3910, d1.loss_bbox: 2.7516, d2.loss_cls: 1.3910, d2.loss_bbox: 2.7516, d3.loss_cls: 1.3910, d3.loss_bbox: 2.7516, d4.loss_cls: 1.3910, d4.loss_bbox: 2.7516, dn_loss_cls: 0.9336, dn_loss_bbox: 2.4596, d0.dn_loss_cls: 0.9336, d0.dn_loss_bbox: 2.4596, d1.dn_loss_cls: 0.9336, d1.dn_loss_bbox: 2.4596, d2.dn_loss_cls: 0.9336, d2.dn_loss_bbox: 2.4596, d3.dn_loss_cls: 0.9336, d3.dn_loss_bbox: 2.4596, d4.dn_loss_cls: 0.9336, d4.dn_loss_bbox: 2.4596, loss_cls_lane: 0.6456, loss_cls_H: 0.6456, loss_bbox_lane: 6.6377, loss_bbox_H: 6.6194, d0.loss_cls_lane: 0.6456, d0.loss_cls_H: 0.6456, d0.loss_bbox_lane: 6.6377, d0.loss_bbox_H: 6.6194, d1.loss_cls_lane: 0.6456, d1.loss_cls_H: 0.6456, d1.loss_bbox_lane: 6.6377, d1.loss_bbox_H: 6.6194, d2.loss_cls_lane: 0.6456, d2.loss_cls_H: 0.6456, d2.loss_bbox_lane: 6.6377, d2.loss_bbox_H: 6.6194, d3.loss_cls_lane: 0.6456, d3.loss_cls_H: 0.6456, d3.loss_bbox_lane: 6.6377, d3.loss_bbox_H: 6.6194, d4.loss_cls_lane: 0.6456, d4.loss_cls_H: 0.6456, d4.loss_bbox_lane: 6.6377, d4.loss_bbox_H: 6.6194, vlm_loss: 0.0000, loss: 132.5049, grad_norm: nan
2025-03-01 03:36:19,694 - mmdet - INFO - Iter [6900/168780]	lr: 3.984e-04, eta: 5 days, 15:28:07, time: 2.973, data_time: 0.017, memory: 22041, loss_cls: 1.5626, loss_bbox: 2.7744, d0.loss_cls: 1.5626, d0.loss_bbox: 2.7744, d1.loss_cls: 1.5626, d1.loss_bbox: 2.7744, d2.loss_cls: 1.5626, d2.loss_bbox: 2.7744, d3.loss_cls: 1.5626, d3.loss_bbox: 2.7744, d4.loss_cls: 1.5626, d4.loss_bbox: 2.7744, dn_loss_cls: 0.8567, dn_loss_bbox: 2.4550, d0.dn_loss_cls: 0.8567, d0.dn_loss_bbox: 2.4550, d1.dn_loss_cls: 0.8567, d1.dn_loss_bbox: 2.4550, d2.dn_loss_cls: 0.8567, d2.dn_loss_bbox: 2.4550, d3.dn_loss_cls: 0.8567, d3.dn_loss_bbox: 2.4550, d4.dn_loss_cls: 0.8567, d4.dn_loss_bbox: 2.4550, loss_cls_lane: 0.6853, loss_cls_H: 0.6853, loss_bbox_lane: 7.0196, loss_bbox_H: 6.9993, d0.loss_cls_lane: 0.6853, d0.loss_cls_H: 0.6853, d0.loss_bbox_lane: 7.0196, d0.loss_bbox_H: 6.9993, d1.loss_cls_lane: 0.6853, d1.loss_cls_H: 0.6853, d1.loss_bbox_lane: 7.0196, d1.loss_bbox_H: 6.9993, d2.loss_cls_lane: 0.6853, d2.loss_cls_H: 0.6853, d2.loss_bbox_lane: 7.0196, d2.loss_bbox_H: 6.9993, d3.loss_cls_lane: 0.6853, d3.loss_cls_H: 0.6853, d3.loss_bbox_lane: 7.0196, d3.loss_bbox_H: 6.9993, d4.loss_cls_lane: 0.6853, d4.loss_cls_H: 0.6853, d4.loss_bbox_lane: 7.0196, d4.loss_bbox_H: 6.9993, vlm_loss: 0.0000, loss: 138.2290, grad_norm: nan
2025-03-01 03:38:49,395 - mmdet - INFO - Iter [6950/168780]	lr: 3.983e-04, eta: 5 days, 15:25:15, time: 2.994, data_time: 0.019, memory: 22041, loss_cls: 1.4426, loss_bbox: 2.6944, d0.loss_cls: 1.4426, d0.loss_bbox: 2.6944, d1.loss_cls: 1.4426, d1.loss_bbox: 2.6944, d2.loss_cls: 1.4426, d2.loss_bbox: 2.6944, d3.loss_cls: 1.4426, d3.loss_bbox: 2.6944, d4.loss_cls: 1.4426, d4.loss_bbox: 2.6944, dn_loss_cls: 0.9624, dn_loss_bbox: 2.2888, d0.dn_loss_cls: 0.9624, d0.dn_loss_bbox: 2.2888, d1.dn_loss_cls: 0.9624, d1.dn_loss_bbox: 2.2888, d2.dn_loss_cls: 0.9624, d2.dn_loss_bbox: 2.2888, d3.dn_loss_cls: 0.9624, d3.dn_loss_bbox: 2.2888, d4.dn_loss_cls: 0.9624, d4.dn_loss_bbox: 2.2888, loss_cls_lane: 0.6443, loss_cls_H: 0.6443, loss_bbox_lane: 6.8603, loss_bbox_H: 6.8532, d0.loss_cls_lane: 0.6443, d0.loss_cls_H: 0.6443, d0.loss_bbox_lane: 6.8603, d0.loss_bbox_H: 6.8532, d1.loss_cls_lane: 0.6443, d1.loss_cls_H: 0.6443, d1.loss_bbox_lane: 6.8603, d1.loss_bbox_H: 6.8532, d2.loss_cls_lane: 0.6443, d2.loss_cls_H: 0.6443, d2.loss_bbox_lane: 6.8603, d2.loss_bbox_H: 6.8532, d3.loss_cls_lane: 0.6443, d3.loss_cls_H: 0.6443, d3.loss_bbox_lane: 6.8603, d3.loss_bbox_H: 6.8532, d4.loss_cls_lane: 0.6443, d4.loss_cls_H: 0.6443, d4.loss_bbox_lane: 6.8603, d4.loss_bbox_H: 6.8532, vlm_loss: 0.0000, loss: 134.3420, grad_norm: nan
2025-03-01 03:41:29,472 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 03:41:29,472 - mmdet - INFO - Iter [7000/168780]	lr: 3.983e-04, eta: 5 days, 15:26:23, time: 3.202, data_time: 0.018, memory: 22041, loss_cls: 1.4251, loss_bbox: 2.7052, d0.loss_cls: 1.4251, d0.loss_bbox: 2.7052, d1.loss_cls: 1.4251, d1.loss_bbox: 2.7052, d2.loss_cls: 1.4251, d2.loss_bbox: 2.7052, d3.loss_cls: 1.4251, d3.loss_bbox: 2.7052, d4.loss_cls: 1.4251, d4.loss_bbox: 2.7052, dn_loss_cls: 0.9144, dn_loss_bbox: 2.3599, d0.dn_loss_cls: 0.9144, d0.dn_loss_bbox: 2.3599, d1.dn_loss_cls: 0.9144, d1.dn_loss_bbox: 2.3599, d2.dn_loss_cls: 0.9144, d2.dn_loss_bbox: 2.3599, d3.dn_loss_cls: 0.9144, d3.dn_loss_bbox: 2.3599, d4.dn_loss_cls: 0.9144, d4.dn_loss_bbox: 2.3599, loss_cls_lane: 0.6403, loss_cls_H: 0.6403, loss_bbox_lane: 6.5794, loss_bbox_H: 6.5864, d0.loss_cls_lane: 0.6403, d0.loss_cls_H: 0.6403, d0.loss_bbox_lane: 6.5794, d0.loss_bbox_H: 6.5864, d1.loss_cls_lane: 0.6403, d1.loss_cls_H: 0.6403, d1.loss_bbox_lane: 6.5794, d1.loss_bbox_H: 6.5864, d2.loss_cls_lane: 0.6403, d2.loss_cls_H: 0.6403, d2.loss_bbox_lane: 6.5794, d2.loss_bbox_H: 6.5864, d3.loss_cls_lane: 0.6403, d3.loss_cls_H: 0.6403, d3.loss_bbox_lane: 6.5794, d3.loss_bbox_H: 6.5864, d4.loss_cls_lane: 0.6403, d4.loss_cls_H: 0.6403, d4.loss_bbox_lane: 6.5794, d4.loss_bbox_H: 6.5864, vlm_loss: 0.0000, loss: 131.1065, grad_norm: nan
2025-03-01 03:43:56,244 - mmdet - INFO - Iter [7050/168780]	lr: 3.983e-04, eta: 5 days, 15:22:22, time: 2.935, data_time: 0.017, memory: 22041, loss_cls: 1.8698, loss_bbox: 2.6859, d0.loss_cls: 1.8698, d0.loss_bbox: 2.6859, d1.loss_cls: 1.8698, d1.loss_bbox: 2.6859, d2.loss_cls: 1.8698, d2.loss_bbox: 2.6859, d3.loss_cls: 1.8698, d3.loss_bbox: 2.6859, d4.loss_cls: 1.8698, d4.loss_bbox: 2.6859, dn_loss_cls: 0.9884, dn_loss_bbox: 2.3850, d0.dn_loss_cls: 0.9884, d0.dn_loss_bbox: 2.3850, d1.dn_loss_cls: 0.9884, d1.dn_loss_bbox: 2.3850, d2.dn_loss_cls: 0.9884, d2.dn_loss_bbox: 2.3850, d3.dn_loss_cls: 0.9884, d3.dn_loss_bbox: 2.3850, d4.dn_loss_cls: 0.9884, d4.dn_loss_bbox: 2.3850, loss_cls_lane: 0.6532, loss_cls_H: 1.0832, loss_bbox_lane: 5.6543, loss_bbox_H: 5.6650, d0.loss_cls_lane: 0.6532, d0.loss_cls_H: 1.0832, d0.loss_bbox_lane: 5.6543, d0.loss_bbox_H: 5.6650, d1.loss_cls_lane: 0.6532, d1.loss_cls_H: 1.0832, d1.loss_bbox_lane: 5.6543, d1.loss_bbox_H: 5.6650, d2.loss_cls_lane: 0.6532, d2.loss_cls_H: 1.0832, d2.loss_bbox_lane: 5.6543, d2.loss_bbox_H: 5.6650, d3.loss_cls_lane: 0.6532, d3.loss_cls_H: 1.0832, d3.loss_bbox_lane: 5.6543, d3.loss_bbox_H: 5.6650, d4.loss_cls_lane: 0.6532, d4.loss_cls_H: 1.0832, d4.loss_bbox_lane: 5.6543, d4.loss_bbox_H: 5.6650, vlm_loss: 0.0000, loss: 125.9083, grad_norm: nan
2025-03-01 03:46:22,711 - mmdet - INFO - Iter [7100/168780]	lr: 3.983e-04, eta: 5 days, 15:18:16, time: 2.929, data_time: 0.016, memory: 22041, loss_cls: 2.2588, loss_bbox: 2.8713, d0.loss_cls: 2.2588, d0.loss_bbox: 2.8713, d1.loss_cls: 2.2588, d1.loss_bbox: 2.8713, d2.loss_cls: 2.2588, d2.loss_bbox: 2.8713, d3.loss_cls: 2.2588, d3.loss_bbox: 2.8713, d4.loss_cls: 2.2588, d4.loss_bbox: 2.8713, dn_loss_cls: 0.9711, dn_loss_bbox: 2.5542, d0.dn_loss_cls: 0.9711, d0.dn_loss_bbox: 2.5542, d1.dn_loss_cls: 0.9711, d1.dn_loss_bbox: 2.5542, d2.dn_loss_cls: 0.9711, d2.dn_loss_bbox: 2.5542, d3.dn_loss_cls: 0.9711, d3.dn_loss_bbox: 2.5542, d4.dn_loss_cls: 0.9711, d4.dn_loss_bbox: 2.5542, loss_cls_lane: 0.6462, loss_cls_H: 0.9149, loss_bbox_lane: 5.7775, loss_bbox_H: 5.7870, d0.loss_cls_lane: 0.6462, d0.loss_cls_H: 0.9149, d0.loss_bbox_lane: 5.7775, d0.loss_bbox_H: 5.7870, d1.loss_cls_lane: 0.6462, d1.loss_cls_H: 0.9149, d1.loss_bbox_lane: 5.7775, d1.loss_bbox_H: 5.7870, d2.loss_cls_lane: 0.6462, d2.loss_cls_H: 0.9149, d2.loss_bbox_lane: 5.7775, d2.loss_bbox_H: 5.7870, d3.loss_cls_lane: 0.6462, d3.loss_cls_H: 0.9149, d3.loss_bbox_lane: 5.7775, d3.loss_bbox_H: 5.7870, d4.loss_cls_lane: 0.6462, d4.loss_cls_H: 0.9149, d4.loss_bbox_lane: 5.7775, d4.loss_bbox_H: 5.7870, vlm_loss: 0.0000, loss: 130.6856, grad_norm: nan
2025-03-01 03:48:49,475 - mmdet - INFO - Iter [7150/168780]	lr: 3.982e-04, eta: 5 days, 15:14:17, time: 2.935, data_time: 0.017, memory: 22041, loss_cls: 1.9703, loss_bbox: 2.8640, d0.loss_cls: 1.9703, d0.loss_bbox: 2.8640, d1.loss_cls: 1.9703, d1.loss_bbox: 2.8640, d2.loss_cls: 1.9703, d2.loss_bbox: 2.8640, d3.loss_cls: 1.9703, d3.loss_bbox: 2.8640, d4.loss_cls: 1.9703, d4.loss_bbox: 2.8640, dn_loss_cls: 0.8888, dn_loss_bbox: 2.4268, d0.dn_loss_cls: 0.8888, d0.dn_loss_bbox: 2.4268, d1.dn_loss_cls: 0.8888, d1.dn_loss_bbox: 2.4268, d2.dn_loss_cls: 0.8888, d2.dn_loss_bbox: 2.4268, d3.dn_loss_cls: 0.8888, d3.dn_loss_bbox: 2.4268, d4.dn_loss_cls: 0.8888, d4.dn_loss_bbox: 2.4268, loss_cls_lane: 0.6360, loss_cls_H: 0.6360, loss_bbox_lane: 6.4054, loss_bbox_H: 6.4008, d0.loss_cls_lane: 0.6360, d0.loss_cls_H: 0.6360, d0.loss_bbox_lane: 6.4054, d0.loss_bbox_H: 6.4008, d1.loss_cls_lane: 0.6360, d1.loss_cls_H: 0.6360, d1.loss_bbox_lane: 6.4054, d1.loss_bbox_H: 6.4008, d2.loss_cls_lane: 0.6360, d2.loss_cls_H: 0.6360, d2.loss_bbox_lane: 6.4054, d2.loss_bbox_H: 6.4008, d3.loss_cls_lane: 0.6360, d3.loss_cls_H: 0.6360, d3.loss_bbox_lane: 6.4054, d3.loss_bbox_H: 6.4008, d4.loss_cls_lane: 0.6360, d4.loss_cls_H: 0.6360, d4.loss_bbox_lane: 6.4054, d4.loss_bbox_H: 6.4008, vlm_loss: 0.0000, loss: 133.3682, grad_norm: nan
2025-03-01 03:51:17,697 - mmdet - INFO - Iter [7200/168780]	lr: 3.982e-04, eta: 5 days, 15:10:53, time: 2.964, data_time: 0.017, memory: 22041, loss_cls: 1.6760, loss_bbox: 3.0801, d0.loss_cls: 1.6760, d0.loss_bbox: 3.0801, d1.loss_cls: 1.6760, d1.loss_bbox: 3.0801, d2.loss_cls: 1.6760, d2.loss_bbox: 3.0801, d3.loss_cls: 1.6760, d3.loss_bbox: 3.0801, d4.loss_cls: 1.6760, d4.loss_bbox: 3.0801, dn_loss_cls: 0.9233, dn_loss_bbox: 2.6682, d0.dn_loss_cls: 0.9233, d0.dn_loss_bbox: 2.6682, d1.dn_loss_cls: 0.9233, d1.dn_loss_bbox: 2.6682, d2.dn_loss_cls: 0.9233, d2.dn_loss_bbox: 2.6682, d3.dn_loss_cls: 0.9233, d3.dn_loss_bbox: 2.6682, d4.dn_loss_cls: 0.9233, d4.dn_loss_bbox: 2.6682, loss_cls_lane: 0.6396, loss_cls_H: 0.6396, loss_bbox_lane: 6.6663, loss_bbox_H: 6.6565, d0.loss_cls_lane: 0.6396, d0.loss_cls_H: 0.6396, d0.loss_bbox_lane: 6.6663, d0.loss_bbox_H: 6.6565, d1.loss_cls_lane: 0.6396, d1.loss_cls_H: 0.6396, d1.loss_bbox_lane: 6.6663, d1.loss_bbox_H: 6.6565, d2.loss_cls_lane: 0.6396, d2.loss_cls_H: 0.6396, d2.loss_bbox_lane: 6.6663, d2.loss_bbox_H: 6.6565, d3.loss_cls_lane: 0.6396, d3.loss_cls_H: 0.6396, d3.loss_bbox_lane: 6.6663, d3.loss_bbox_H: 6.6565, d4.loss_cls_lane: 0.6396, d4.loss_cls_H: 0.6396, d4.loss_bbox_lane: 6.6663, d4.loss_bbox_H: 6.6565, vlm_loss: 0.0000, loss: 137.6983, grad_norm: nan
2025-03-01 03:53:46,747 - mmdet - INFO - Iter [7250/168780]	lr: 3.982e-04, eta: 5 days, 15:07:48, time: 2.981, data_time: 0.018, memory: 22041, loss_cls: 1.6153, loss_bbox: 2.6684, d0.loss_cls: 1.6153, d0.loss_bbox: 2.6684, d1.loss_cls: 1.6153, d1.loss_bbox: 2.6684, d2.loss_cls: 1.6153, d2.loss_bbox: 2.6684, d3.loss_cls: 1.6153, d3.loss_bbox: 2.6684, d4.loss_cls: 1.6153, d4.loss_bbox: 2.6684, dn_loss_cls: 1.0904, dn_loss_bbox: 2.3493, d0.dn_loss_cls: 1.0904, d0.dn_loss_bbox: 2.3493, d1.dn_loss_cls: 1.0904, d1.dn_loss_bbox: 2.3493, d2.dn_loss_cls: 1.0904, d2.dn_loss_bbox: 2.3493, d3.dn_loss_cls: 1.0904, d3.dn_loss_bbox: 2.3493, d4.dn_loss_cls: 1.0904, d4.dn_loss_bbox: 2.3493, loss_cls_lane: 0.6322, loss_cls_H: 0.6322, loss_bbox_lane: 6.5208, loss_bbox_H: 6.5159, d0.loss_cls_lane: 0.6322, d0.loss_cls_H: 0.6322, d0.loss_bbox_lane: 6.5208, d0.loss_bbox_H: 6.5159, d1.loss_cls_lane: 0.6322, d1.loss_cls_H: 0.6322, d1.loss_bbox_lane: 6.5208, d1.loss_bbox_H: 6.5159, d2.loss_cls_lane: 0.6322, d2.loss_cls_H: 0.6322, d2.loss_bbox_lane: 6.5208, d2.loss_bbox_H: 6.5159, d3.loss_cls_lane: 0.6322, d3.loss_cls_H: 0.6322, d3.loss_bbox_lane: 6.5208, d3.loss_bbox_H: 6.5159, d4.loss_cls_lane: 0.6322, d4.loss_cls_H: 0.6322, d4.loss_bbox_lane: 6.5208, d4.loss_bbox_H: 6.5159, vlm_loss: 0.0000, loss: 132.1471, grad_norm: nan
2025-03-01 03:56:39,164 - mmdet - INFO - Iter [7300/168780]	lr: 3.982e-04, eta: 5 days, 15:13:21, time: 3.448, data_time: 0.018, memory: 22041, loss_cls: 1.3494, loss_bbox: 2.9733, d0.loss_cls: 1.3494, d0.loss_bbox: 2.9733, d1.loss_cls: 1.3494, d1.loss_bbox: 2.9733, d2.loss_cls: 1.3494, d2.loss_bbox: 2.9733, d3.loss_cls: 1.3494, d3.loss_bbox: 2.9733, d4.loss_cls: 1.3494, d4.loss_bbox: 2.9733, dn_loss_cls: 0.9120, dn_loss_bbox: 2.4390, d0.dn_loss_cls: 0.9120, d0.dn_loss_bbox: 2.4390, d1.dn_loss_cls: 0.9120, d1.dn_loss_bbox: 2.4390, d2.dn_loss_cls: 0.9120, d2.dn_loss_bbox: 2.4390, d3.dn_loss_cls: 0.9120, d3.dn_loss_bbox: 2.4390, d4.dn_loss_cls: 0.9120, d4.dn_loss_bbox: 2.4390, loss_cls_lane: 0.6433, loss_cls_H: 0.6433, loss_bbox_lane: 6.3092, loss_bbox_H: 6.3246, d0.loss_cls_lane: 0.6433, d0.loss_cls_H: 0.6433, d0.loss_bbox_lane: 6.3092, d0.loss_bbox_H: 6.3246, d1.loss_cls_lane: 0.6433, d1.loss_cls_H: 0.6433, d1.loss_bbox_lane: 6.3092, d1.loss_bbox_H: 6.3246, d2.loss_cls_lane: 0.6433, d2.loss_cls_H: 0.6433, d2.loss_bbox_lane: 6.3092, d2.loss_bbox_H: 6.3246, d3.loss_cls_lane: 0.6433, d3.loss_cls_H: 0.6433, d3.loss_bbox_lane: 6.3092, d3.loss_bbox_H: 6.3246, d4.loss_cls_lane: 0.6433, d4.loss_cls_H: 0.6433, d4.loss_bbox_lane: 6.3092, d4.loss_bbox_H: 6.3246, vlm_loss: 0.0000, loss: 129.5650, grad_norm: nan
2025-03-01 03:59:08,823 - mmdet - INFO - Iter [7350/168780]	lr: 3.981e-04, eta: 5 days, 15:10:26, time: 2.993, data_time: 0.017, memory: 22041, loss_cls: 1.3861, loss_bbox: 2.9035, d0.loss_cls: 1.3861, d0.loss_bbox: 2.9035, d1.loss_cls: 1.3861, d1.loss_bbox: 2.9035, d2.loss_cls: 1.3861, d2.loss_bbox: 2.9035, d3.loss_cls: 1.3861, d3.loss_bbox: 2.9035, d4.loss_cls: 1.3861, d4.loss_bbox: 2.9035, dn_loss_cls: 0.9317, dn_loss_bbox: 2.5507, d0.dn_loss_cls: 0.9317, d0.dn_loss_bbox: 2.5507, d1.dn_loss_cls: 0.9317, d1.dn_loss_bbox: 2.5507, d2.dn_loss_cls: 0.9317, d2.dn_loss_bbox: 2.5507, d3.dn_loss_cls: 0.9317, d3.dn_loss_bbox: 2.5507, d4.dn_loss_cls: 0.9317, d4.dn_loss_bbox: 2.5507, loss_cls_lane: 0.6293, loss_cls_H: 0.6293, loss_bbox_lane: 6.2195, loss_bbox_H: 6.2256, d0.loss_cls_lane: 0.6293, d0.loss_cls_H: 0.6293, d0.loss_bbox_lane: 6.2195, d0.loss_bbox_H: 6.2256, d1.loss_cls_lane: 0.6293, d1.loss_cls_H: 0.6293, d1.loss_bbox_lane: 6.2195, d1.loss_bbox_H: 6.2256, d2.loss_cls_lane: 0.6293, d2.loss_cls_H: 0.6293, d2.loss_bbox_lane: 6.2195, d2.loss_bbox_H: 6.2256, d3.loss_cls_lane: 0.6293, d3.loss_cls_H: 0.6293, d3.loss_bbox_lane: 6.2195, d3.loss_bbox_H: 6.2256, d4.loss_cls_lane: 0.6293, d4.loss_cls_H: 0.6293, d4.loss_bbox_lane: 6.2195, d4.loss_bbox_H: 6.2256, vlm_loss: 0.0000, loss: 128.8531, grad_norm: nan
2025-03-01 04:01:36,526 - mmdet - INFO - Iter [7400/168780]	lr: 3.981e-04, eta: 5 days, 15:06:50, time: 2.954, data_time: 0.017, memory: 22041, loss_cls: 1.6635, loss_bbox: 2.6690, d0.loss_cls: 1.6635, d0.loss_bbox: 2.6690, d1.loss_cls: 1.6635, d1.loss_bbox: 2.6690, d2.loss_cls: 1.6635, d2.loss_bbox: 2.6690, d3.loss_cls: 1.6635, d3.loss_bbox: 2.6690, d4.loss_cls: 1.6635, d4.loss_bbox: 2.6690, dn_loss_cls: 0.9710, dn_loss_bbox: 2.3372, d0.dn_loss_cls: 0.9710, d0.dn_loss_bbox: 2.3372, d1.dn_loss_cls: 0.9710, d1.dn_loss_bbox: 2.3372, d2.dn_loss_cls: 0.9710, d2.dn_loss_bbox: 2.3372, d3.dn_loss_cls: 0.9710, d3.dn_loss_bbox: 2.3372, d4.dn_loss_cls: 0.9710, d4.dn_loss_bbox: 2.3372, loss_cls_lane: 0.6368, loss_cls_H: 0.6368, loss_bbox_lane: 6.5536, loss_bbox_H: 6.5416, d0.loss_cls_lane: 0.6368, d0.loss_cls_H: 0.6368, d0.loss_bbox_lane: 6.5536, d0.loss_bbox_H: 6.5416, d1.loss_cls_lane: 0.6368, d1.loss_cls_H: 0.6368, d1.loss_bbox_lane: 6.5536, d1.loss_bbox_H: 6.5416, d2.loss_cls_lane: 0.6368, d2.loss_cls_H: 0.6368, d2.loss_bbox_lane: 6.5536, d2.loss_bbox_H: 6.5416, d3.loss_cls_lane: 0.6368, d3.loss_cls_H: 0.6368, d3.loss_bbox_lane: 6.5536, d3.loss_bbox_H: 6.5416, d4.loss_cls_lane: 0.6368, d4.loss_cls_H: 0.6368, d4.loss_bbox_lane: 6.5536, d4.loss_bbox_H: 6.5416, vlm_loss: 0.0000, loss: 132.0573, grad_norm: nan
2025-03-01 04:04:04,581 - mmdet - INFO - Iter [7450/168780]	lr: 3.981e-04, eta: 5 days, 15:03:22, time: 2.961, data_time: 0.017, memory: 22041, loss_cls: 1.6391, loss_bbox: 2.8325, d0.loss_cls: 1.6391, d0.loss_bbox: 2.8325, d1.loss_cls: 1.6391, d1.loss_bbox: 2.8325, d2.loss_cls: 1.6391, d2.loss_bbox: 2.8325, d3.loss_cls: 1.6391, d3.loss_bbox: 2.8325, d4.loss_cls: 1.6391, d4.loss_bbox: 2.8325, dn_loss_cls: 0.9555, dn_loss_bbox: 2.5318, d0.dn_loss_cls: 0.9555, d0.dn_loss_bbox: 2.5318, d1.dn_loss_cls: 0.9555, d1.dn_loss_bbox: 2.5318, d2.dn_loss_cls: 0.9555, d2.dn_loss_bbox: 2.5318, d3.dn_loss_cls: 0.9555, d3.dn_loss_bbox: 2.5318, d4.dn_loss_cls: 0.9555, d4.dn_loss_bbox: 2.5318, loss_cls_lane: 0.6397, loss_cls_H: 0.6397, loss_bbox_lane: 6.7974, loss_bbox_H: 6.7863, d0.loss_cls_lane: 0.6397, d0.loss_cls_H: 0.6397, d0.loss_bbox_lane: 6.7974, d0.loss_bbox_H: 6.7863, d1.loss_cls_lane: 0.6397, d1.loss_cls_H: 0.6397, d1.loss_bbox_lane: 6.7974, d1.loss_bbox_H: 6.7863, d2.loss_cls_lane: 0.6397, d2.loss_cls_H: 0.6397, d2.loss_bbox_lane: 6.7974, d2.loss_bbox_H: 6.7863, d3.loss_cls_lane: 0.6397, d3.loss_cls_H: 0.6397, d3.loss_bbox_lane: 6.7974, d3.loss_bbox_H: 6.7863, d4.loss_cls_lane: 0.6397, d4.loss_cls_H: 0.6397, d4.loss_bbox_lane: 6.7974, d4.loss_bbox_H: 6.7863, vlm_loss: 0.0000, loss: 136.9325, grad_norm: nan
2025-03-01 04:06:32,489 - mmdet - INFO - Iter [7500/168780]	lr: 3.981e-04, eta: 5 days, 14:59:51, time: 2.958, data_time: 0.018, memory: 22041, loss_cls: 1.3998, loss_bbox: 2.6999, d0.loss_cls: 1.3998, d0.loss_bbox: 2.6999, d1.loss_cls: 1.3998, d1.loss_bbox: 2.6999, d2.loss_cls: 1.3998, d2.loss_bbox: 2.6999, d3.loss_cls: 1.3998, d3.loss_bbox: 2.6999, d4.loss_cls: 1.3998, d4.loss_bbox: 2.6999, dn_loss_cls: 0.9522, dn_loss_bbox: 2.3063, d0.dn_loss_cls: 0.9522, d0.dn_loss_bbox: 2.3063, d1.dn_loss_cls: 0.9522, d1.dn_loss_bbox: 2.3063, d2.dn_loss_cls: 0.9522, d2.dn_loss_bbox: 2.3063, d3.dn_loss_cls: 0.9522, d3.dn_loss_bbox: 2.3063, d4.dn_loss_cls: 0.9522, d4.dn_loss_bbox: 2.3063, loss_cls_lane: 0.6330, loss_cls_H: 0.6330, loss_bbox_lane: 6.3267, loss_bbox_H: 6.3283, d0.loss_cls_lane: 0.6330, d0.loss_cls_H: 0.6330, d0.loss_bbox_lane: 6.3267, d0.loss_bbox_H: 6.3283, d1.loss_cls_lane: 0.6330, d1.loss_cls_H: 0.6330, d1.loss_bbox_lane: 6.3267, d1.loss_bbox_H: 6.3283, d2.loss_cls_lane: 0.6330, d2.loss_cls_H: 0.6330, d2.loss_bbox_lane: 6.3267, d2.loss_bbox_H: 6.3283, d3.loss_cls_lane: 0.6330, d3.loss_cls_H: 0.6330, d3.loss_bbox_lane: 6.3267, d3.loss_bbox_H: 6.3283, d4.loss_cls_lane: 0.6330, d4.loss_cls_H: 0.6330, d4.loss_bbox_lane: 6.3267, d4.loss_bbox_H: 6.3283, vlm_loss: 0.0000, loss: 127.6759, grad_norm: nan
2025-03-01 04:09:00,611 - mmdet - INFO - Iter [7550/168780]	lr: 3.980e-04, eta: 5 days, 14:56:26, time: 2.962, data_time: 0.017, memory: 22041, loss_cls: 1.5367, loss_bbox: 2.7221, d0.loss_cls: 1.5367, d0.loss_bbox: 2.7221, d1.loss_cls: 1.5367, d1.loss_bbox: 2.7221, d2.loss_cls: 1.5367, d2.loss_bbox: 2.7221, d3.loss_cls: 1.5367, d3.loss_bbox: 2.7221, d4.loss_cls: 1.5367, d4.loss_bbox: 2.7221, dn_loss_cls: 0.9093, dn_loss_bbox: 2.4004, d0.dn_loss_cls: 0.9093, d0.dn_loss_bbox: 2.4004, d1.dn_loss_cls: 0.9093, d1.dn_loss_bbox: 2.4004, d2.dn_loss_cls: 0.9093, d2.dn_loss_bbox: 2.4004, d3.dn_loss_cls: 0.9093, d3.dn_loss_bbox: 2.4004, d4.dn_loss_cls: 0.9093, d4.dn_loss_bbox: 2.4004, loss_cls_lane: 0.6473, loss_cls_H: 0.6473, loss_bbox_lane: 6.5748, loss_bbox_H: 6.5707, d0.loss_cls_lane: 0.6473, d0.loss_cls_H: 0.6473, d0.loss_bbox_lane: 6.5748, d0.loss_bbox_H: 6.5707, d1.loss_cls_lane: 0.6473, d1.loss_cls_H: 0.6473, d1.loss_bbox_lane: 6.5748, d1.loss_bbox_H: 6.5707, d2.loss_cls_lane: 0.6473, d2.loss_cls_H: 0.6473, d2.loss_bbox_lane: 6.5748, d2.loss_bbox_H: 6.5707, d3.loss_cls_lane: 0.6473, d3.loss_cls_H: 0.6473, d3.loss_bbox_lane: 6.5748, d3.loss_bbox_H: 6.5707, d4.loss_cls_lane: 0.6473, d4.loss_cls_H: 0.6473, d4.loss_bbox_lane: 6.5748, d4.loss_bbox_H: 6.5707, vlm_loss: 0.0000, loss: 132.0525, grad_norm: nan
2025-03-01 04:11:29,271 - mmdet - INFO - Iter [7600/168780]	lr: 3.980e-04, eta: 5 days, 14:53:13, time: 2.973, data_time: 0.018, memory: 22041, loss_cls: 1.3520, loss_bbox: 2.7251, d0.loss_cls: 1.3520, d0.loss_bbox: 2.7251, d1.loss_cls: 1.3520, d1.loss_bbox: 2.7251, d2.loss_cls: 1.3520, d2.loss_bbox: 2.7251, d3.loss_cls: 1.3520, d3.loss_bbox: 2.7251, d4.loss_cls: 1.3520, d4.loss_bbox: 2.7251, dn_loss_cls: 0.9455, dn_loss_bbox: 2.3505, d0.dn_loss_cls: 0.9455, d0.dn_loss_bbox: 2.3505, d1.dn_loss_cls: 0.9455, d1.dn_loss_bbox: 2.3505, d2.dn_loss_cls: 0.9455, d2.dn_loss_bbox: 2.3505, d3.dn_loss_cls: 0.9455, d3.dn_loss_bbox: 2.3505, d4.dn_loss_cls: 0.9455, d4.dn_loss_bbox: 2.3505, loss_cls_lane: 0.6426, loss_cls_H: 0.7501, loss_bbox_lane: 6.3436, loss_bbox_H: 6.3409, d0.loss_cls_lane: 0.6426, d0.loss_cls_H: 0.7501, d0.loss_bbox_lane: 6.3436, d0.loss_bbox_H: 6.3409, d1.loss_cls_lane: 0.6426, d1.loss_cls_H: 0.7501, d1.loss_bbox_lane: 6.3436, d1.loss_bbox_H: 6.3409, d2.loss_cls_lane: 0.6426, d2.loss_cls_H: 0.7501, d2.loss_bbox_lane: 6.3436, d2.loss_bbox_H: 6.3409, d3.loss_cls_lane: 0.6426, d3.loss_cls_H: 0.7501, d3.loss_bbox_lane: 6.3436, d3.loss_bbox_H: 6.3409, d4.loss_cls_lane: 0.6426, d4.loss_cls_H: 0.7501, d4.loss_bbox_lane: 6.3436, d4.loss_bbox_H: 6.3409, vlm_loss: 0.0000, loss: 128.7025, grad_norm: nan
2025-03-01 04:13:58,294 - mmdet - INFO - Iter [7650/168780]	lr: 3.980e-04, eta: 5 days, 14:50:09, time: 2.980, data_time: 0.018, memory: 22041, loss_cls: 1.3842, loss_bbox: 2.6691, d0.loss_cls: 1.3842, d0.loss_bbox: 2.6691, d1.loss_cls: 1.3842, d1.loss_bbox: 2.6691, d2.loss_cls: 1.3842, d2.loss_bbox: 2.6691, d3.loss_cls: 1.3842, d3.loss_bbox: 2.6691, d4.loss_cls: 1.3842, d4.loss_bbox: 2.6691, dn_loss_cls: 0.9148, dn_loss_bbox: 2.3899, d0.dn_loss_cls: 0.9148, d0.dn_loss_bbox: 2.3899, d1.dn_loss_cls: 0.9148, d1.dn_loss_bbox: 2.3899, d2.dn_loss_cls: 0.9148, d2.dn_loss_bbox: 2.3899, d3.dn_loss_cls: 0.9148, d3.dn_loss_bbox: 2.3899, d4.dn_loss_cls: 0.9148, d4.dn_loss_bbox: 2.3899, loss_cls_lane: 0.6352, loss_cls_H: 0.6352, loss_bbox_lane: 6.3913, loss_bbox_H: 6.3961, d0.loss_cls_lane: 0.6352, d0.loss_cls_H: 0.6352, d0.loss_bbox_lane: 6.3913, d0.loss_bbox_H: 6.3961, d1.loss_cls_lane: 0.6352, d1.loss_cls_H: 0.6352, d1.loss_bbox_lane: 6.3913, d1.loss_bbox_H: 6.3961, d2.loss_cls_lane: 0.6352, d2.loss_cls_H: 0.6352, d2.loss_bbox_lane: 6.3913, d2.loss_bbox_H: 6.3961, d3.loss_cls_lane: 0.6352, d3.loss_cls_H: 0.6352, d3.loss_bbox_lane: 6.3913, d3.loss_bbox_H: 6.3961, d4.loss_cls_lane: 0.6352, d4.loss_cls_H: 0.6352, d4.loss_bbox_lane: 6.3913, d4.loss_bbox_H: 6.3961, vlm_loss: 0.0000, loss: 128.4941, grad_norm: nan
2025-03-01 04:16:26,964 - mmdet - INFO - Iter [7700/168780]	lr: 3.980e-04, eta: 5 days, 14:46:57, time: 2.973, data_time: 0.017, memory: 22041, loss_cls: 1.5369, loss_bbox: 2.8553, d0.loss_cls: 1.5369, d0.loss_bbox: 2.8553, d1.loss_cls: 1.5369, d1.loss_bbox: 2.8553, d2.loss_cls: 1.5369, d2.loss_bbox: 2.8553, d3.loss_cls: 1.5369, d3.loss_bbox: 2.8553, d4.loss_cls: 1.5369, d4.loss_bbox: 2.8553, dn_loss_cls: 0.9533, dn_loss_bbox: 2.6014, d0.dn_loss_cls: 0.9533, d0.dn_loss_bbox: 2.6014, d1.dn_loss_cls: 0.9533, d1.dn_loss_bbox: 2.6014, d2.dn_loss_cls: 0.9533, d2.dn_loss_bbox: 2.6014, d3.dn_loss_cls: 0.9533, d3.dn_loss_bbox: 2.6014, d4.dn_loss_cls: 0.9533, d4.dn_loss_bbox: 2.6014, loss_cls_lane: 0.6285, loss_cls_H: 0.6285, loss_bbox_lane: 6.6819, loss_bbox_H: 6.6713, d0.loss_cls_lane: 0.6285, d0.loss_cls_H: 0.6285, d0.loss_bbox_lane: 6.6819, d0.loss_bbox_H: 6.6713, d1.loss_cls_lane: 0.6285, d1.loss_cls_H: 0.6285, d1.loss_bbox_lane: 6.6819, d1.loss_bbox_H: 6.6713, d2.loss_cls_lane: 0.6285, d2.loss_cls_H: 0.6285, d2.loss_bbox_lane: 6.6819, d2.loss_bbox_H: 6.6713, d3.loss_cls_lane: 0.6285, d3.loss_cls_H: 0.6285, d3.loss_bbox_lane: 6.6819, d3.loss_bbox_H: 6.6713, d4.loss_cls_lane: 0.6285, d4.loss_cls_H: 0.6285, d4.loss_bbox_lane: 6.6819, d4.loss_bbox_H: 6.6713, vlm_loss: 0.0000, loss: 135.3425, grad_norm: nan
2025-03-01 04:18:55,982 - mmdet - INFO - Iter [7750/168780]	lr: 3.979e-04, eta: 5 days, 14:43:53, time: 2.980, data_time: 0.017, memory: 22041, loss_cls: 1.6031, loss_bbox: 2.8819, d0.loss_cls: 1.6031, d0.loss_bbox: 2.8819, d1.loss_cls: 1.6031, d1.loss_bbox: 2.8819, d2.loss_cls: 1.6031, d2.loss_bbox: 2.8819, d3.loss_cls: 1.6031, d3.loss_bbox: 2.8819, d4.loss_cls: 1.6031, d4.loss_bbox: 2.8819, dn_loss_cls: 0.9279, dn_loss_bbox: 2.5760, d0.dn_loss_cls: 0.9279, d0.dn_loss_bbox: 2.5760, d1.dn_loss_cls: 0.9279, d1.dn_loss_bbox: 2.5760, d2.dn_loss_cls: 0.9279, d2.dn_loss_bbox: 2.5760, d3.dn_loss_cls: 0.9279, d3.dn_loss_bbox: 2.5760, d4.dn_loss_cls: 0.9279, d4.dn_loss_bbox: 2.5760, loss_cls_lane: 0.6364, loss_cls_H: 0.6364, loss_bbox_lane: 6.3550, loss_bbox_H: 6.3773, d0.loss_cls_lane: 0.6364, d0.loss_cls_H: 0.6364, d0.loss_bbox_lane: 6.3550, d0.loss_bbox_H: 6.3773, d1.loss_cls_lane: 0.6364, d1.loss_cls_H: 0.6364, d1.loss_bbox_lane: 6.3550, d1.loss_bbox_H: 6.3773, d2.loss_cls_lane: 0.6364, d2.loss_cls_H: 0.6364, d2.loss_bbox_lane: 6.3550, d2.loss_bbox_H: 6.3773, d3.loss_cls_lane: 0.6364, d3.loss_cls_H: 0.6364, d3.loss_bbox_lane: 6.3550, d3.loss_bbox_H: 6.3773, d4.loss_cls_lane: 0.6364, d4.loss_cls_H: 0.6364, d4.loss_bbox_lane: 6.3550, d4.loss_bbox_H: 6.3773, vlm_loss: 0.0000, loss: 131.9640, grad_norm: nan
2025-03-01 04:21:23,447 - mmdet - INFO - Iter [7800/168780]	lr: 3.979e-04, eta: 5 days, 14:40:18, time: 2.949, data_time: 0.017, memory: 22041, loss_cls: 2.4816, loss_bbox: 2.9647, d0.loss_cls: 2.4816, d0.loss_bbox: 2.9647, d1.loss_cls: 2.4816, d1.loss_bbox: 2.9647, d2.loss_cls: 2.4816, d2.loss_bbox: 2.9647, d3.loss_cls: 2.4816, d3.loss_bbox: 2.9647, d4.loss_cls: 2.4816, d4.loss_bbox: 2.9647, dn_loss_cls: 0.8765, dn_loss_bbox: 2.7021, d0.dn_loss_cls: 0.8765, d0.dn_loss_bbox: 2.7021, d1.dn_loss_cls: 0.8765, d1.dn_loss_bbox: 2.7021, d2.dn_loss_cls: 0.8765, d2.dn_loss_bbox: 2.7021, d3.dn_loss_cls: 0.8765, d3.dn_loss_bbox: 2.7021, d4.dn_loss_cls: 0.8765, d4.dn_loss_bbox: 2.7021, loss_cls_lane: 0.6347, loss_cls_H: 0.6347, loss_bbox_lane: 6.5130, loss_bbox_H: 6.5106, d0.loss_cls_lane: 0.6347, d0.loss_cls_H: 0.6347, d0.loss_bbox_lane: 6.5130, d0.loss_bbox_H: 6.5106, d1.loss_cls_lane: 0.6347, d1.loss_cls_H: 0.6347, d1.loss_bbox_lane: 6.5130, d1.loss_bbox_H: 6.5106, d2.loss_cls_lane: 0.6347, d2.loss_cls_H: 0.6347, d2.loss_bbox_lane: 6.5130, d2.loss_bbox_H: 6.5106, d3.loss_cls_lane: 0.6347, d3.loss_cls_H: 0.6347, d3.loss_bbox_lane: 6.5130, d3.loss_bbox_H: 6.5106, d4.loss_cls_lane: 0.6347, d4.loss_cls_H: 0.6347, d4.loss_bbox_lane: 6.5130, d4.loss_bbox_H: 6.5106, vlm_loss: 0.0000, loss: 139.9058, grad_norm: nan
2025-03-01 04:23:49,867 - mmdet - INFO - Iter [7850/168780]	lr: 3.979e-04, eta: 5 days, 14:36:22, time: 2.928, data_time: 0.017, memory: 22041, loss_cls: 2.8348, loss_bbox: 2.6857, d0.loss_cls: 2.8348, d0.loss_bbox: 2.6857, d1.loss_cls: 2.8348, d1.loss_bbox: 2.6857, d2.loss_cls: 2.8348, d2.loss_bbox: 2.6857, d3.loss_cls: 2.8348, d3.loss_bbox: 2.6857, d4.loss_cls: 2.8348, d4.loss_bbox: 2.6857, dn_loss_cls: 1.3371, dn_loss_bbox: 2.2788, d0.dn_loss_cls: 1.3371, d0.dn_loss_bbox: 2.2788, d1.dn_loss_cls: 1.3371, d1.dn_loss_bbox: 2.2788, d2.dn_loss_cls: 1.3371, d2.dn_loss_bbox: 2.2788, d3.dn_loss_cls: 1.3371, d3.dn_loss_bbox: 2.2788, d4.dn_loss_cls: 1.3371, d4.dn_loss_bbox: 2.2788, loss_cls_lane: 0.6517, loss_cls_H: 0.6517, loss_bbox_lane: 6.6021, loss_bbox_H: 6.6062, d0.loss_cls_lane: 0.6517, d0.loss_cls_H: 0.6517, d0.loss_bbox_lane: 6.6021, d0.loss_bbox_H: 6.6062, d1.loss_cls_lane: 0.6517, d1.loss_cls_H: 0.6517, d1.loss_bbox_lane: 6.6021, d1.loss_bbox_H: 6.6062, d2.loss_cls_lane: 0.6517, d2.loss_cls_H: 0.6517, d2.loss_bbox_lane: 6.6021, d2.loss_bbox_H: 6.6062, d3.loss_cls_lane: 0.6517, d3.loss_cls_H: 0.6517, d3.loss_bbox_lane: 6.6021, d3.loss_bbox_H: 6.6062, d4.loss_cls_lane: 0.6517, d4.loss_cls_H: 0.6517, d4.loss_bbox_lane: 6.6021, d4.loss_bbox_H: 6.6062, vlm_loss: 0.0000, loss: 141.8884, grad_norm: nan
2025-03-01 04:26:18,593 - mmdet - INFO - Iter [7900/168780]	lr: 3.978e-04, eta: 5 days, 14:33:14, time: 2.975, data_time: 0.017, memory: 22041, loss_cls: 1.3890, loss_bbox: 2.7233, d0.loss_cls: 1.3890, d0.loss_bbox: 2.7233, d1.loss_cls: 1.3890, d1.loss_bbox: 2.7233, d2.loss_cls: 1.3890, d2.loss_bbox: 2.7233, d3.loss_cls: 1.3890, d3.loss_bbox: 2.7233, d4.loss_cls: 1.3890, d4.loss_bbox: 2.7233, dn_loss_cls: 0.9899, dn_loss_bbox: 2.2212, d0.dn_loss_cls: 0.9899, d0.dn_loss_bbox: 2.2212, d1.dn_loss_cls: 0.9899, d1.dn_loss_bbox: 2.2212, d2.dn_loss_cls: 0.9899, d2.dn_loss_bbox: 2.2212, d3.dn_loss_cls: 0.9899, d3.dn_loss_bbox: 2.2212, d4.dn_loss_cls: 0.9899, d4.dn_loss_bbox: 2.2212, loss_cls_lane: 0.6266, loss_cls_H: 0.6266, loss_bbox_lane: 6.6229, loss_bbox_H: 6.6151, d0.loss_cls_lane: 0.6266, d0.loss_cls_H: 0.6266, d0.loss_bbox_lane: 6.6229, d0.loss_bbox_H: 6.6151, d1.loss_cls_lane: 0.6266, d1.loss_cls_H: 0.6266, d1.loss_bbox_lane: 6.6229, d1.loss_bbox_H: 6.6151, d2.loss_cls_lane: 0.6266, d2.loss_cls_H: 0.6266, d2.loss_bbox_lane: 6.6229, d2.loss_bbox_H: 6.6151, d3.loss_cls_lane: 0.6266, d3.loss_cls_H: 0.6266, d3.loss_bbox_lane: 6.6229, d3.loss_bbox_H: 6.6151, d4.loss_cls_lane: 0.6266, d4.loss_cls_H: 0.6266, d4.loss_bbox_lane: 6.6229, d4.loss_bbox_H: 6.6151, vlm_loss: 0.0000, loss: 130.8890, grad_norm: nan
2025-03-01 04:28:46,566 - mmdet - INFO - Iter [7950/168780]	lr: 3.978e-04, eta: 5 days, 14:29:52, time: 2.959, data_time: 0.017, memory: 22041, loss_cls: 2.0006, loss_bbox: 2.6380, d0.loss_cls: 2.0006, d0.loss_bbox: 2.6380, d1.loss_cls: 2.0006, d1.loss_bbox: 2.6380, d2.loss_cls: 2.0006, d2.loss_bbox: 2.6380, d3.loss_cls: 2.0006, d3.loss_bbox: 2.6380, d4.loss_cls: 2.0006, d4.loss_bbox: 2.6380, dn_loss_cls: 0.9821, dn_loss_bbox: 2.3425, d0.dn_loss_cls: 0.9821, d0.dn_loss_bbox: 2.3425, d1.dn_loss_cls: 0.9821, d1.dn_loss_bbox: 2.3425, d2.dn_loss_cls: 0.9821, d2.dn_loss_bbox: 2.3425, d3.dn_loss_cls: 0.9821, d3.dn_loss_bbox: 2.3425, d4.dn_loss_cls: 0.9821, d4.dn_loss_bbox: 2.3425, loss_cls_lane: 0.6338, loss_cls_H: 0.6338, loss_bbox_lane: 6.3991, loss_bbox_H: 6.3860, d0.loss_cls_lane: 0.6338, d0.loss_cls_H: 0.6338, d0.loss_bbox_lane: 6.3991, d0.loss_bbox_H: 6.3860, d1.loss_cls_lane: 0.6338, d1.loss_cls_H: 0.6338, d1.loss_bbox_lane: 6.3991, d1.loss_bbox_H: 6.3860, d2.loss_cls_lane: 0.6338, d2.loss_cls_H: 0.6338, d2.loss_bbox_lane: 6.3991, d2.loss_bbox_H: 6.3860, d3.loss_cls_lane: 0.6338, d3.loss_cls_H: 0.6338, d3.loss_bbox_lane: 6.3991, d3.loss_bbox_H: 6.3860, d4.loss_cls_lane: 0.6338, d4.loss_cls_H: 0.6338, d4.loss_bbox_lane: 6.3991, d4.loss_bbox_H: 6.3860, vlm_loss: 0.0000, loss: 132.0951, grad_norm: nan
2025-03-01 04:31:14,205 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 04:31:14,205 - mmdet - INFO - Iter [8000/168780]	lr: 3.978e-04, eta: 5 days, 14:26:23, time: 2.953, data_time: 0.017, memory: 22041, loss_cls: 1.8046, loss_bbox: 2.7053, d0.loss_cls: 1.8046, d0.loss_bbox: 2.7053, d1.loss_cls: 1.8046, d1.loss_bbox: 2.7053, d2.loss_cls: 1.8046, d2.loss_bbox: 2.7053, d3.loss_cls: 1.8046, d3.loss_bbox: 2.7053, d4.loss_cls: 1.8046, d4.loss_bbox: 2.7053, dn_loss_cls: 0.9844, dn_loss_bbox: 2.3963, d0.dn_loss_cls: 0.9844, d0.dn_loss_bbox: 2.3963, d1.dn_loss_cls: 0.9844, d1.dn_loss_bbox: 2.3963, d2.dn_loss_cls: 0.9844, d2.dn_loss_bbox: 2.3963, d3.dn_loss_cls: 0.9844, d3.dn_loss_bbox: 2.3963, d4.dn_loss_cls: 0.9844, d4.dn_loss_bbox: 2.3963, loss_cls_lane: 0.6351, loss_cls_H: 0.6351, loss_bbox_lane: 6.3814, loss_bbox_H: 6.3880, d0.loss_cls_lane: 0.6351, d0.loss_cls_H: 0.6351, d0.loss_bbox_lane: 6.3814, d0.loss_bbox_H: 6.3880, d1.loss_cls_lane: 0.6351, d1.loss_cls_H: 0.6351, d1.loss_bbox_lane: 6.3814, d1.loss_bbox_H: 6.3880, d2.loss_cls_lane: 0.6351, d2.loss_cls_H: 0.6351, d2.loss_bbox_lane: 6.3814, d2.loss_bbox_H: 6.3880, d3.loss_cls_lane: 0.6351, d3.loss_cls_H: 0.6351, d3.loss_bbox_lane: 6.3814, d3.loss_bbox_H: 6.3880, d4.loss_cls_lane: 0.6351, d4.loss_cls_H: 0.6351, d4.loss_bbox_lane: 6.3814, d4.loss_bbox_H: 6.3880, vlm_loss: 0.0000, loss: 131.5814, grad_norm: nan
2025-03-01 04:33:43,465 - mmdet - INFO - Iter [8050/168780]	lr: 3.978e-04, eta: 5 days, 14:23:28, time: 2.985, data_time: 0.017, memory: 22041, loss_cls: 1.5549, loss_bbox: 2.9283, d0.loss_cls: 1.5549, d0.loss_bbox: 2.9283, d1.loss_cls: 1.5549, d1.loss_bbox: 2.9283, d2.loss_cls: 1.5549, d2.loss_bbox: 2.9283, d3.loss_cls: 1.5549, d3.loss_bbox: 2.9283, d4.loss_cls: 1.5549, d4.loss_bbox: 2.9283, dn_loss_cls: 0.9452, dn_loss_bbox: 2.5442, d0.dn_loss_cls: 0.9452, d0.dn_loss_bbox: 2.5442, d1.dn_loss_cls: 0.9452, d1.dn_loss_bbox: 2.5442, d2.dn_loss_cls: 0.9452, d2.dn_loss_bbox: 2.5442, d3.dn_loss_cls: 0.9452, d3.dn_loss_bbox: 2.5442, d4.dn_loss_cls: 0.9452, d4.dn_loss_bbox: 2.5442, loss_cls_lane: 0.6308, loss_cls_H: 0.6308, loss_bbox_lane: 6.5445, loss_bbox_H: 6.5482, d0.loss_cls_lane: 0.6308, d0.loss_cls_H: 0.6308, d0.loss_bbox_lane: 6.5445, d0.loss_bbox_H: 6.5482, d1.loss_cls_lane: 0.6308, d1.loss_cls_H: 0.6308, d1.loss_bbox_lane: 6.5445, d1.loss_bbox_H: 6.5482, d2.loss_cls_lane: 0.6308, d2.loss_cls_H: 0.6308, d2.loss_bbox_lane: 6.5445, d2.loss_bbox_H: 6.5482, d3.loss_cls_lane: 0.6308, d3.loss_cls_H: 0.6308, d3.loss_bbox_lane: 6.5445, d3.loss_bbox_H: 6.5482, d4.loss_cls_lane: 0.6308, d4.loss_cls_H: 0.6308, d4.loss_bbox_lane: 6.5445, d4.loss_bbox_H: 6.5482, vlm_loss: 0.0000, loss: 133.9621, grad_norm: nan
2025-03-01 04:36:12,947 - mmdet - INFO - Iter [8100/168780]	lr: 3.977e-04, eta: 5 days, 14:20:37, time: 2.990, data_time: 0.018, memory: 22041, loss_cls: 1.4913, loss_bbox: 2.7373, d0.loss_cls: 1.4913, d0.loss_bbox: 2.7373, d1.loss_cls: 1.4913, d1.loss_bbox: 2.7373, d2.loss_cls: 1.4913, d2.loss_bbox: 2.7373, d3.loss_cls: 1.4913, d3.loss_bbox: 2.7373, d4.loss_cls: 1.4913, d4.loss_bbox: 2.7373, dn_loss_cls: 0.9144, dn_loss_bbox: 2.4358, d0.dn_loss_cls: 0.9144, d0.dn_loss_bbox: 2.4358, d1.dn_loss_cls: 0.9144, d1.dn_loss_bbox: 2.4358, d2.dn_loss_cls: 0.9144, d2.dn_loss_bbox: 2.4358, d3.dn_loss_cls: 0.9144, d3.dn_loss_bbox: 2.4358, d4.dn_loss_cls: 0.9144, d4.dn_loss_bbox: 2.4358, loss_cls_lane: 0.6353, loss_cls_H: 0.6353, loss_bbox_lane: 6.3801, loss_bbox_H: 6.3866, d0.loss_cls_lane: 0.6353, d0.loss_cls_H: 0.6353, d0.loss_bbox_lane: 6.3801, d0.loss_bbox_H: 6.3866, d1.loss_cls_lane: 0.6353, d1.loss_cls_H: 0.6353, d1.loss_bbox_lane: 6.3801, d1.loss_bbox_H: 6.3866, d2.loss_cls_lane: 0.6353, d2.loss_cls_H: 0.6353, d2.loss_bbox_lane: 6.3801, d2.loss_bbox_H: 6.3866, d3.loss_cls_lane: 0.6353, d3.loss_cls_H: 0.6353, d3.loss_bbox_lane: 6.3801, d3.loss_bbox_H: 6.3866, d4.loss_cls_lane: 0.6353, d4.loss_cls_H: 0.6353, d4.loss_bbox_lane: 6.3801, d4.loss_bbox_H: 6.3866, vlm_loss: 0.0000, loss: 129.6970, grad_norm: nan
2025-03-01 04:38:40,043 - mmdet - INFO - Iter [8150/168780]	lr: 3.977e-04, eta: 5 days, 14:16:59, time: 2.942, data_time: 0.016, memory: 22041, loss_cls: 1.9637, loss_bbox: 2.8475, d0.loss_cls: 1.9637, d0.loss_bbox: 2.8475, d1.loss_cls: 1.9637, d1.loss_bbox: 2.8475, d2.loss_cls: 1.9637, d2.loss_bbox: 2.8475, d3.loss_cls: 1.9637, d3.loss_bbox: 2.8475, d4.loss_cls: 1.9637, d4.loss_bbox: 2.8475, dn_loss_cls: 0.9175, dn_loss_bbox: 2.5268, d0.dn_loss_cls: 0.9175, d0.dn_loss_bbox: 2.5268, d1.dn_loss_cls: 0.9175, d1.dn_loss_bbox: 2.5268, d2.dn_loss_cls: 0.9175, d2.dn_loss_bbox: 2.5268, d3.dn_loss_cls: 0.9175, d3.dn_loss_bbox: 2.5268, d4.dn_loss_cls: 0.9175, d4.dn_loss_bbox: 2.5268, loss_cls_lane: 0.6448, loss_cls_H: 0.6448, loss_bbox_lane: 6.5971, loss_bbox_H: 6.5921, d0.loss_cls_lane: 0.6448, d0.loss_cls_H: 0.6448, d0.loss_bbox_lane: 6.5971, d0.loss_bbox_H: 6.5921, d1.loss_cls_lane: 0.6448, d1.loss_cls_H: 0.6448, d1.loss_bbox_lane: 6.5971, d1.loss_bbox_H: 6.5921, d2.loss_cls_lane: 0.6448, d2.loss_cls_H: 0.6448, d2.loss_bbox_lane: 6.5971, d2.loss_bbox_H: 6.5921, d3.loss_cls_lane: 0.6448, d3.loss_cls_H: 0.6448, d3.loss_bbox_lane: 6.5971, d3.loss_bbox_H: 6.5921, d4.loss_cls_lane: 0.6448, d4.loss_cls_H: 0.6448, d4.loss_bbox_lane: 6.5971, d4.loss_bbox_H: 6.5921, vlm_loss: 0.0000, loss: 136.4057, grad_norm: nan
2025-03-01 04:41:09,291 - mmdet - INFO - Iter [8200/168780]	lr: 3.977e-04, eta: 5 days, 14:14:05, time: 2.985, data_time: 0.017, memory: 22041, loss_cls: 1.4392, loss_bbox: 2.6533, d0.loss_cls: 1.4392, d0.loss_bbox: 2.6533, d1.loss_cls: 1.4392, d1.loss_bbox: 2.6533, d2.loss_cls: 1.4392, d2.loss_bbox: 2.6533, d3.loss_cls: 1.4392, d3.loss_bbox: 2.6533, d4.loss_cls: 1.4392, d4.loss_bbox: 2.6533, dn_loss_cls: 0.9472, dn_loss_bbox: 2.2951, d0.dn_loss_cls: 0.9472, d0.dn_loss_bbox: 2.2951, d1.dn_loss_cls: 0.9472, d1.dn_loss_bbox: 2.2951, d2.dn_loss_cls: 0.9472, d2.dn_loss_bbox: 2.2951, d3.dn_loss_cls: 0.9472, d3.dn_loss_bbox: 2.2951, d4.dn_loss_cls: 0.9472, d4.dn_loss_bbox: 2.2951, loss_cls_lane: 0.6575, loss_cls_H: 0.6575, loss_bbox_lane: 7.0643, loss_bbox_H: 7.0457, d0.loss_cls_lane: 0.6575, d0.loss_cls_H: 0.6575, d0.loss_bbox_lane: 7.0643, d0.loss_bbox_H: 7.0457, d1.loss_cls_lane: 0.6575, d1.loss_cls_H: 0.6575, d1.loss_bbox_lane: 7.0643, d1.loss_bbox_H: 7.0457, d2.loss_cls_lane: 0.6575, d2.loss_cls_H: 0.6575, d2.loss_bbox_lane: 7.0643, d2.loss_bbox_H: 7.0457, d3.loss_cls_lane: 0.6575, d3.loss_cls_H: 0.6575, d3.loss_bbox_lane: 7.0643, d3.loss_bbox_H: 7.0457, d4.loss_cls_lane: 0.6575, d4.loss_cls_H: 0.6575, d4.loss_bbox_lane: 7.0643, d4.loss_bbox_H: 7.0457, vlm_loss: 0.0000, loss: 136.5585, grad_norm: nan
2025-03-01 04:43:38,783 - mmdet - INFO - Iter [8250/168780]	lr: 3.976e-04, eta: 5 days, 14:11:15, time: 2.990, data_time: 0.018, memory: 22041, loss_cls: 1.3414, loss_bbox: 2.7140, d0.loss_cls: 1.3414, d0.loss_bbox: 2.7140, d1.loss_cls: 1.3414, d1.loss_bbox: 2.7140, d2.loss_cls: 1.3414, d2.loss_bbox: 2.7140, d3.loss_cls: 1.3414, d3.loss_bbox: 2.7140, d4.loss_cls: 1.3414, d4.loss_bbox: 2.7140, dn_loss_cls: 0.9836, dn_loss_bbox: 2.3593, d0.dn_loss_cls: 0.9836, d0.dn_loss_bbox: 2.3593, d1.dn_loss_cls: 0.9836, d1.dn_loss_bbox: 2.3593, d2.dn_loss_cls: 0.9836, d2.dn_loss_bbox: 2.3593, d3.dn_loss_cls: 0.9836, d3.dn_loss_bbox: 2.3593, d4.dn_loss_cls: 0.9836, d4.dn_loss_bbox: 2.3593, loss_cls_lane: 0.6325, loss_cls_H: 0.6325, loss_bbox_lane: 6.4600, loss_bbox_H: 6.4611, d0.loss_cls_lane: 0.6325, d0.loss_cls_H: 0.6325, d0.loss_bbox_lane: 6.4600, d0.loss_bbox_H: 6.4611, d1.loss_cls_lane: 0.6325, d1.loss_cls_H: 0.6325, d1.loss_bbox_lane: 6.4600, d1.loss_bbox_H: 6.4611, d2.loss_cls_lane: 0.6325, d2.loss_cls_H: 0.6325, d2.loss_bbox_lane: 6.4600, d2.loss_bbox_H: 6.4611, d3.loss_cls_lane: 0.6325, d3.loss_cls_H: 0.6325, d3.loss_bbox_lane: 6.4600, d3.loss_bbox_H: 6.4611, d4.loss_cls_lane: 0.6325, d4.loss_cls_H: 0.6325, d4.loss_bbox_lane: 6.4600, d4.loss_bbox_H: 6.4611, vlm_loss: 0.0000, loss: 129.5056, grad_norm: nan
2025-03-01 04:46:08,411 - mmdet - INFO - Iter [8300/168780]	lr: 3.976e-04, eta: 5 days, 14:08:29, time: 2.993, data_time: 0.017, memory: 22041, loss_cls: 1.5523, loss_bbox: 2.6710, d0.loss_cls: 1.5523, d0.loss_bbox: 2.6710, d1.loss_cls: 1.5523, d1.loss_bbox: 2.6710, d2.loss_cls: 1.5523, d2.loss_bbox: 2.6710, d3.loss_cls: 1.5523, d3.loss_bbox: 2.6710, d4.loss_cls: 1.5523, d4.loss_bbox: 2.6710, dn_loss_cls: 0.9340, dn_loss_bbox: 2.2646, d0.dn_loss_cls: 0.9340, d0.dn_loss_bbox: 2.2646, d1.dn_loss_cls: 0.9340, d1.dn_loss_bbox: 2.2646, d2.dn_loss_cls: 0.9340, d2.dn_loss_bbox: 2.2646, d3.dn_loss_cls: 0.9340, d3.dn_loss_bbox: 2.2646, d4.dn_loss_cls: 0.9340, d4.dn_loss_bbox: 2.2646, loss_cls_lane: 0.6361, loss_cls_H: 0.6361, loss_bbox_lane: 6.3486, loss_bbox_H: 6.3562, d0.loss_cls_lane: 0.6361, d0.loss_cls_H: 0.6361, d0.loss_bbox_lane: 6.3486, d0.loss_bbox_H: 6.3562, d1.loss_cls_lane: 0.6361, d1.loss_cls_H: 0.6361, d1.loss_bbox_lane: 6.3486, d1.loss_bbox_H: 6.3562, d2.loss_cls_lane: 0.6361, d2.loss_cls_H: 0.6361, d2.loss_bbox_lane: 6.3486, d2.loss_bbox_H: 6.3562, d3.loss_cls_lane: 0.6361, d3.loss_cls_H: 0.6361, d3.loss_bbox_lane: 6.3486, d3.loss_bbox_H: 6.3562, d4.loss_cls_lane: 0.6361, d4.loss_cls_H: 0.6361, d4.loss_bbox_lane: 6.3486, d4.loss_bbox_H: 6.3562, vlm_loss: 0.0000, loss: 128.3939, grad_norm: nan
2025-03-01 04:48:36,727 - mmdet - INFO - Iter [8350/168780]	lr: 3.976e-04, eta: 5 days, 14:05:17, time: 2.966, data_time: 0.017, memory: 22041, loss_cls: 1.6618, loss_bbox: 2.7371, d0.loss_cls: 1.6618, d0.loss_bbox: 2.7371, d1.loss_cls: 1.6618, d1.loss_bbox: 2.7371, d2.loss_cls: 1.6618, d2.loss_bbox: 2.7371, d3.loss_cls: 1.6618, d3.loss_bbox: 2.7371, d4.loss_cls: 1.6618, d4.loss_bbox: 2.7371, dn_loss_cls: 0.9007, dn_loss_bbox: 2.4396, d0.dn_loss_cls: 0.9007, d0.dn_loss_bbox: 2.4396, d1.dn_loss_cls: 0.9007, d1.dn_loss_bbox: 2.4396, d2.dn_loss_cls: 0.9007, d2.dn_loss_bbox: 2.4396, d3.dn_loss_cls: 0.9007, d3.dn_loss_bbox: 2.4396, d4.dn_loss_cls: 0.9007, d4.dn_loss_bbox: 2.4396, loss_cls_lane: 0.6808, loss_cls_H: 0.6808, loss_bbox_lane: 6.5694, loss_bbox_H: 6.5625, d0.loss_cls_lane: 0.6808, d0.loss_cls_H: 0.6808, d0.loss_bbox_lane: 6.5694, d0.loss_bbox_H: 6.5625, d1.loss_cls_lane: 0.6808, d1.loss_cls_H: 0.6808, d1.loss_bbox_lane: 6.5694, d1.loss_bbox_H: 6.5625, d2.loss_cls_lane: 0.6808, d2.loss_cls_H: 0.6808, d2.loss_bbox_lane: 6.5694, d2.loss_bbox_H: 6.5625, d3.loss_cls_lane: 0.6808, d3.loss_cls_H: 0.6808, d3.loss_bbox_lane: 6.5694, d3.loss_bbox_H: 6.5625, d4.loss_cls_lane: 0.6808, d4.loss_cls_H: 0.6808, d4.loss_bbox_lane: 6.5694, d4.loss_bbox_H: 6.5625, vlm_loss: 0.0000, loss: 133.3963, grad_norm: nan
2025-03-01 04:51:05,783 - mmdet - INFO - Iter [8400/168780]	lr: 3.976e-04, eta: 5 days, 14:02:20, time: 2.981, data_time: 0.017, memory: 22041, loss_cls: 1.7054, loss_bbox: 2.8603, d0.loss_cls: 1.7054, d0.loss_bbox: 2.8603, d1.loss_cls: 1.7054, d1.loss_bbox: 2.8603, d2.loss_cls: 1.7054, d2.loss_bbox: 2.8603, d3.loss_cls: 1.7054, d3.loss_bbox: 2.8603, d4.loss_cls: 1.7054, d4.loss_bbox: 2.8603, dn_loss_cls: 1.0426, dn_loss_bbox: 2.3989, d0.dn_loss_cls: 1.0426, d0.dn_loss_bbox: 2.3989, d1.dn_loss_cls: 1.0426, d1.dn_loss_bbox: 2.3989, d2.dn_loss_cls: 1.0426, d2.dn_loss_bbox: 2.3989, d3.dn_loss_cls: 1.0426, d3.dn_loss_bbox: 2.3989, d4.dn_loss_cls: 1.0426, d4.dn_loss_bbox: 2.3989, loss_cls_lane: 0.6346, loss_cls_H: 0.6346, loss_bbox_lane: 6.4738, loss_bbox_H: 6.4812, d0.loss_cls_lane: 0.6346, d0.loss_cls_H: 0.6346, d0.loss_bbox_lane: 6.4738, d0.loss_bbox_H: 6.4812, d1.loss_cls_lane: 0.6346, d1.loss_cls_H: 0.6346, d1.loss_bbox_lane: 6.4738, d1.loss_bbox_H: 6.4812, d2.loss_cls_lane: 0.6346, d2.loss_cls_H: 0.6346, d2.loss_bbox_lane: 6.4738, d2.loss_bbox_H: 6.4812, d3.loss_cls_lane: 0.6346, d3.loss_cls_H: 0.6346, d3.loss_bbox_lane: 6.4738, d3.loss_bbox_H: 6.4812, d4.loss_cls_lane: 0.6346, d4.loss_cls_H: 0.6346, d4.loss_bbox_lane: 6.4738, d4.loss_bbox_H: 6.4812, vlm_loss: 0.0000, loss: 133.3884, grad_norm: nan
2025-03-01 04:53:35,987 - mmdet - INFO - Iter [8450/168780]	lr: 3.975e-04, eta: 5 days, 13:59:45, time: 3.004, data_time: 0.018, memory: 22041, loss_cls: 1.3446, loss_bbox: 2.7439, d0.loss_cls: 1.3446, d0.loss_bbox: 2.7439, d1.loss_cls: 1.3446, d1.loss_bbox: 2.7439, d2.loss_cls: 1.3446, d2.loss_bbox: 2.7439, d3.loss_cls: 1.3446, d3.loss_bbox: 2.7439, d4.loss_cls: 1.3446, d4.loss_bbox: 2.7439, dn_loss_cls: 0.8992, dn_loss_bbox: 2.4403, d0.dn_loss_cls: 0.8992, d0.dn_loss_bbox: 2.4403, d1.dn_loss_cls: 0.8992, d1.dn_loss_bbox: 2.4403, d2.dn_loss_cls: 0.8992, d2.dn_loss_bbox: 2.4403, d3.dn_loss_cls: 0.8992, d3.dn_loss_bbox: 2.4403, d4.dn_loss_cls: 0.8992, d4.dn_loss_bbox: 2.4403, loss_cls_lane: 0.6283, loss_cls_H: 0.6283, loss_bbox_lane: 6.4133, loss_bbox_H: 6.4070, d0.loss_cls_lane: 0.6283, d0.loss_cls_H: 0.6283, d0.loss_bbox_lane: 6.4133, d0.loss_bbox_H: 6.4070, d1.loss_cls_lane: 0.6283, d1.loss_cls_H: 0.6283, d1.loss_bbox_lane: 6.4133, d1.loss_bbox_H: 6.4070, d2.loss_cls_lane: 0.6283, d2.loss_cls_H: 0.6283, d2.loss_bbox_lane: 6.4133, d2.loss_bbox_H: 6.4070, d3.loss_cls_lane: 0.6283, d3.loss_cls_H: 0.6283, d3.loss_bbox_lane: 6.4133, d3.loss_bbox_H: 6.4070, d4.loss_cls_lane: 0.6283, d4.loss_cls_H: 0.6283, d4.loss_bbox_lane: 6.4133, d4.loss_bbox_H: 6.4070, vlm_loss: 0.0000, loss: 129.0294, grad_norm: nan
2025-03-01 04:56:04,716 - mmdet - INFO - Iter [8500/168780]	lr: 3.975e-04, eta: 5 days, 13:56:43, time: 2.975, data_time: 0.017, memory: 22041, loss_cls: 1.5752, loss_bbox: 2.7933, d0.loss_cls: 1.5752, d0.loss_bbox: 2.7933, d1.loss_cls: 1.5752, d1.loss_bbox: 2.7933, d2.loss_cls: 1.5752, d2.loss_bbox: 2.7933, d3.loss_cls: 1.5752, d3.loss_bbox: 2.7933, d4.loss_cls: 1.5752, d4.loss_bbox: 2.7933, dn_loss_cls: 0.8551, dn_loss_bbox: 2.3823, d0.dn_loss_cls: 0.8551, d0.dn_loss_bbox: 2.3823, d1.dn_loss_cls: 0.8551, d1.dn_loss_bbox: 2.3823, d2.dn_loss_cls: 0.8551, d2.dn_loss_bbox: 2.3823, d3.dn_loss_cls: 0.8551, d3.dn_loss_bbox: 2.3823, d4.dn_loss_cls: 0.8551, d4.dn_loss_bbox: 2.3823, loss_cls_lane: 0.6448, loss_cls_H: 0.6448, loss_bbox_lane: 6.5286, loss_bbox_H: 6.5226, d0.loss_cls_lane: 0.6448, d0.loss_cls_H: 0.6448, d0.loss_bbox_lane: 6.5286, d0.loss_bbox_H: 6.5226, d1.loss_cls_lane: 0.6448, d1.loss_cls_H: 0.6448, d1.loss_bbox_lane: 6.5286, d1.loss_bbox_H: 6.5226, d2.loss_cls_lane: 0.6448, d2.loss_cls_H: 0.6448, d2.loss_bbox_lane: 6.5286, d2.loss_bbox_H: 6.5226, d3.loss_cls_lane: 0.6448, d3.loss_cls_H: 0.6448, d3.loss_bbox_lane: 6.5286, d3.loss_bbox_H: 6.5226, d4.loss_cls_lane: 0.6448, d4.loss_cls_H: 0.6448, d4.loss_bbox_lane: 6.5286, d4.loss_bbox_H: 6.5226, vlm_loss: 0.0000, loss: 131.6806, grad_norm: nan
2025-03-01 04:58:34,145 - mmdet - INFO - Iter [8550/168780]	lr: 3.975e-04, eta: 5 days, 13:53:54, time: 2.989, data_time: 0.018, memory: 22041, loss_cls: 1.7132, loss_bbox: 2.7430, d0.loss_cls: 1.7132, d0.loss_bbox: 2.7430, d1.loss_cls: 1.7132, d1.loss_bbox: 2.7430, d2.loss_cls: 1.7132, d2.loss_bbox: 2.7430, d3.loss_cls: 1.7132, d3.loss_bbox: 2.7430, d4.loss_cls: 1.7132, d4.loss_bbox: 2.7430, dn_loss_cls: 0.9876, dn_loss_bbox: 2.4153, d0.dn_loss_cls: 0.9876, d0.dn_loss_bbox: 2.4153, d1.dn_loss_cls: 0.9876, d1.dn_loss_bbox: 2.4153, d2.dn_loss_cls: 0.9876, d2.dn_loss_bbox: 2.4153, d3.dn_loss_cls: 0.9876, d3.dn_loss_bbox: 2.4153, d4.dn_loss_cls: 0.9876, d4.dn_loss_bbox: 2.4153, loss_cls_lane: 0.6306, loss_cls_H: 0.6306, loss_bbox_lane: 6.2972, loss_bbox_H: 6.2889, d0.loss_cls_lane: 0.6306, d0.loss_cls_H: 0.6306, d0.loss_bbox_lane: 6.2972, d0.loss_bbox_H: 6.2889, d1.loss_cls_lane: 0.6306, d1.loss_cls_H: 0.6306, d1.loss_bbox_lane: 6.2972, d1.loss_bbox_H: 6.2889, d2.loss_cls_lane: 0.6306, d2.loss_cls_H: 0.6306, d2.loss_bbox_lane: 6.2972, d2.loss_bbox_H: 6.2889, d3.loss_cls_lane: 0.6306, d3.loss_cls_H: 0.6306, d3.loss_bbox_lane: 6.2972, d3.loss_bbox_H: 6.2889, d4.loss_cls_lane: 0.6306, d4.loss_cls_H: 0.6306, d4.loss_bbox_lane: 6.2972, d4.loss_bbox_H: 6.2889, vlm_loss: 0.0000, loss: 130.2387, grad_norm: nan
2025-03-01 05:01:03,358 - mmdet - INFO - Iter [8600/168780]	lr: 3.974e-04, eta: 5 days, 13:51:01, time: 2.984, data_time: 0.017, memory: 22041, loss_cls: 1.6079, loss_bbox: 2.8114, d0.loss_cls: 1.6079, d0.loss_bbox: 2.8114, d1.loss_cls: 1.6079, d1.loss_bbox: 2.8114, d2.loss_cls: 1.6079, d2.loss_bbox: 2.8114, d3.loss_cls: 1.6079, d3.loss_bbox: 2.8114, d4.loss_cls: 1.6079, d4.loss_bbox: 2.8114, dn_loss_cls: 0.9207, dn_loss_bbox: 2.5081, d0.dn_loss_cls: 0.9207, d0.dn_loss_bbox: 2.5081, d1.dn_loss_cls: 0.9207, d1.dn_loss_bbox: 2.5081, d2.dn_loss_cls: 0.9207, d2.dn_loss_bbox: 2.5081, d3.dn_loss_cls: 0.9207, d3.dn_loss_bbox: 2.5081, d4.dn_loss_cls: 0.9207, d4.dn_loss_bbox: 2.5081, loss_cls_lane: 0.6649, loss_cls_H: 0.6649, loss_bbox_lane: 6.7357, loss_bbox_H: 6.7394, d0.loss_cls_lane: 0.6649, d0.loss_cls_H: 0.6649, d0.loss_bbox_lane: 6.7357, d0.loss_bbox_H: 6.7394, d1.loss_cls_lane: 0.6649, d1.loss_cls_H: 0.6649, d1.loss_bbox_lane: 6.7357, d1.loss_bbox_H: 6.7394, d2.loss_cls_lane: 0.6649, d2.loss_cls_H: 0.6649, d2.loss_bbox_lane: 6.7357, d2.loss_bbox_H: 6.7394, d3.loss_cls_lane: 0.6649, d3.loss_cls_H: 0.6649, d3.loss_bbox_lane: 6.7357, d3.loss_bbox_H: 6.7394, d4.loss_cls_lane: 0.6649, d4.loss_cls_H: 0.6649, d4.loss_bbox_lane: 6.7357, d4.loss_bbox_H: 6.7394, vlm_loss: 0.0000, loss: 135.9174, grad_norm: nan
2025-03-01 05:03:31,207 - mmdet - INFO - Iter [8650/168780]	lr: 3.974e-04, eta: 5 days, 13:47:43, time: 2.957, data_time: 0.017, memory: 22041, loss_cls: 1.9627, loss_bbox: 2.8126, d0.loss_cls: 1.9627, d0.loss_bbox: 2.8126, d1.loss_cls: 1.9627, d1.loss_bbox: 2.8126, d2.loss_cls: 1.9627, d2.loss_bbox: 2.8126, d3.loss_cls: 1.9627, d3.loss_bbox: 2.8126, d4.loss_cls: 1.9627, d4.loss_bbox: 2.8126, dn_loss_cls: 0.9266, dn_loss_bbox: 2.4894, d0.dn_loss_cls: 0.9266, d0.dn_loss_bbox: 2.4894, d1.dn_loss_cls: 0.9266, d1.dn_loss_bbox: 2.4894, d2.dn_loss_cls: 0.9266, d2.dn_loss_bbox: 2.4894, d3.dn_loss_cls: 0.9266, d3.dn_loss_bbox: 2.4894, d4.dn_loss_cls: 0.9266, d4.dn_loss_bbox: 2.4894, loss_cls_lane: 0.6505, loss_cls_H: 0.6505, loss_bbox_lane: 6.4223, loss_bbox_H: 6.4260, d0.loss_cls_lane: 0.6505, d0.loss_cls_H: 0.6505, d0.loss_bbox_lane: 6.4223, d0.loss_bbox_H: 6.4260, d1.loss_cls_lane: 0.6505, d1.loss_cls_H: 0.6505, d1.loss_bbox_lane: 6.4223, d1.loss_bbox_H: 6.4260, d2.loss_cls_lane: 0.6505, d2.loss_cls_H: 0.6505, d2.loss_bbox_lane: 6.4223, d2.loss_bbox_H: 6.4260, d3.loss_cls_lane: 0.6505, d3.loss_cls_H: 0.6505, d3.loss_bbox_lane: 6.4223, d3.loss_bbox_H: 6.4260, d4.loss_cls_lane: 0.6505, d4.loss_cls_H: 0.6505, d4.loss_bbox_lane: 6.4223, d4.loss_bbox_H: 6.4260, vlm_loss: 0.0000, loss: 134.0445, grad_norm: nan
2025-03-01 05:05:59,689 - mmdet - INFO - Iter [8700/168780]	lr: 3.974e-04, eta: 5 days, 13:44:37, time: 2.970, data_time: 0.017, memory: 22041, loss_cls: 2.0831, loss_bbox: 2.8536, d0.loss_cls: 2.0831, d0.loss_bbox: 2.8536, d1.loss_cls: 2.0831, d1.loss_bbox: 2.8536, d2.loss_cls: 2.0831, d2.loss_bbox: 2.8536, d3.loss_cls: 2.0831, d3.loss_bbox: 2.8536, d4.loss_cls: 2.0831, d4.loss_bbox: 2.8536, dn_loss_cls: 0.8752, dn_loss_bbox: 2.4869, d0.dn_loss_cls: 0.8752, d0.dn_loss_bbox: 2.4869, d1.dn_loss_cls: 0.8752, d1.dn_loss_bbox: 2.4869, d2.dn_loss_cls: 0.8752, d2.dn_loss_bbox: 2.4869, d3.dn_loss_cls: 0.8752, d3.dn_loss_bbox: 2.4869, d4.dn_loss_cls: 0.8752, d4.dn_loss_bbox: 2.4869, loss_cls_lane: 0.6668, loss_cls_H: 0.6668, loss_bbox_lane: 6.8384, loss_bbox_H: 6.8398, d0.loss_cls_lane: 0.6668, d0.loss_cls_H: 0.6668, d0.loss_bbox_lane: 6.8384, d0.loss_bbox_H: 6.8398, d1.loss_cls_lane: 0.6668, d1.loss_cls_H: 0.6668, d1.loss_bbox_lane: 6.8384, d1.loss_bbox_H: 6.8398, d2.loss_cls_lane: 0.6668, d2.loss_cls_H: 0.6668, d2.loss_bbox_lane: 6.8384, d2.loss_bbox_H: 6.8398, d3.loss_cls_lane: 0.6668, d3.loss_cls_H: 0.6668, d3.loss_bbox_lane: 6.8384, d3.loss_bbox_H: 6.8398, d4.loss_cls_lane: 0.6668, d4.loss_cls_H: 0.6668, d4.loss_bbox_lane: 6.8384, d4.loss_bbox_H: 6.8398, vlm_loss: 0.0000, loss: 139.8635, grad_norm: nan
2025-03-01 05:08:28,544 - mmdet - INFO - Iter [8750/168780]	lr: 3.974e-04, eta: 5 days, 13:41:39, time: 2.977, data_time: 0.017, memory: 22041, loss_cls: 1.3495, loss_bbox: 2.8144, d0.loss_cls: 1.3495, d0.loss_bbox: 2.8144, d1.loss_cls: 1.3495, d1.loss_bbox: 2.8144, d2.loss_cls: 1.3495, d2.loss_bbox: 2.8144, d3.loss_cls: 1.3495, d3.loss_bbox: 2.8144, d4.loss_cls: 1.3495, d4.loss_bbox: 2.8144, dn_loss_cls: 0.9121, dn_loss_bbox: 2.5049, d0.dn_loss_cls: 0.9121, d0.dn_loss_bbox: 2.5049, d1.dn_loss_cls: 0.9121, d1.dn_loss_bbox: 2.5049, d2.dn_loss_cls: 0.9121, d2.dn_loss_bbox: 2.5049, d3.dn_loss_cls: 0.9121, d3.dn_loss_bbox: 2.5049, d4.dn_loss_cls: 0.9121, d4.dn_loss_bbox: 2.5049, loss_cls_lane: 0.6791, loss_cls_H: 0.6791, loss_bbox_lane: 7.7855, loss_bbox_H: 7.7694, d0.loss_cls_lane: 0.6791, d0.loss_cls_H: 0.6791, d0.loss_bbox_lane: 7.7855, d0.loss_bbox_H: 7.7694, d1.loss_cls_lane: 0.6791, d1.loss_cls_H: 0.6791, d1.loss_bbox_lane: 7.7855, d1.loss_bbox_H: 7.7694, d2.loss_cls_lane: 0.6791, d2.loss_cls_H: 0.6791, d2.loss_bbox_lane: 7.7855, d2.loss_bbox_H: 7.7694, d3.loss_cls_lane: 0.6791, d3.loss_cls_H: 0.6791, d3.loss_bbox_lane: 7.7855, d3.loss_bbox_H: 7.7694, d4.loss_cls_lane: 0.6791, d4.loss_cls_H: 0.6791, d4.loss_bbox_lane: 7.7855, d4.loss_bbox_H: 7.7694, vlm_loss: 0.0000, loss: 146.9648, grad_norm: nan
2025-03-01 05:10:57,008 - mmdet - INFO - Iter [8800/168780]	lr: 3.973e-04, eta: 5 days, 13:38:34, time: 2.969, data_time: 0.017, memory: 22041, loss_cls: 1.2260, loss_bbox: 2.5791, d0.loss_cls: 1.2260, d0.loss_bbox: 2.5791, d1.loss_cls: 1.2260, d1.loss_bbox: 2.5791, d2.loss_cls: 1.2260, d2.loss_bbox: 2.5791, d3.loss_cls: 1.2260, d3.loss_bbox: 2.5791, d4.loss_cls: 1.2260, d4.loss_bbox: 2.5791, dn_loss_cls: 0.9059, dn_loss_bbox: 2.3310, d0.dn_loss_cls: 0.9059, d0.dn_loss_bbox: 2.3310, d1.dn_loss_cls: 0.9059, d1.dn_loss_bbox: 2.3310, d2.dn_loss_cls: 0.9059, d2.dn_loss_bbox: 2.3310, d3.dn_loss_cls: 0.9059, d3.dn_loss_bbox: 2.3310, d4.dn_loss_cls: 0.9059, d4.dn_loss_bbox: 2.3310, loss_cls_lane: 0.6325, loss_cls_H: 0.6325, loss_bbox_lane: 6.2433, loss_bbox_H: 6.2503, d0.loss_cls_lane: 0.6325, d0.loss_cls_H: 0.6325, d0.loss_bbox_lane: 6.2433, d0.loss_bbox_H: 6.2503, d1.loss_cls_lane: 0.6325, d1.loss_cls_H: 0.6325, d1.loss_bbox_lane: 6.2433, d1.loss_bbox_H: 6.2503, d2.loss_cls_lane: 0.6325, d2.loss_cls_H: 0.6325, d2.loss_bbox_lane: 6.2433, d2.loss_bbox_H: 6.2503, d3.loss_cls_lane: 0.6325, d3.loss_cls_H: 0.6325, d3.loss_bbox_lane: 6.2433, d3.loss_bbox_H: 6.2503, d4.loss_cls_lane: 0.6325, d4.loss_cls_H: 0.6325, d4.loss_bbox_lane: 6.2433, d4.loss_bbox_H: 6.2503, vlm_loss: 0.0000, loss: 124.8040, grad_norm: nan
2025-03-01 05:13:26,409 - mmdet - INFO - Iter [8850/168780]	lr: 3.973e-04, eta: 5 days, 13:35:46, time: 2.988, data_time: 0.017, memory: 22041, loss_cls: 1.4036, loss_bbox: 2.7359, d0.loss_cls: 1.4036, d0.loss_bbox: 2.7359, d1.loss_cls: 1.4036, d1.loss_bbox: 2.7359, d2.loss_cls: 1.4036, d2.loss_bbox: 2.7359, d3.loss_cls: 1.4036, d3.loss_bbox: 2.7359, d4.loss_cls: 1.4036, d4.loss_bbox: 2.7359, dn_loss_cls: 0.8694, dn_loss_bbox: 2.3600, d0.dn_loss_cls: 0.8694, d0.dn_loss_bbox: 2.3600, d1.dn_loss_cls: 0.8694, d1.dn_loss_bbox: 2.3600, d2.dn_loss_cls: 0.8694, d2.dn_loss_bbox: 2.3600, d3.dn_loss_cls: 0.8694, d3.dn_loss_bbox: 2.3600, d4.dn_loss_cls: 0.8694, d4.dn_loss_bbox: 2.3600, loss_cls_lane: 0.6423, loss_cls_H: 0.6423, loss_bbox_lane: 6.5458, loss_bbox_H: 6.5371, d0.loss_cls_lane: 0.6423, d0.loss_cls_H: 0.6423, d0.loss_bbox_lane: 6.5458, d0.loss_bbox_H: 6.5371, d1.loss_cls_lane: 0.6423, d1.loss_cls_H: 0.6423, d1.loss_bbox_lane: 6.5458, d1.loss_bbox_H: 6.5371, d2.loss_cls_lane: 0.6423, d2.loss_cls_H: 0.6423, d2.loss_bbox_lane: 6.5458, d2.loss_bbox_H: 6.5371, d3.loss_cls_lane: 0.6423, d3.loss_cls_H: 0.6423, d3.loss_bbox_lane: 6.5458, d3.loss_bbox_H: 6.5371, d4.loss_cls_lane: 0.6423, d4.loss_cls_H: 0.6423, d4.loss_bbox_lane: 6.5458, d4.loss_bbox_H: 6.5371, vlm_loss: 0.0000, loss: 130.4189, grad_norm: nan
2025-03-01 05:15:56,150 - mmdet - INFO - Iter [8900/168780]	lr: 3.973e-04, eta: 5 days, 13:33:04, time: 2.995, data_time: 0.017, memory: 22041, loss_cls: 1.3512, loss_bbox: 2.7163, d0.loss_cls: 1.3512, d0.loss_bbox: 2.7163, d1.loss_cls: 1.3512, d1.loss_bbox: 2.7163, d2.loss_cls: 1.3512, d2.loss_bbox: 2.7163, d3.loss_cls: 1.3512, d3.loss_bbox: 2.7163, d4.loss_cls: 1.3512, d4.loss_bbox: 2.7163, dn_loss_cls: 0.9133, dn_loss_bbox: 2.3480, d0.dn_loss_cls: 0.9133, d0.dn_loss_bbox: 2.3480, d1.dn_loss_cls: 0.9133, d1.dn_loss_bbox: 2.3480, d2.dn_loss_cls: 0.9133, d2.dn_loss_bbox: 2.3480, d3.dn_loss_cls: 0.9133, d3.dn_loss_bbox: 2.3480, d4.dn_loss_cls: 0.9133, d4.dn_loss_bbox: 2.3480, loss_cls_lane: 0.6332, loss_cls_H: 0.6332, loss_bbox_lane: 6.6856, loss_bbox_H: 6.6735, d0.loss_cls_lane: 0.6332, d0.loss_cls_H: 0.6332, d0.loss_bbox_lane: 6.6856, d0.loss_bbox_H: 6.6735, d1.loss_cls_lane: 0.6332, d1.loss_cls_H: 0.6332, d1.loss_bbox_lane: 6.6856, d1.loss_bbox_H: 6.6735, d2.loss_cls_lane: 0.6332, d2.loss_cls_H: 0.6332, d2.loss_bbox_lane: 6.6856, d2.loss_bbox_H: 6.6735, d3.loss_cls_lane: 0.6332, d3.loss_cls_H: 0.6332, d3.loss_bbox_lane: 6.6856, d3.loss_bbox_H: 6.6735, d4.loss_cls_lane: 0.6332, d4.loss_cls_H: 0.6332, d4.loss_bbox_lane: 6.6856, d4.loss_bbox_H: 6.6735, vlm_loss: 0.0000, loss: 131.7265, grad_norm: nan
2025-03-01 05:18:25,940 - mmdet - INFO - Iter [8950/168780]	lr: 3.972e-04, eta: 5 days, 13:30:24, time: 2.996, data_time: 0.016, memory: 22041, loss_cls: 1.6251, loss_bbox: 2.7383, d0.loss_cls: 1.6251, d0.loss_bbox: 2.7383, d1.loss_cls: 1.6251, d1.loss_bbox: 2.7383, d2.loss_cls: 1.6251, d2.loss_bbox: 2.7383, d3.loss_cls: 1.6251, d3.loss_bbox: 2.7383, d4.loss_cls: 1.6251, d4.loss_bbox: 2.7383, dn_loss_cls: 0.9411, dn_loss_bbox: 2.2590, d0.dn_loss_cls: 0.9411, d0.dn_loss_bbox: 2.2590, d1.dn_loss_cls: 0.9411, d1.dn_loss_bbox: 2.2590, d2.dn_loss_cls: 0.9411, d2.dn_loss_bbox: 2.2590, d3.dn_loss_cls: 0.9411, d3.dn_loss_bbox: 2.2590, d4.dn_loss_cls: 0.9411, d4.dn_loss_bbox: 2.2590, loss_cls_lane: 0.6394, loss_cls_H: 0.6394, loss_bbox_lane: 6.5055, loss_bbox_H: 6.4894, d0.loss_cls_lane: 0.6394, d0.loss_cls_H: 0.6394, d0.loss_bbox_lane: 6.5055, d0.loss_bbox_H: 6.4894, d1.loss_cls_lane: 0.6394, d1.loss_cls_H: 0.6394, d1.loss_bbox_lane: 6.5055, d1.loss_bbox_H: 6.4894, d2.loss_cls_lane: 0.6394, d2.loss_cls_H: 0.6394, d2.loss_bbox_lane: 6.5055, d2.loss_bbox_H: 6.4894, d3.loss_cls_lane: 0.6394, d3.loss_cls_H: 0.6394, d3.loss_bbox_lane: 6.5055, d3.loss_bbox_H: 6.4894, d4.loss_cls_lane: 0.6394, d4.loss_cls_H: 0.6394, d4.loss_bbox_lane: 6.5055, d4.loss_bbox_H: 6.4894, vlm_loss: 0.0000, loss: 131.0230, grad_norm: nan
2025-03-01 05:21:21,632 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 05:21:21,632 - mmdet - INFO - Iter [9000/168780]	lr: 3.972e-04, eta: 5 days, 13:35:23, time: 3.514, data_time: 0.018, memory: 22041, loss_cls: 1.3720, loss_bbox: 2.7106, d0.loss_cls: 1.3720, d0.loss_bbox: 2.7106, d1.loss_cls: 1.3720, d1.loss_bbox: 2.7106, d2.loss_cls: 1.3720, d2.loss_bbox: 2.7106, d3.loss_cls: 1.3720, d3.loss_bbox: 2.7106, d4.loss_cls: 1.3720, d4.loss_bbox: 2.7106, dn_loss_cls: 0.9308, dn_loss_bbox: 2.3539, d0.dn_loss_cls: 0.9308, d0.dn_loss_bbox: 2.3539, d1.dn_loss_cls: 0.9308, d1.dn_loss_bbox: 2.3539, d2.dn_loss_cls: 0.9308, d2.dn_loss_bbox: 2.3539, d3.dn_loss_cls: 0.9308, d3.dn_loss_bbox: 2.3539, d4.dn_loss_cls: 0.9308, d4.dn_loss_bbox: 2.3539, loss_cls_lane: 0.6290, loss_cls_H: 0.6290, loss_bbox_lane: 6.4218, loss_bbox_H: 6.4170, d0.loss_cls_lane: 0.6290, d0.loss_cls_H: 0.6290, d0.loss_bbox_lane: 6.4218, d0.loss_bbox_H: 6.4170, d1.loss_cls_lane: 0.6290, d1.loss_cls_H: 0.6290, d1.loss_bbox_lane: 6.4218, d1.loss_bbox_H: 6.4170, d2.loss_cls_lane: 0.6290, d2.loss_cls_H: 0.6290, d2.loss_bbox_lane: 6.4218, d2.loss_bbox_H: 6.4170, d3.loss_cls_lane: 0.6290, d3.loss_cls_H: 0.6290, d3.loss_bbox_lane: 6.4218, d3.loss_bbox_H: 6.4170, d4.loss_cls_lane: 0.6290, d4.loss_cls_H: 0.6290, d4.loss_bbox_lane: 6.4218, d4.loss_bbox_H: 6.4170, vlm_loss: 0.0000, loss: 128.7850, grad_norm: nan
2025-03-01 05:23:51,037 - mmdet - INFO - Iter [9050/168780]	lr: 3.972e-04, eta: 5 days, 13:32:33, time: 2.988, data_time: 0.018, memory: 22041, loss_cls: 1.3743, loss_bbox: 2.5963, d0.loss_cls: 1.3743, d0.loss_bbox: 2.5963, d1.loss_cls: 1.3743, d1.loss_bbox: 2.5963, d2.loss_cls: 1.3743, d2.loss_bbox: 2.5963, d3.loss_cls: 1.3743, d3.loss_bbox: 2.5963, d4.loss_cls: 1.3743, d4.loss_bbox: 2.5963, dn_loss_cls: 0.8981, dn_loss_bbox: 2.2516, d0.dn_loss_cls: 0.8981, d0.dn_loss_bbox: 2.2516, d1.dn_loss_cls: 0.8981, d1.dn_loss_bbox: 2.2516, d2.dn_loss_cls: 0.8981, d2.dn_loss_bbox: 2.2516, d3.dn_loss_cls: 0.8981, d3.dn_loss_bbox: 2.2516, d4.dn_loss_cls: 0.8981, d4.dn_loss_bbox: 2.2516, loss_cls_lane: 0.6280, loss_cls_H: 0.6280, loss_bbox_lane: 6.3654, loss_bbox_H: 6.3572, d0.loss_cls_lane: 0.6280, d0.loss_cls_H: 0.6280, d0.loss_bbox_lane: 6.3654, d0.loss_bbox_H: 6.3572, d1.loss_cls_lane: 0.6280, d1.loss_cls_H: 0.6280, d1.loss_bbox_lane: 6.3654, d1.loss_bbox_H: 6.3572, d2.loss_cls_lane: 0.6280, d2.loss_cls_H: 0.6280, d2.loss_bbox_lane: 6.3654, d2.loss_bbox_H: 6.3572, d3.loss_cls_lane: 0.6280, d3.loss_cls_H: 0.6280, d3.loss_bbox_lane: 6.3654, d3.loss_bbox_H: 6.3572, d4.loss_cls_lane: 0.6280, d4.loss_cls_H: 0.6280, d4.loss_bbox_lane: 6.3654, d4.loss_bbox_H: 6.3572, vlm_loss: 0.0000, loss: 126.5939, grad_norm: nan
2025-03-01 05:26:20,709 - mmdet - INFO - Iter [9100/168780]	lr: 3.971e-04, eta: 5 days, 13:29:49, time: 2.993, data_time: 0.018, memory: 22041, loss_cls: 1.3933, loss_bbox: 2.5941, d0.loss_cls: 1.3933, d0.loss_bbox: 2.5941, d1.loss_cls: 1.3933, d1.loss_bbox: 2.5941, d2.loss_cls: 1.3933, d2.loss_bbox: 2.5941, d3.loss_cls: 1.3933, d3.loss_bbox: 2.5941, d4.loss_cls: 1.3933, d4.loss_bbox: 2.5941, dn_loss_cls: 0.9676, dn_loss_bbox: 2.2742, d0.dn_loss_cls: 0.9676, d0.dn_loss_bbox: 2.2742, d1.dn_loss_cls: 0.9676, d1.dn_loss_bbox: 2.2742, d2.dn_loss_cls: 0.9676, d2.dn_loss_bbox: 2.2742, d3.dn_loss_cls: 0.9676, d3.dn_loss_bbox: 2.2742, d4.dn_loss_cls: 0.9676, d4.dn_loss_bbox: 2.2742, loss_cls_lane: 0.6388, loss_cls_H: 0.6388, loss_bbox_lane: 6.5350, loss_bbox_H: 6.5444, d0.loss_cls_lane: 0.6388, d0.loss_cls_H: 0.6388, d0.loss_bbox_lane: 6.5350, d0.loss_bbox_H: 6.5444, d1.loss_cls_lane: 0.6388, d1.loss_cls_H: 0.6388, d1.loss_bbox_lane: 6.5350, d1.loss_bbox_H: 6.5444, d2.loss_cls_lane: 0.6388, d2.loss_cls_H: 0.6388, d2.loss_bbox_lane: 6.5350, d2.loss_bbox_H: 6.5444, d3.loss_cls_lane: 0.6388, d3.loss_cls_H: 0.6388, d3.loss_bbox_lane: 6.5350, d3.loss_bbox_H: 6.5444, d4.loss_cls_lane: 0.6388, d4.loss_cls_H: 0.6388, d4.loss_bbox_lane: 6.5350, d4.loss_bbox_H: 6.5444, vlm_loss: 0.0000, loss: 129.5182, grad_norm: nan
2025-03-01 05:28:48,580 - mmdet - INFO - Iter [9150/168780]	lr: 3.971e-04, eta: 5 days, 13:26:33, time: 2.957, data_time: 0.018, memory: 22041, loss_cls: 1.3909, loss_bbox: 2.8454, d0.loss_cls: 1.3909, d0.loss_bbox: 2.8454, d1.loss_cls: 1.3909, d1.loss_bbox: 2.8454, d2.loss_cls: 1.3909, d2.loss_bbox: 2.8454, d3.loss_cls: 1.3909, d3.loss_bbox: 2.8454, d4.loss_cls: 1.3909, d4.loss_bbox: 2.8454, dn_loss_cls: 0.9848, dn_loss_bbox: 2.5094, d0.dn_loss_cls: 0.9848, d0.dn_loss_bbox: 2.5094, d1.dn_loss_cls: 0.9848, d1.dn_loss_bbox: 2.5094, d2.dn_loss_cls: 0.9848, d2.dn_loss_bbox: 2.5094, d3.dn_loss_cls: 0.9848, d3.dn_loss_bbox: 2.5094, d4.dn_loss_cls: 0.9848, d4.dn_loss_bbox: 2.5094, loss_cls_lane: 0.6285, loss_cls_H: 0.7360, loss_bbox_lane: 6.1939, loss_bbox_H: 6.1917, d0.loss_cls_lane: 0.6285, d0.loss_cls_H: 0.7360, d0.loss_bbox_lane: 6.1939, d0.loss_bbox_H: 6.1917, d1.loss_cls_lane: 0.6285, d1.loss_cls_H: 0.7360, d1.loss_bbox_lane: 6.1939, d1.loss_bbox_H: 6.1917, d2.loss_cls_lane: 0.6285, d2.loss_cls_H: 0.7360, d2.loss_bbox_lane: 6.1939, d2.loss_bbox_H: 6.1917, d3.loss_cls_lane: 0.6285, d3.loss_cls_H: 0.7360, d3.loss_bbox_lane: 6.1939, d3.loss_bbox_H: 6.1917, d4.loss_cls_lane: 0.6285, d4.loss_cls_H: 0.7360, d4.loss_bbox_lane: 6.1939, d4.loss_bbox_H: 6.1917, vlm_loss: 0.0000, loss: 128.8832, grad_norm: nan
2025-03-01 05:31:17,804 - mmdet - INFO - Iter [9200/168780]	lr: 3.971e-04, eta: 5 days, 13:23:40, time: 2.985, data_time: 0.018, memory: 22041, loss_cls: 1.7031, loss_bbox: 2.8776, d0.loss_cls: 1.7031, d0.loss_bbox: 2.8776, d1.loss_cls: 1.7031, d1.loss_bbox: 2.8776, d2.loss_cls: 1.7031, d2.loss_bbox: 2.8776, d3.loss_cls: 1.7031, d3.loss_bbox: 2.8776, d4.loss_cls: 1.7031, d4.loss_bbox: 2.8776, dn_loss_cls: 0.9919, dn_loss_bbox: 2.4812, d0.dn_loss_cls: 0.9919, d0.dn_loss_bbox: 2.4812, d1.dn_loss_cls: 0.9919, d1.dn_loss_bbox: 2.4812, d2.dn_loss_cls: 0.9919, d2.dn_loss_bbox: 2.4812, d3.dn_loss_cls: 0.9919, d3.dn_loss_bbox: 2.4812, d4.dn_loss_cls: 0.9919, d4.dn_loss_bbox: 2.4812, loss_cls_lane: 0.6338, loss_cls_H: 0.6338, loss_bbox_lane: 6.4290, loss_bbox_H: 6.4339, d0.loss_cls_lane: 0.6338, d0.loss_cls_H: 0.6338, d0.loss_bbox_lane: 6.4290, d0.loss_bbox_H: 6.4339, d1.loss_cls_lane: 0.6338, d1.loss_cls_H: 0.6338, d1.loss_bbox_lane: 6.4290, d1.loss_bbox_H: 6.4339, d2.loss_cls_lane: 0.6338, d2.loss_cls_H: 0.6338, d2.loss_bbox_lane: 6.4290, d2.loss_bbox_H: 6.4339, d3.loss_cls_lane: 0.6338, d3.loss_cls_H: 0.6338, d3.loss_bbox_lane: 6.4290, d3.loss_bbox_H: 6.4339, d4.loss_cls_lane: 0.6338, d4.loss_cls_H: 0.6338, d4.loss_bbox_lane: 6.4290, d4.loss_bbox_H: 6.4339, vlm_loss: 0.0000, loss: 133.1056, grad_norm: nan
2025-03-01 05:33:48,308 - mmdet - INFO - Iter [9250/168780]	lr: 3.970e-04, eta: 5 days, 13:21:11, time: 3.010, data_time: 0.017, memory: 22041, loss_cls: 1.5564, loss_bbox: 3.1835, d0.loss_cls: 1.5564, d0.loss_bbox: 3.1835, d1.loss_cls: 1.5564, d1.loss_bbox: 3.1835, d2.loss_cls: 1.5564, d2.loss_bbox: 3.1835, d3.loss_cls: 1.5564, d3.loss_bbox: 3.1835, d4.loss_cls: 1.5564, d4.loss_bbox: 3.1835, dn_loss_cls: 0.9577, dn_loss_bbox: 2.7525, d0.dn_loss_cls: 0.9577, d0.dn_loss_bbox: 2.7525, d1.dn_loss_cls: 0.9577, d1.dn_loss_bbox: 2.7525, d2.dn_loss_cls: 0.9577, d2.dn_loss_bbox: 2.7525, d3.dn_loss_cls: 0.9577, d3.dn_loss_bbox: 2.7525, d4.dn_loss_cls: 0.9577, d4.dn_loss_bbox: 2.7525, loss_cls_lane: 0.6422, loss_cls_H: 0.6422, loss_bbox_lane: 6.3891, loss_bbox_H: 6.4116, d0.loss_cls_lane: 0.6422, d0.loss_cls_H: 0.6422, d0.loss_bbox_lane: 6.3891, d0.loss_bbox_H: 6.4116, d1.loss_cls_lane: 0.6422, d1.loss_cls_H: 0.6422, d1.loss_bbox_lane: 6.3891, d1.loss_bbox_H: 6.4116, d2.loss_cls_lane: 0.6422, d2.loss_cls_H: 0.6422, d2.loss_bbox_lane: 6.3891, d2.loss_bbox_H: 6.4116, d3.loss_cls_lane: 0.6422, d3.loss_cls_H: 0.6422, d3.loss_bbox_lane: 6.3891, d3.loss_bbox_H: 6.4116, d4.loss_cls_lane: 0.6422, d4.loss_cls_H: 0.6422, d4.loss_bbox_lane: 6.3891, d4.loss_bbox_H: 6.4116, vlm_loss: 0.0000, loss: 135.2116, grad_norm: nan
2025-03-01 05:36:16,941 - mmdet - INFO - Iter [9300/168780]	lr: 3.970e-04, eta: 5 days, 13:18:09, time: 2.973, data_time: 0.017, memory: 22041, loss_cls: 1.6765, loss_bbox: 2.7505, d0.loss_cls: 1.6765, d0.loss_bbox: 2.7505, d1.loss_cls: 1.6765, d1.loss_bbox: 2.7505, d2.loss_cls: 1.6765, d2.loss_bbox: 2.7505, d3.loss_cls: 1.6765, d3.loss_bbox: 2.7505, d4.loss_cls: 1.6765, d4.loss_bbox: 2.7505, dn_loss_cls: 0.9192, dn_loss_bbox: 2.3976, d0.dn_loss_cls: 0.9192, d0.dn_loss_bbox: 2.3976, d1.dn_loss_cls: 0.9192, d1.dn_loss_bbox: 2.3976, d2.dn_loss_cls: 0.9192, d2.dn_loss_bbox: 2.3976, d3.dn_loss_cls: 0.9192, d3.dn_loss_bbox: 2.3976, d4.dn_loss_cls: 0.9192, d4.dn_loss_bbox: 2.3976, loss_cls_lane: 0.6296, loss_cls_H: 0.6296, loss_bbox_lane: 6.2885, loss_bbox_H: 6.2840, d0.loss_cls_lane: 0.6296, d0.loss_cls_H: 0.6296, d0.loss_bbox_lane: 6.2885, d0.loss_bbox_H: 6.2840, d1.loss_cls_lane: 0.6296, d1.loss_cls_H: 0.6296, d1.loss_bbox_lane: 6.2885, d1.loss_bbox_H: 6.2840, d2.loss_cls_lane: 0.6296, d2.loss_cls_H: 0.6296, d2.loss_bbox_lane: 6.2885, d2.loss_bbox_H: 6.2840, d3.loss_cls_lane: 0.6296, d3.loss_cls_H: 0.6296, d3.loss_bbox_lane: 6.2885, d3.loss_bbox_H: 6.2840, d4.loss_cls_lane: 0.6296, d4.loss_cls_H: 0.6296, d4.loss_bbox_lane: 6.2885, d4.loss_bbox_H: 6.2840, vlm_loss: 0.0000, loss: 129.4532, grad_norm: nan
2025-03-01 05:38:47,149 - mmdet - INFO - Iter [9350/168780]	lr: 3.970e-04, eta: 5 days, 13:15:34, time: 3.004, data_time: 0.016, memory: 22041, loss_cls: 2.1268, loss_bbox: 2.7270, d0.loss_cls: 2.1268, d0.loss_bbox: 2.7270, d1.loss_cls: 2.1268, d1.loss_bbox: 2.7270, d2.loss_cls: 2.1268, d2.loss_bbox: 2.7270, d3.loss_cls: 2.1268, d3.loss_bbox: 2.7270, d4.loss_cls: 2.1268, d4.loss_bbox: 2.7270, dn_loss_cls: 1.0289, dn_loss_bbox: 2.5129, d0.dn_loss_cls: 1.0289, d0.dn_loss_bbox: 2.5129, d1.dn_loss_cls: 1.0289, d1.dn_loss_bbox: 2.5129, d2.dn_loss_cls: 1.0289, d2.dn_loss_bbox: 2.5129, d3.dn_loss_cls: 1.0289, d3.dn_loss_bbox: 2.5129, d4.dn_loss_cls: 1.0289, d4.dn_loss_bbox: 2.5129, loss_cls_lane: 0.6305, loss_cls_H: 0.6305, loss_bbox_lane: 6.3868, loss_bbox_H: 6.3862, d0.loss_cls_lane: 0.6305, d0.loss_cls_H: 0.6305, d0.loss_bbox_lane: 6.3868, d0.loss_bbox_H: 6.3862, d1.loss_cls_lane: 0.6305, d1.loss_cls_H: 0.6305, d1.loss_bbox_lane: 6.3868, d1.loss_bbox_H: 6.3862, d2.loss_cls_lane: 0.6305, d2.loss_cls_H: 0.6305, d2.loss_bbox_lane: 6.3868, d2.loss_bbox_H: 6.3862, d3.loss_cls_lane: 0.6305, d3.loss_cls_H: 0.6305, d3.loss_bbox_lane: 6.3868, d3.loss_bbox_H: 6.3862, d4.loss_cls_lane: 0.6305, d4.loss_cls_H: 0.6305, d4.loss_bbox_lane: 6.3868, d4.loss_bbox_H: 6.3862, vlm_loss: 0.0000, loss: 134.5770, grad_norm: nan
2025-03-01 05:41:12,099 - mmdet - INFO - Iter [9400/168780]	lr: 3.970e-04, eta: 5 days, 13:11:30, time: 2.899, data_time: 0.018, memory: 22041, loss_cls: 1.7308, loss_bbox: 3.0843, d0.loss_cls: 1.7308, d0.loss_bbox: 3.0843, d1.loss_cls: 1.7308, d1.loss_bbox: 3.0843, d2.loss_cls: 1.7308, d2.loss_bbox: 3.0843, d3.loss_cls: 1.7308, d3.loss_bbox: 3.0843, d4.loss_cls: 1.7308, d4.loss_bbox: 3.0843, dn_loss_cls: 0.9778, dn_loss_bbox: 2.7867, d0.dn_loss_cls: 0.9778, d0.dn_loss_bbox: 2.7867, d1.dn_loss_cls: 0.9778, d1.dn_loss_bbox: 2.7867, d2.dn_loss_cls: 0.9778, d2.dn_loss_bbox: 2.7867, d3.dn_loss_cls: 0.9778, d3.dn_loss_bbox: 2.7867, d4.dn_loss_cls: 0.9778, d4.dn_loss_bbox: 2.7867, loss_cls_lane: 0.6357, loss_cls_H: 0.9582, loss_bbox_lane: 5.6583, loss_bbox_H: 5.6579, d0.loss_cls_lane: 0.6357, d0.loss_cls_H: 0.9582, d0.loss_bbox_lane: 5.6583, d0.loss_bbox_H: 5.6579, d1.loss_cls_lane: 0.6357, d1.loss_cls_H: 0.9582, d1.loss_bbox_lane: 5.6583, d1.loss_bbox_H: 5.6579, d2.loss_cls_lane: 0.6357, d2.loss_cls_H: 0.9582, d2.loss_bbox_lane: 5.6583, d2.loss_bbox_H: 5.6579, d3.loss_cls_lane: 0.6357, d3.loss_cls_H: 0.9582, d3.loss_bbox_lane: 5.6583, d3.loss_bbox_H: 5.6579, d4.loss_cls_lane: 0.6357, d4.loss_cls_H: 0.9582, d4.loss_bbox_lane: 5.6583, d4.loss_bbox_H: 5.6579, vlm_loss: 0.0000, loss: 128.9382, grad_norm: nan
2025-03-01 05:43:41,743 - mmdet - INFO - Iter [9450/168780]	lr: 3.969e-04, eta: 5 days, 13:08:47, time: 2.993, data_time: 0.018, memory: 22041, loss_cls: 1.3461, loss_bbox: 2.7690, d0.loss_cls: 1.3461, d0.loss_bbox: 2.7690, d1.loss_cls: 1.3461, d1.loss_bbox: 2.7690, d2.loss_cls: 1.3461, d2.loss_bbox: 2.7690, d3.loss_cls: 1.3461, d3.loss_bbox: 2.7690, d4.loss_cls: 1.3461, d4.loss_bbox: 2.7690, dn_loss_cls: 0.9576, dn_loss_bbox: 2.4279, d0.dn_loss_cls: 0.9576, d0.dn_loss_bbox: 2.4279, d1.dn_loss_cls: 0.9576, d1.dn_loss_bbox: 2.4279, d2.dn_loss_cls: 0.9576, d2.dn_loss_bbox: 2.4279, d3.dn_loss_cls: 0.9576, d3.dn_loss_bbox: 2.4279, d4.dn_loss_cls: 0.9576, d4.dn_loss_bbox: 2.4279, loss_cls_lane: 0.6263, loss_cls_H: 0.6263, loss_bbox_lane: 6.3951, loss_bbox_H: 6.4046, d0.loss_cls_lane: 0.6263, d0.loss_cls_H: 0.6263, d0.loss_bbox_lane: 6.3951, d0.loss_bbox_H: 6.4046, d1.loss_cls_lane: 0.6263, d1.loss_cls_H: 0.6263, d1.loss_bbox_lane: 6.3951, d1.loss_bbox_H: 6.4046, d2.loss_cls_lane: 0.6263, d2.loss_cls_H: 0.6263, d2.loss_bbox_lane: 6.3951, d2.loss_bbox_H: 6.4046, d3.loss_cls_lane: 0.6263, d3.loss_cls_H: 0.6263, d3.loss_bbox_lane: 6.3951, d3.loss_bbox_H: 6.4046, d4.loss_cls_lane: 0.6263, d4.loss_cls_H: 0.6263, d4.loss_bbox_lane: 6.3951, d4.loss_bbox_H: 6.4046, vlm_loss: 0.0000, loss: 129.3174, grad_norm: nan
2025-03-01 05:46:10,520 - mmdet - INFO - Iter [9500/168780]	lr: 3.969e-04, eta: 5 days, 13:05:49, time: 2.976, data_time: 0.017, memory: 22041, loss_cls: 1.7277, loss_bbox: 3.1528, d0.loss_cls: 1.7277, d0.loss_bbox: 3.1528, d1.loss_cls: 1.7277, d1.loss_bbox: 3.1528, d2.loss_cls: 1.7277, d2.loss_bbox: 3.1528, d3.loss_cls: 1.7277, d3.loss_bbox: 3.1528, d4.loss_cls: 1.7277, d4.loss_bbox: 3.1528, dn_loss_cls: 0.9206, dn_loss_bbox: 2.8754, d0.dn_loss_cls: 0.9206, d0.dn_loss_bbox: 2.8754, d1.dn_loss_cls: 0.9206, d1.dn_loss_bbox: 2.8754, d2.dn_loss_cls: 0.9206, d2.dn_loss_bbox: 2.8754, d3.dn_loss_cls: 0.9206, d3.dn_loss_bbox: 2.8754, d4.dn_loss_cls: 0.9206, d4.dn_loss_bbox: 2.8754, loss_cls_lane: 0.6566, loss_cls_H: 0.6566, loss_bbox_lane: 6.4749, loss_bbox_H: 6.4836, d0.loss_cls_lane: 0.6566, d0.loss_cls_H: 0.6566, d0.loss_bbox_lane: 6.4749, d0.loss_bbox_H: 6.4836, d1.loss_cls_lane: 0.6566, d1.loss_cls_H: 0.6566, d1.loss_bbox_lane: 6.4749, d1.loss_bbox_H: 6.4836, d2.loss_cls_lane: 0.6566, d2.loss_cls_H: 0.6566, d2.loss_bbox_lane: 6.4749, d2.loss_bbox_H: 6.4836, d3.loss_cls_lane: 0.6566, d3.loss_cls_H: 0.6566, d3.loss_bbox_lane: 6.4749, d3.loss_bbox_H: 6.4836, d4.loss_cls_lane: 0.6566, d4.loss_cls_H: 0.6566, d4.loss_bbox_lane: 6.4749, d4.loss_bbox_H: 6.4836, vlm_loss: 0.0000, loss: 137.6892, grad_norm: nan
2025-03-01 05:48:37,805 - mmdet - INFO - Iter [9550/168780]	lr: 3.969e-04, eta: 5 days, 13:02:26, time: 2.946, data_time: 0.017, memory: 22041, loss_cls: 1.8437, loss_bbox: 2.8425, d0.loss_cls: 1.8437, d0.loss_bbox: 2.8425, d1.loss_cls: 1.8437, d1.loss_bbox: 2.8425, d2.loss_cls: 1.8437, d2.loss_bbox: 2.8425, d3.loss_cls: 1.8437, d3.loss_bbox: 2.8425, d4.loss_cls: 1.8437, d4.loss_bbox: 2.8425, dn_loss_cls: 0.9192, dn_loss_bbox: 2.4355, d0.dn_loss_cls: 0.9192, d0.dn_loss_bbox: 2.4355, d1.dn_loss_cls: 0.9192, d1.dn_loss_bbox: 2.4355, d2.dn_loss_cls: 0.9192, d2.dn_loss_bbox: 2.4355, d3.dn_loss_cls: 0.9192, d3.dn_loss_bbox: 2.4355, d4.dn_loss_cls: 0.9192, d4.dn_loss_bbox: 2.4355, loss_cls_lane: 0.6300, loss_cls_H: 0.6300, loss_bbox_lane: 6.4657, loss_bbox_H: 6.4628, d0.loss_cls_lane: 0.6300, d0.loss_cls_H: 0.6300, d0.loss_bbox_lane: 6.4657, d0.loss_bbox_H: 6.4628, d1.loss_cls_lane: 0.6300, d1.loss_cls_H: 0.6300, d1.loss_bbox_lane: 6.4657, d1.loss_bbox_H: 6.4628, d2.loss_cls_lane: 0.6300, d2.loss_cls_H: 0.6300, d2.loss_bbox_lane: 6.4657, d2.loss_bbox_H: 6.4628, d3.loss_cls_lane: 0.6300, d3.loss_cls_H: 0.6300, d3.loss_bbox_lane: 6.4657, d3.loss_bbox_H: 6.4628, d4.loss_cls_lane: 0.6300, d4.loss_cls_H: 0.6300, d4.loss_bbox_lane: 6.4657, d4.loss_bbox_H: 6.4628, vlm_loss: 0.0000, loss: 133.3760, grad_norm: nan
2025-03-01 05:51:07,579 - mmdet - INFO - Iter [9600/168780]	lr: 3.968e-04, eta: 5 days, 12:59:46, time: 2.996, data_time: 0.017, memory: 22041, loss_cls: 1.4446, loss_bbox: 2.5333, d0.loss_cls: 1.4446, d0.loss_bbox: 2.5333, d1.loss_cls: 1.4446, d1.loss_bbox: 2.5333, d2.loss_cls: 1.4446, d2.loss_bbox: 2.5333, d3.loss_cls: 1.4446, d3.loss_bbox: 2.5333, d4.loss_cls: 1.4446, d4.loss_bbox: 2.5333, dn_loss_cls: 1.0107, dn_loss_bbox: 2.2226, d0.dn_loss_cls: 1.0107, d0.dn_loss_bbox: 2.2226, d1.dn_loss_cls: 1.0107, d1.dn_loss_bbox: 2.2226, d2.dn_loss_cls: 1.0107, d2.dn_loss_bbox: 2.2226, d3.dn_loss_cls: 1.0107, d3.dn_loss_bbox: 2.2226, d4.dn_loss_cls: 1.0107, d4.dn_loss_bbox: 2.2226, loss_cls_lane: 0.6431, loss_cls_H: 0.6431, loss_bbox_lane: 6.5050, loss_bbox_H: 6.5030, d0.loss_cls_lane: 0.6431, d0.loss_cls_H: 0.6431, d0.loss_bbox_lane: 6.5050, d0.loss_bbox_H: 6.5030, d1.loss_cls_lane: 0.6431, d1.loss_cls_H: 0.6431, d1.loss_bbox_lane: 6.5050, d1.loss_bbox_H: 6.5030, d2.loss_cls_lane: 0.6431, d2.loss_cls_H: 0.6431, d2.loss_bbox_lane: 6.5050, d2.loss_bbox_H: 6.5030, d3.loss_cls_lane: 0.6431, d3.loss_cls_H: 0.6431, d3.loss_bbox_lane: 6.5050, d3.loss_bbox_H: 6.5030, d4.loss_cls_lane: 0.6431, d4.loss_cls_H: 0.6431, d4.loss_bbox_lane: 6.5050, d4.loss_bbox_H: 6.5030, vlm_loss: 0.0000, loss: 129.0324, grad_norm: nan
2025-03-01 05:53:36,275 - mmdet - INFO - Iter [9650/168780]	lr: 3.968e-04, eta: 5 days, 12:56:47, time: 2.974, data_time: 0.016, memory: 22041, loss_cls: 1.8393, loss_bbox: 2.8761, d0.loss_cls: 1.8393, d0.loss_bbox: 2.8761, d1.loss_cls: 1.8393, d1.loss_bbox: 2.8761, d2.loss_cls: 1.8393, d2.loss_bbox: 2.8761, d3.loss_cls: 1.8393, d3.loss_bbox: 2.8761, d4.loss_cls: 1.8393, d4.loss_bbox: 2.8761, dn_loss_cls: 0.9563, dn_loss_bbox: 2.5864, d0.dn_loss_cls: 0.9563, d0.dn_loss_bbox: 2.5864, d1.dn_loss_cls: 0.9563, d1.dn_loss_bbox: 2.5864, d2.dn_loss_cls: 0.9563, d2.dn_loss_bbox: 2.5864, d3.dn_loss_cls: 0.9563, d3.dn_loss_bbox: 2.5864, d4.dn_loss_cls: 0.9563, d4.dn_loss_bbox: 2.5864, loss_cls_lane: 0.6619, loss_cls_H: 0.6619, loss_bbox_lane: 6.9384, loss_bbox_H: 6.9341, d0.loss_cls_lane: 0.6619, d0.loss_cls_H: 0.6619, d0.loss_bbox_lane: 6.9384, d0.loss_bbox_H: 6.9341, d1.loss_cls_lane: 0.6619, d1.loss_cls_H: 0.6619, d1.loss_bbox_lane: 6.9384, d1.loss_bbox_H: 6.9341, d2.loss_cls_lane: 0.6619, d2.loss_cls_H: 0.6619, d2.loss_bbox_lane: 6.9384, d2.loss_bbox_H: 6.9341, d3.loss_cls_lane: 0.6619, d3.loss_cls_H: 0.6619, d3.loss_bbox_lane: 6.9384, d3.loss_bbox_H: 6.9341, d4.loss_cls_lane: 0.6619, d4.loss_cls_H: 0.6619, d4.loss_bbox_lane: 6.9384, d4.loss_bbox_H: 6.9341, vlm_loss: 0.0000, loss: 140.7267, grad_norm: nan
2025-03-01 05:56:05,093 - mmdet - INFO - Iter [9700/168780]	lr: 3.968e-04, eta: 5 days, 12:53:51, time: 2.976, data_time: 0.019, memory: 22041, loss_cls: 1.5877, loss_bbox: 2.6423, d0.loss_cls: 1.5877, d0.loss_bbox: 2.6423, d1.loss_cls: 1.5877, d1.loss_bbox: 2.6423, d2.loss_cls: 1.5877, d2.loss_bbox: 2.6423, d3.loss_cls: 1.5877, d3.loss_bbox: 2.6423, d4.loss_cls: 1.5877, d4.loss_bbox: 2.6423, dn_loss_cls: 0.8866, dn_loss_bbox: 2.2684, d0.dn_loss_cls: 0.8866, d0.dn_loss_bbox: 2.2684, d1.dn_loss_cls: 0.8866, d1.dn_loss_bbox: 2.2684, d2.dn_loss_cls: 0.8866, d2.dn_loss_bbox: 2.2684, d3.dn_loss_cls: 0.8866, d3.dn_loss_bbox: 2.2684, d4.dn_loss_cls: 0.8866, d4.dn_loss_bbox: 2.2684, loss_cls_lane: 0.6533, loss_cls_H: 0.6533, loss_bbox_lane: 6.5551, loss_bbox_H: 6.5522, d0.loss_cls_lane: 0.6533, d0.loss_cls_H: 0.6533, d0.loss_bbox_lane: 6.5551, d0.loss_bbox_H: 6.5522, d1.loss_cls_lane: 0.6533, d1.loss_cls_H: 0.6533, d1.loss_bbox_lane: 6.5551, d1.loss_bbox_H: 6.5522, d2.loss_cls_lane: 0.6533, d2.loss_cls_H: 0.6533, d2.loss_bbox_lane: 6.5551, d2.loss_bbox_H: 6.5522, d3.loss_cls_lane: 0.6533, d3.loss_cls_H: 0.6533, d3.loss_bbox_lane: 6.5551, d3.loss_bbox_H: 6.5522, d4.loss_cls_lane: 0.6533, d4.loss_cls_H: 0.6533, d4.loss_bbox_lane: 6.5551, d4.loss_bbox_H: 6.5522, vlm_loss: 0.0000, loss: 130.7930, grad_norm: nan
2025-03-01 05:58:32,289 - mmdet - INFO - Iter [9750/168780]	lr: 3.967e-04, eta: 5 days, 12:50:29, time: 2.944, data_time: 0.018, memory: 22041, loss_cls: 1.4817, loss_bbox: 2.7615, d0.loss_cls: 1.4817, d0.loss_bbox: 2.7615, d1.loss_cls: 1.4817, d1.loss_bbox: 2.7615, d2.loss_cls: 1.4817, d2.loss_bbox: 2.7615, d3.loss_cls: 1.4817, d3.loss_bbox: 2.7615, d4.loss_cls: 1.4817, d4.loss_bbox: 2.7615, dn_loss_cls: 0.9702, dn_loss_bbox: 2.5642, d0.dn_loss_cls: 0.9702, d0.dn_loss_bbox: 2.5642, d1.dn_loss_cls: 0.9702, d1.dn_loss_bbox: 2.5642, d2.dn_loss_cls: 0.9702, d2.dn_loss_bbox: 2.5642, d3.dn_loss_cls: 0.9702, d3.dn_loss_bbox: 2.5642, d4.dn_loss_cls: 0.9702, d4.dn_loss_bbox: 2.5642, loss_cls_lane: 0.6463, loss_cls_H: 0.7538, loss_bbox_lane: 6.0561, loss_bbox_H: 6.0813, d0.loss_cls_lane: 0.6463, d0.loss_cls_H: 0.7538, d0.loss_bbox_lane: 6.0561, d0.loss_bbox_H: 6.0813, d1.loss_cls_lane: 0.6463, d1.loss_cls_H: 0.7538, d1.loss_bbox_lane: 6.0561, d1.loss_bbox_H: 6.0813, d2.loss_cls_lane: 0.6463, d2.loss_cls_H: 0.7538, d2.loss_bbox_lane: 6.0561, d2.loss_bbox_H: 6.0813, d3.loss_cls_lane: 0.6463, d3.loss_cls_H: 0.7538, d3.loss_bbox_lane: 6.0561, d3.loss_bbox_H: 6.0813, d4.loss_cls_lane: 0.6463, d4.loss_cls_H: 0.7538, d4.loss_bbox_lane: 6.0561, d4.loss_bbox_H: 6.0813, vlm_loss: 0.0000, loss: 127.8910, grad_norm: nan
2025-03-01 06:01:01,842 - mmdet - INFO - Iter [9800/168780]	lr: 3.967e-04, eta: 5 days, 12:47:46, time: 2.991, data_time: 0.017, memory: 22041, loss_cls: 1.3466, loss_bbox: 2.7864, d0.loss_cls: 1.3466, d0.loss_bbox: 2.7864, d1.loss_cls: 1.3466, d1.loss_bbox: 2.7864, d2.loss_cls: 1.3466, d2.loss_bbox: 2.7864, d3.loss_cls: 1.3466, d3.loss_bbox: 2.7864, d4.loss_cls: 1.3466, d4.loss_bbox: 2.7864, dn_loss_cls: 0.8986, dn_loss_bbox: 2.5203, d0.dn_loss_cls: 0.8986, d0.dn_loss_bbox: 2.5203, d1.dn_loss_cls: 0.8986, d1.dn_loss_bbox: 2.5203, d2.dn_loss_cls: 0.8986, d2.dn_loss_bbox: 2.5203, d3.dn_loss_cls: 0.8986, d3.dn_loss_bbox: 2.5203, d4.dn_loss_cls: 0.8986, d4.dn_loss_bbox: 2.5203, loss_cls_lane: 0.6434, loss_cls_H: 0.6434, loss_bbox_lane: 6.5709, loss_bbox_H: 6.5542, d0.loss_cls_lane: 0.6434, d0.loss_cls_H: 0.6434, d0.loss_bbox_lane: 6.5709, d0.loss_bbox_H: 6.5542, d1.loss_cls_lane: 0.6434, d1.loss_cls_H: 0.6434, d1.loss_bbox_lane: 6.5709, d1.loss_bbox_H: 6.5542, d2.loss_cls_lane: 0.6434, d2.loss_cls_H: 0.6434, d2.loss_bbox_lane: 6.5709, d2.loss_bbox_H: 6.5542, d3.loss_cls_lane: 0.6434, d3.loss_cls_H: 0.6434, d3.loss_bbox_lane: 6.5709, d3.loss_bbox_H: 6.5542, d4.loss_cls_lane: 0.6434, d4.loss_cls_H: 0.6434, d4.loss_bbox_lane: 6.5709, d4.loss_bbox_H: 6.5542, vlm_loss: 0.0000, loss: 131.7821, grad_norm: nan
2025-03-01 06:03:27,902 - mmdet - INFO - Iter [9850/168780]	lr: 3.967e-04, eta: 5 days, 12:44:06, time: 2.921, data_time: 0.017, memory: 22041, loss_cls: 2.4155, loss_bbox: 3.0343, d0.loss_cls: 2.4155, d0.loss_bbox: 3.0343, d1.loss_cls: 2.4155, d1.loss_bbox: 3.0343, d2.loss_cls: 2.4155, d2.loss_bbox: 3.0343, d3.loss_cls: 2.4155, d3.loss_bbox: 3.0343, d4.loss_cls: 2.4155, d4.loss_bbox: 3.0343, dn_loss_cls: 0.9270, dn_loss_bbox: 2.7552, d0.dn_loss_cls: 0.9270, d0.dn_loss_bbox: 2.7552, d1.dn_loss_cls: 0.9270, d1.dn_loss_bbox: 2.7552, d2.dn_loss_cls: 0.9270, d2.dn_loss_bbox: 2.7552, d3.dn_loss_cls: 0.9270, d3.dn_loss_bbox: 2.7552, d4.dn_loss_cls: 0.9270, d4.dn_loss_bbox: 2.7552, loss_cls_lane: 0.6604, loss_cls_H: 0.6604, loss_bbox_lane: 6.6155, loss_bbox_H: 6.6128, d0.loss_cls_lane: 0.6604, d0.loss_cls_H: 0.6604, d0.loss_bbox_lane: 6.6155, d0.loss_bbox_H: 6.6128, d1.loss_cls_lane: 0.6604, d1.loss_cls_H: 0.6604, d1.loss_bbox_lane: 6.6155, d1.loss_bbox_H: 6.6128, d2.loss_cls_lane: 0.6604, d2.loss_cls_H: 0.6604, d2.loss_bbox_lane: 6.6155, d2.loss_bbox_H: 6.6128, d3.loss_cls_lane: 0.6604, d3.loss_cls_H: 0.6604, d3.loss_bbox_lane: 6.6155, d3.loss_bbox_H: 6.6128, d4.loss_cls_lane: 0.6604, d4.loss_cls_H: 0.6604, d4.loss_bbox_lane: 6.6155, d4.loss_bbox_H: 6.6128, vlm_loss: 0.0000, loss: 142.0862, grad_norm: nan
2025-03-01 06:05:56,202 - mmdet - INFO - Iter [9900/168780]	lr: 3.966e-04, eta: 5 days, 12:41:03, time: 2.966, data_time: 0.017, memory: 22041, loss_cls: 1.7242, loss_bbox: 2.9402, d0.loss_cls: 1.7242, d0.loss_bbox: 2.9402, d1.loss_cls: 1.7242, d1.loss_bbox: 2.9402, d2.loss_cls: 1.7242, d2.loss_bbox: 2.9402, d3.loss_cls: 1.7242, d3.loss_bbox: 2.9402, d4.loss_cls: 1.7242, d4.loss_bbox: 2.9402, dn_loss_cls: 0.8938, dn_loss_bbox: 2.6020, d0.dn_loss_cls: 0.8938, d0.dn_loss_bbox: 2.6020, d1.dn_loss_cls: 0.8938, d1.dn_loss_bbox: 2.6020, d2.dn_loss_cls: 0.8938, d2.dn_loss_bbox: 2.6020, d3.dn_loss_cls: 0.8938, d3.dn_loss_bbox: 2.6020, d4.dn_loss_cls: 0.8938, d4.dn_loss_bbox: 2.6020, loss_cls_lane: 0.6360, loss_cls_H: 0.6360, loss_bbox_lane: 6.6748, loss_bbox_H: 6.6602, d0.loss_cls_lane: 0.6360, d0.loss_cls_H: 0.6360, d0.loss_bbox_lane: 6.6748, d0.loss_bbox_H: 6.6602, d1.loss_cls_lane: 0.6360, d1.loss_cls_H: 0.6360, d1.loss_bbox_lane: 6.6748, d1.loss_bbox_H: 6.6602, d2.loss_cls_lane: 0.6360, d2.loss_cls_H: 0.6360, d2.loss_bbox_lane: 6.6748, d2.loss_bbox_H: 6.6602, d3.loss_cls_lane: 0.6360, d3.loss_cls_H: 0.6360, d3.loss_bbox_lane: 6.6748, d3.loss_bbox_H: 6.6602, d4.loss_cls_lane: 0.6360, d4.loss_cls_H: 0.6360, d4.loss_bbox_lane: 6.6748, d4.loss_bbox_H: 6.6602, vlm_loss: 0.0000, loss: 136.6029, grad_norm: nan
2025-03-01 06:08:23,817 - mmdet - INFO - Iter [9950/168780]	lr: 3.966e-04, eta: 5 days, 12:37:49, time: 2.952, data_time: 0.016, memory: 22041, loss_cls: 2.8542, loss_bbox: 3.2955, d0.loss_cls: 2.8542, d0.loss_bbox: 3.2955, d1.loss_cls: 2.8542, d1.loss_bbox: 3.2955, d2.loss_cls: 2.8542, d2.loss_bbox: 3.2955, d3.loss_cls: 2.8542, d3.loss_bbox: 3.2955, d4.loss_cls: 2.8542, d4.loss_bbox: 3.2955, dn_loss_cls: 0.9849, dn_loss_bbox: 3.0458, d0.dn_loss_cls: 0.9849, d0.dn_loss_bbox: 3.0458, d1.dn_loss_cls: 0.9849, d1.dn_loss_bbox: 3.0458, d2.dn_loss_cls: 0.9849, d2.dn_loss_bbox: 3.0458, d3.dn_loss_cls: 0.9849, d3.dn_loss_bbox: 3.0458, d4.dn_loss_cls: 0.9849, d4.dn_loss_bbox: 3.0458, loss_cls_lane: 0.6301, loss_cls_H: 0.6301, loss_bbox_lane: 6.4290, loss_bbox_H: 6.4234, d0.loss_cls_lane: 0.6301, d0.loss_cls_H: 0.6301, d0.loss_bbox_lane: 6.4290, d0.loss_bbox_H: 6.4234, d1.loss_cls_lane: 0.6301, d1.loss_cls_H: 0.6301, d1.loss_bbox_lane: 6.4290, d1.loss_bbox_H: 6.4234, d2.loss_cls_lane: 0.6301, d2.loss_cls_H: 0.6301, d2.loss_bbox_lane: 6.4290, d2.loss_bbox_H: 6.4234, d3.loss_cls_lane: 0.6301, d3.loss_cls_H: 0.6301, d3.loss_bbox_lane: 6.4290, d3.loss_bbox_H: 6.4234, d4.loss_cls_lane: 0.6301, d4.loss_cls_H: 0.6301, d4.loss_bbox_lane: 6.4290, d4.loss_bbox_H: 6.4234, vlm_loss: 0.0000, loss: 145.7576, grad_norm: nan
2025-03-01 06:10:52,054 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 06:10:52,054 - mmdet - INFO - Iter [10000/168780]	lr: 3.965e-04, eta: 5 days, 12:34:46, time: 2.965, data_time: 0.017, memory: 22041, loss_cls: 1.3254, loss_bbox: 2.9546, d0.loss_cls: 1.3254, d0.loss_bbox: 2.9546, d1.loss_cls: 1.3254, d1.loss_bbox: 2.9546, d2.loss_cls: 1.3254, d2.loss_bbox: 2.9546, d3.loss_cls: 1.3254, d3.loss_bbox: 2.9546, d4.loss_cls: 1.3254, d4.loss_bbox: 2.9546, dn_loss_cls: 0.9613, dn_loss_bbox: 2.5552, d0.dn_loss_cls: 0.9613, d0.dn_loss_bbox: 2.5552, d1.dn_loss_cls: 0.9613, d1.dn_loss_bbox: 2.5552, d2.dn_loss_cls: 0.9613, d2.dn_loss_bbox: 2.5552, d3.dn_loss_cls: 0.9613, d3.dn_loss_bbox: 2.5552, d4.dn_loss_cls: 0.9613, d4.dn_loss_bbox: 2.5552, loss_cls_lane: 0.6433, loss_cls_H: 0.6433, loss_bbox_lane: 6.5576, loss_bbox_H: 6.5412, d0.loss_cls_lane: 0.6433, d0.loss_cls_H: 0.6433, d0.loss_bbox_lane: 6.5576, d0.loss_bbox_H: 6.5412, d1.loss_cls_lane: 0.6433, d1.loss_cls_H: 0.6433, d1.loss_bbox_lane: 6.5576, d1.loss_bbox_H: 6.5412, d2.loss_cls_lane: 0.6433, d2.loss_cls_H: 0.6433, d2.loss_bbox_lane: 6.5576, d2.loss_bbox_H: 6.5412, d3.loss_cls_lane: 0.6433, d3.loss_cls_H: 0.6433, d3.loss_bbox_lane: 6.5576, d3.loss_bbox_H: 6.5412, d4.loss_cls_lane: 0.6433, d4.loss_cls_H: 0.6433, d4.loss_bbox_lane: 6.5576, d4.loss_bbox_H: 6.5412, vlm_loss: 0.0000, loss: 133.0912, grad_norm: nan
2025-03-01 06:13:20,940 - mmdet - INFO - Iter [10050/168780]	lr: 3.965e-04, eta: 5 days, 12:31:54, time: 2.978, data_time: 0.017, memory: 22041, loss_cls: 1.5628, loss_bbox: 2.8961, d0.loss_cls: 1.5628, d0.loss_bbox: 2.8961, d1.loss_cls: 1.5628, d1.loss_bbox: 2.8961, d2.loss_cls: 1.5628, d2.loss_bbox: 2.8961, d3.loss_cls: 1.5628, d3.loss_bbox: 2.8961, d4.loss_cls: 1.5628, d4.loss_bbox: 2.8961, dn_loss_cls: 0.9870, dn_loss_bbox: 2.4931, d0.dn_loss_cls: 0.9870, d0.dn_loss_bbox: 2.4931, d1.dn_loss_cls: 0.9870, d1.dn_loss_bbox: 2.4931, d2.dn_loss_cls: 0.9870, d2.dn_loss_bbox: 2.4931, d3.dn_loss_cls: 0.9870, d3.dn_loss_bbox: 2.4931, d4.dn_loss_cls: 0.9870, d4.dn_loss_bbox: 2.4931, loss_cls_lane: 0.6847, loss_cls_H: 0.6847, loss_bbox_lane: 7.0699, loss_bbox_H: 7.0597, d0.loss_cls_lane: 0.6847, d0.loss_cls_H: 0.6847, d0.loss_bbox_lane: 7.0699, d0.loss_bbox_H: 7.0597, d1.loss_cls_lane: 0.6847, d1.loss_cls_H: 0.6847, d1.loss_bbox_lane: 7.0699, d1.loss_bbox_H: 7.0597, d2.loss_cls_lane: 0.6847, d2.loss_cls_H: 0.6847, d2.loss_bbox_lane: 7.0699, d2.loss_bbox_H: 7.0597, d3.loss_cls_lane: 0.6847, d3.loss_cls_H: 0.6847, d3.loss_bbox_lane: 7.0699, d3.loss_bbox_H: 7.0597, d4.loss_cls_lane: 0.6847, d4.loss_cls_H: 0.6847, d4.loss_bbox_lane: 7.0699, d4.loss_bbox_H: 7.0597, vlm_loss: 0.0000, loss: 140.6271, grad_norm: nan
2025-03-01 06:15:47,959 - mmdet - INFO - Iter [10100/168780]	lr: 3.965e-04, eta: 5 days, 12:28:32, time: 2.940, data_time: 0.018, memory: 22041, loss_cls: 1.8248, loss_bbox: 2.8023, d0.loss_cls: 1.8248, d0.loss_bbox: 2.8023, d1.loss_cls: 1.8248, d1.loss_bbox: 2.8023, d2.loss_cls: 1.8248, d2.loss_bbox: 2.8023, d3.loss_cls: 1.8248, d3.loss_bbox: 2.8023, d4.loss_cls: 1.8248, d4.loss_bbox: 2.8023, dn_loss_cls: 0.9116, dn_loss_bbox: 2.4038, d0.dn_loss_cls: 0.9116, d0.dn_loss_bbox: 2.4038, d1.dn_loss_cls: 0.9116, d1.dn_loss_bbox: 2.4038, d2.dn_loss_cls: 0.9116, d2.dn_loss_bbox: 2.4038, d3.dn_loss_cls: 0.9116, d3.dn_loss_bbox: 2.4038, d4.dn_loss_cls: 0.9116, d4.dn_loss_bbox: 2.4038, loss_cls_lane: 0.6409, loss_cls_H: 0.6409, loss_bbox_lane: 6.3754, loss_bbox_H: 6.3745, d0.loss_cls_lane: 0.6409, d0.loss_cls_H: 0.6409, d0.loss_bbox_lane: 6.3754, d0.loss_bbox_H: 6.3745, d1.loss_cls_lane: 0.6409, d1.loss_cls_H: 0.6409, d1.loss_bbox_lane: 6.3754, d1.loss_bbox_H: 6.3745, d2.loss_cls_lane: 0.6409, d2.loss_cls_H: 0.6409, d2.loss_bbox_lane: 6.3754, d2.loss_bbox_H: 6.3745, d3.loss_cls_lane: 0.6409, d3.loss_cls_H: 0.6409, d3.loss_bbox_lane: 6.3754, d3.loss_bbox_H: 6.3745, d4.loss_cls_lane: 0.6409, d4.loss_cls_H: 0.6409, d4.loss_bbox_lane: 6.3754, d4.loss_bbox_H: 6.3745, vlm_loss: 0.0000, loss: 131.8443, grad_norm: nan
2025-03-01 06:18:09,558 - mmdet - INFO - Iter [10150/168780]	lr: 3.964e-04, eta: 5 days, 12:23:46, time: 2.832, data_time: 0.016, memory: 22041, loss_cls: 3.5955, loss_bbox: 3.4266, d0.loss_cls: 3.5955, d0.loss_bbox: 3.4266, d1.loss_cls: 3.5955, d1.loss_bbox: 3.4266, d2.loss_cls: 3.5955, d2.loss_bbox: 3.4266, d3.loss_cls: 3.5955, d3.loss_bbox: 3.4266, d4.loss_cls: 3.5955, d4.loss_bbox: 3.4266, dn_loss_cls: 0.8641, dn_loss_bbox: 3.1194, d0.dn_loss_cls: 0.8641, d0.dn_loss_bbox: 3.1194, d1.dn_loss_cls: 0.8641, d1.dn_loss_bbox: 3.1194, d2.dn_loss_cls: 0.8641, d2.dn_loss_bbox: 3.1194, d3.dn_loss_cls: 0.8641, d3.dn_loss_bbox: 3.1194, d4.dn_loss_cls: 0.8641, d4.dn_loss_bbox: 3.1194, loss_cls_lane: 0.6580, loss_cls_H: 0.9268, loss_bbox_lane: 6.0908, loss_bbox_H: 6.0861, d0.loss_cls_lane: 0.6580, d0.loss_cls_H: 0.9268, d0.loss_bbox_lane: 6.0908, d0.loss_bbox_H: 6.0861, d1.loss_cls_lane: 0.6580, d1.loss_cls_H: 0.9268, d1.loss_bbox_lane: 6.0908, d1.loss_bbox_H: 6.0861, d2.loss_cls_lane: 0.6580, d2.loss_cls_H: 0.9268, d2.loss_bbox_lane: 6.0908, d2.loss_bbox_H: 6.0861, d3.loss_cls_lane: 0.6580, d3.loss_cls_H: 0.9268, d3.loss_bbox_lane: 6.0908, d3.loss_bbox_H: 6.0861, d4.loss_cls_lane: 0.6580, d4.loss_cls_H: 0.9268, d4.loss_bbox_lane: 6.0908, d4.loss_bbox_H: 6.0861, vlm_loss: 0.0000, loss: 148.6043, grad_norm: nan
2025-03-01 06:20:39,397 - mmdet - INFO - Iter [10200/168780]	lr: 3.964e-04, eta: 5 days, 12:21:10, time: 2.997, data_time: 0.017, memory: 22041, loss_cls: 1.3367, loss_bbox: 2.9942, d0.loss_cls: 1.3367, d0.loss_bbox: 2.9942, d1.loss_cls: 1.3367, d1.loss_bbox: 2.9942, d2.loss_cls: 1.3367, d2.loss_bbox: 2.9942, d3.loss_cls: 1.3367, d3.loss_bbox: 2.9942, d4.loss_cls: 1.3367, d4.loss_bbox: 2.9942, dn_loss_cls: 0.8716, dn_loss_bbox: 2.6271, d0.dn_loss_cls: 0.8716, d0.dn_loss_bbox: 2.6271, d1.dn_loss_cls: 0.8716, d1.dn_loss_bbox: 2.6271, d2.dn_loss_cls: 0.8716, d2.dn_loss_bbox: 2.6271, d3.dn_loss_cls: 0.8716, d3.dn_loss_bbox: 2.6271, d4.dn_loss_cls: 0.8716, d4.dn_loss_bbox: 2.6271, loss_cls_lane: 0.6359, loss_cls_H: 0.6359, loss_bbox_lane: 6.4782, loss_bbox_H: 6.4640, d0.loss_cls_lane: 0.6359, d0.loss_cls_H: 0.6359, d0.loss_bbox_lane: 6.4782, d0.loss_bbox_H: 6.4640, d1.loss_cls_lane: 0.6359, d1.loss_cls_H: 0.6359, d1.loss_bbox_lane: 6.4782, d1.loss_bbox_H: 6.4640, d2.loss_cls_lane: 0.6359, d2.loss_cls_H: 0.6359, d2.loss_bbox_lane: 6.4782, d2.loss_bbox_H: 6.4640, d3.loss_cls_lane: 0.6359, d3.loss_cls_H: 0.6359, d3.loss_bbox_lane: 6.4782, d3.loss_bbox_H: 6.4640, d4.loss_cls_lane: 0.6359, d4.loss_cls_H: 0.6359, d4.loss_bbox_lane: 6.4782, d4.loss_bbox_H: 6.4640, vlm_loss: 0.0000, loss: 132.2622, grad_norm: nan
2025-03-01 06:23:08,664 - mmdet - INFO - Iter [10250/168780]	lr: 3.964e-04, eta: 5 days, 12:18:25, time: 2.985, data_time: 0.016, memory: 22041, loss_cls: 1.5673, loss_bbox: 2.8645, d0.loss_cls: 1.5673, d0.loss_bbox: 2.8645, d1.loss_cls: 1.5673, d1.loss_bbox: 2.8645, d2.loss_cls: 1.5673, d2.loss_bbox: 2.8645, d3.loss_cls: 1.5673, d3.loss_bbox: 2.8645, d4.loss_cls: 1.5673, d4.loss_bbox: 2.8645, dn_loss_cls: 0.9906, dn_loss_bbox: 2.3907, d0.dn_loss_cls: 0.9906, d0.dn_loss_bbox: 2.3907, d1.dn_loss_cls: 0.9906, d1.dn_loss_bbox: 2.3907, d2.dn_loss_cls: 0.9906, d2.dn_loss_bbox: 2.3907, d3.dn_loss_cls: 0.9906, d3.dn_loss_bbox: 2.3907, d4.dn_loss_cls: 0.9906, d4.dn_loss_bbox: 2.3907, loss_cls_lane: 0.6327, loss_cls_H: 0.6327, loss_bbox_lane: 6.3695, loss_bbox_H: 6.3917, d0.loss_cls_lane: 0.6327, d0.loss_cls_H: 0.6327, d0.loss_bbox_lane: 6.3695, d0.loss_bbox_H: 6.3917, d1.loss_cls_lane: 0.6327, d1.loss_cls_H: 0.6327, d1.loss_bbox_lane: 6.3695, d1.loss_bbox_H: 6.3917, d2.loss_cls_lane: 0.6327, d2.loss_cls_H: 0.6327, d2.loss_bbox_lane: 6.3695, d2.loss_bbox_H: 6.3917, d3.loss_cls_lane: 0.6327, d3.loss_cls_H: 0.6327, d3.loss_bbox_lane: 6.3695, d3.loss_bbox_H: 6.3917, d4.loss_cls_lane: 0.6327, d4.loss_cls_H: 0.6327, d4.loss_bbox_lane: 6.3695, d4.loss_bbox_H: 6.3917, vlm_loss: 0.0000, loss: 131.0383, grad_norm: nan
2025-03-01 06:25:34,676 - mmdet - INFO - Iter [10300/168780]	lr: 3.963e-04, eta: 5 days, 12:14:50, time: 2.920, data_time: 0.016, memory: 22041, loss_cls: 2.5403, loss_bbox: 2.5356, d0.loss_cls: 2.5403, d0.loss_bbox: 2.5356, d1.loss_cls: 2.5403, d1.loss_bbox: 2.5356, d2.loss_cls: 2.5403, d2.loss_bbox: 2.5356, d3.loss_cls: 2.5403, d3.loss_bbox: 2.5356, d4.loss_cls: 2.5403, d4.loss_bbox: 2.5356, dn_loss_cls: 0.9129, dn_loss_bbox: 2.1514, d0.dn_loss_cls: 0.9129, d0.dn_loss_bbox: 2.1514, d1.dn_loss_cls: 0.9129, d1.dn_loss_bbox: 2.1514, d2.dn_loss_cls: 0.9129, d2.dn_loss_bbox: 2.1514, d3.dn_loss_cls: 0.9129, d3.dn_loss_bbox: 2.1514, d4.dn_loss_cls: 0.9129, d4.dn_loss_bbox: 2.1514, loss_cls_lane: 0.6718, loss_cls_H: 0.6718, loss_bbox_lane: 6.8107, loss_bbox_H: 6.8090, d0.loss_cls_lane: 0.6718, d0.loss_cls_H: 0.6718, d0.loss_bbox_lane: 6.8107, d0.loss_bbox_H: 6.8090, d1.loss_cls_lane: 0.6718, d1.loss_cls_H: 0.6718, d1.loss_bbox_lane: 6.8107, d1.loss_bbox_H: 6.8090, d2.loss_cls_lane: 0.6718, d2.loss_cls_H: 0.6718, d2.loss_bbox_lane: 6.8107, d2.loss_bbox_H: 6.8090, d3.loss_cls_lane: 0.6718, d3.loss_cls_H: 0.6718, d3.loss_bbox_lane: 6.8107, d3.loss_bbox_H: 6.8090, d4.loss_cls_lane: 0.6718, d4.loss_cls_H: 0.6718, d4.loss_bbox_lane: 6.8107, d4.loss_bbox_H: 6.8090, vlm_loss: 0.0000, loss: 138.6207, grad_norm: nan
2025-03-01 06:28:03,658 - mmdet - INFO - Iter [10350/168780]	lr: 3.963e-04, eta: 5 days, 12:12:01, time: 2.980, data_time: 0.017, memory: 22041, loss_cls: 1.4600, loss_bbox: 2.6579, d0.loss_cls: 1.4600, d0.loss_bbox: 2.6579, d1.loss_cls: 1.4600, d1.loss_bbox: 2.6579, d2.loss_cls: 1.4600, d2.loss_bbox: 2.6579, d3.loss_cls: 1.4600, d3.loss_bbox: 2.6579, d4.loss_cls: 1.4600, d4.loss_bbox: 2.6579, dn_loss_cls: 0.9485, dn_loss_bbox: 2.3207, d0.dn_loss_cls: 0.9485, d0.dn_loss_bbox: 2.3207, d1.dn_loss_cls: 0.9485, d1.dn_loss_bbox: 2.3207, d2.dn_loss_cls: 0.9485, d2.dn_loss_bbox: 2.3207, d3.dn_loss_cls: 0.9485, d3.dn_loss_bbox: 2.3207, d4.dn_loss_cls: 0.9485, d4.dn_loss_bbox: 2.3207, loss_cls_lane: 0.6476, loss_cls_H: 0.6476, loss_bbox_lane: 6.3719, loss_bbox_H: 6.3741, d0.loss_cls_lane: 0.6476, d0.loss_cls_H: 0.6476, d0.loss_bbox_lane: 6.3719, d0.loss_bbox_H: 6.3741, d1.loss_cls_lane: 0.6476, d1.loss_cls_H: 0.6476, d1.loss_bbox_lane: 6.3719, d1.loss_bbox_H: 6.3741, d2.loss_cls_lane: 0.6476, d2.loss_cls_H: 0.6476, d2.loss_bbox_lane: 6.3719, d2.loss_bbox_H: 6.3741, d3.loss_cls_lane: 0.6476, d3.loss_cls_H: 0.6476, d3.loss_bbox_lane: 6.3719, d3.loss_bbox_H: 6.3741, d4.loss_cls_lane: 0.6476, d4.loss_cls_H: 0.6476, d4.loss_bbox_lane: 6.3719, d4.loss_bbox_H: 6.3741, vlm_loss: 0.0000, loss: 128.5708, grad_norm: nan
2025-03-01 06:30:31,853 - mmdet - INFO - Iter [10400/168780]	lr: 3.963e-04, eta: 5 days, 12:09:00, time: 2.964, data_time: 0.017, memory: 22041, loss_cls: 1.5600, loss_bbox: 2.7963, d0.loss_cls: 1.5600, d0.loss_bbox: 2.7963, d1.loss_cls: 1.5600, d1.loss_bbox: 2.7963, d2.loss_cls: 1.5600, d2.loss_bbox: 2.7963, d3.loss_cls: 1.5600, d3.loss_bbox: 2.7963, d4.loss_cls: 1.5600, d4.loss_bbox: 2.7963, dn_loss_cls: 0.9505, dn_loss_bbox: 2.4980, d0.dn_loss_cls: 0.9505, d0.dn_loss_bbox: 2.4980, d1.dn_loss_cls: 0.9505, d1.dn_loss_bbox: 2.4980, d2.dn_loss_cls: 0.9505, d2.dn_loss_bbox: 2.4980, d3.dn_loss_cls: 0.9505, d3.dn_loss_bbox: 2.4980, d4.dn_loss_cls: 0.9505, d4.dn_loss_bbox: 2.4980, loss_cls_lane: 0.6473, loss_cls_H: 0.6473, loss_bbox_lane: 6.2608, loss_bbox_H: 6.2628, d0.loss_cls_lane: 0.6473, d0.loss_cls_H: 0.6473, d0.loss_bbox_lane: 6.2608, d0.loss_bbox_H: 6.2628, d1.loss_cls_lane: 0.6473, d1.loss_cls_H: 0.6473, d1.loss_bbox_lane: 6.2608, d1.loss_bbox_H: 6.2628, d2.loss_cls_lane: 0.6473, d2.loss_cls_H: 0.6473, d2.loss_bbox_lane: 6.2608, d2.loss_bbox_H: 6.2628, d3.loss_cls_lane: 0.6473, d3.loss_cls_H: 0.6473, d3.loss_bbox_lane: 6.2608, d3.loss_bbox_H: 6.2628, d4.loss_cls_lane: 0.6473, d4.loss_cls_H: 0.6473, d4.loss_bbox_lane: 6.2608, d4.loss_bbox_H: 6.2628, vlm_loss: 0.0000, loss: 129.7384, grad_norm: nan
2025-03-01 06:33:00,758 - mmdet - INFO - Iter [10450/168780]	lr: 3.962e-04, eta: 5 days, 12:06:10, time: 2.978, data_time: 0.017, memory: 22041, loss_cls: 1.4280, loss_bbox: 2.6975, d0.loss_cls: 1.4280, d0.loss_bbox: 2.6975, d1.loss_cls: 1.4280, d1.loss_bbox: 2.6975, d2.loss_cls: 1.4280, d2.loss_bbox: 2.6975, d3.loss_cls: 1.4280, d3.loss_bbox: 2.6975, d4.loss_cls: 1.4280, d4.loss_bbox: 2.6975, dn_loss_cls: 0.9147, dn_loss_bbox: 2.2784, d0.dn_loss_cls: 0.9147, d0.dn_loss_bbox: 2.2784, d1.dn_loss_cls: 0.9147, d1.dn_loss_bbox: 2.2784, d2.dn_loss_cls: 0.9147, d2.dn_loss_bbox: 2.2784, d3.dn_loss_cls: 0.9147, d3.dn_loss_bbox: 2.2784, d4.dn_loss_cls: 0.9147, d4.dn_loss_bbox: 2.2784, loss_cls_lane: 0.6429, loss_cls_H: 0.6429, loss_bbox_lane: 6.6563, loss_bbox_H: 6.6537, d0.loss_cls_lane: 0.6429, d0.loss_cls_H: 0.6429, d0.loss_bbox_lane: 6.6563, d0.loss_bbox_H: 6.6537, d1.loss_cls_lane: 0.6429, d1.loss_cls_H: 0.6429, d1.loss_bbox_lane: 6.6563, d1.loss_bbox_H: 6.6537, d2.loss_cls_lane: 0.6429, d2.loss_cls_H: 0.6429, d2.loss_bbox_lane: 6.6563, d2.loss_bbox_H: 6.6537, d3.loss_cls_lane: 0.6429, d3.loss_cls_H: 0.6429, d3.loss_bbox_lane: 6.6563, d3.loss_bbox_H: 6.6537, d4.loss_cls_lane: 0.6429, d4.loss_cls_H: 0.6429, d4.loss_bbox_lane: 6.6563, d4.loss_bbox_H: 6.6537, vlm_loss: 0.0000, loss: 131.4868, grad_norm: nan
2025-03-01 06:35:28,642 - mmdet - INFO - Iter [10500/168780]	lr: 3.962e-04, eta: 5 days, 12:03:05, time: 2.958, data_time: 0.017, memory: 22041, loss_cls: 1.9963, loss_bbox: 2.7245, d0.loss_cls: 1.9963, d0.loss_bbox: 2.7245, d1.loss_cls: 1.9963, d1.loss_bbox: 2.7245, d2.loss_cls: 1.9963, d2.loss_bbox: 2.7245, d3.loss_cls: 1.9963, d3.loss_bbox: 2.7245, d4.loss_cls: 1.9963, d4.loss_bbox: 2.7245, dn_loss_cls: 0.9695, dn_loss_bbox: 2.3369, d0.dn_loss_cls: 0.9695, d0.dn_loss_bbox: 2.3369, d1.dn_loss_cls: 0.9695, d1.dn_loss_bbox: 2.3369, d2.dn_loss_cls: 0.9695, d2.dn_loss_bbox: 2.3369, d3.dn_loss_cls: 0.9695, d3.dn_loss_bbox: 2.3369, d4.dn_loss_cls: 0.9695, d4.dn_loss_bbox: 2.3369, loss_cls_lane: 0.6430, loss_cls_H: 0.6968, loss_bbox_lane: 6.6141, loss_bbox_H: 6.5939, d0.loss_cls_lane: 0.6430, d0.loss_cls_H: 0.6968, d0.loss_bbox_lane: 6.6141, d0.loss_bbox_H: 6.5939, d1.loss_cls_lane: 0.6430, d1.loss_cls_H: 0.6968, d1.loss_bbox_lane: 6.6141, d1.loss_bbox_H: 6.5939, d2.loss_cls_lane: 0.6430, d2.loss_cls_H: 0.6968, d2.loss_bbox_lane: 6.6141, d2.loss_bbox_H: 6.5939, d3.loss_cls_lane: 0.6430, d3.loss_cls_H: 0.6968, d3.loss_bbox_lane: 6.6141, d3.loss_bbox_H: 6.5939, d4.loss_cls_lane: 0.6430, d4.loss_cls_H: 0.6968, d4.loss_bbox_lane: 6.6141, d4.loss_bbox_H: 6.5939, vlm_loss: 0.0000, loss: 135.4500, grad_norm: nan
2025-03-01 06:37:58,389 - mmdet - INFO - Iter [10550/168780]	lr: 3.962e-04, eta: 5 days, 12:00:29, time: 2.995, data_time: 0.017, memory: 22041, loss_cls: 1.3708, loss_bbox: 2.7079, d0.loss_cls: 1.3708, d0.loss_bbox: 2.7079, d1.loss_cls: 1.3708, d1.loss_bbox: 2.7079, d2.loss_cls: 1.3708, d2.loss_bbox: 2.7079, d3.loss_cls: 1.3708, d3.loss_bbox: 2.7079, d4.loss_cls: 1.3708, d4.loss_bbox: 2.7079, dn_loss_cls: 0.8553, dn_loss_bbox: 2.3712, d0.dn_loss_cls: 0.8553, d0.dn_loss_bbox: 2.3712, d1.dn_loss_cls: 0.8553, d1.dn_loss_bbox: 2.3712, d2.dn_loss_cls: 0.8553, d2.dn_loss_bbox: 2.3712, d3.dn_loss_cls: 0.8553, d3.dn_loss_bbox: 2.3712, d4.dn_loss_cls: 0.8553, d4.dn_loss_bbox: 2.3712, loss_cls_lane: 0.6379, loss_cls_H: 0.6379, loss_bbox_lane: 6.5692, loss_bbox_H: 6.5732, d0.loss_cls_lane: 0.6379, d0.loss_cls_H: 0.6379, d0.loss_bbox_lane: 6.5692, d0.loss_bbox_H: 6.5732, d1.loss_cls_lane: 0.6379, d1.loss_cls_H: 0.6379, d1.loss_bbox_lane: 6.5692, d1.loss_bbox_H: 6.5732, d2.loss_cls_lane: 0.6379, d2.loss_cls_H: 0.6379, d2.loss_bbox_lane: 6.5692, d2.loss_bbox_H: 6.5732, d3.loss_cls_lane: 0.6379, d3.loss_cls_H: 0.6379, d3.loss_bbox_lane: 6.5692, d3.loss_bbox_H: 6.5732, d4.loss_cls_lane: 0.6379, d4.loss_cls_H: 0.6379, d4.loss_bbox_lane: 6.5692, d4.loss_bbox_H: 6.5732, vlm_loss: 0.0000, loss: 130.3401, grad_norm: nan
2025-03-01 06:40:27,403 - mmdet - INFO - Iter [10600/168780]	lr: 3.961e-04, eta: 5 days, 11:57:41, time: 2.980, data_time: 0.017, memory: 22041, loss_cls: 1.5491, loss_bbox: 2.7849, d0.loss_cls: 1.5491, d0.loss_bbox: 2.7849, d1.loss_cls: 1.5491, d1.loss_bbox: 2.7849, d2.loss_cls: 1.5491, d2.loss_bbox: 2.7849, d3.loss_cls: 1.5491, d3.loss_bbox: 2.7849, d4.loss_cls: 1.5491, d4.loss_bbox: 2.7849, dn_loss_cls: 0.9282, dn_loss_bbox: 2.5831, d0.dn_loss_cls: 0.9282, d0.dn_loss_bbox: 2.5831, d1.dn_loss_cls: 0.9282, d1.dn_loss_bbox: 2.5831, d2.dn_loss_cls: 0.9282, d2.dn_loss_bbox: 2.5831, d3.dn_loss_cls: 0.9282, d3.dn_loss_bbox: 2.5831, d4.dn_loss_cls: 0.9282, d4.dn_loss_bbox: 2.5831, loss_cls_lane: 0.6391, loss_cls_H: 0.6391, loss_bbox_lane: 6.4773, loss_bbox_H: 6.4770, d0.loss_cls_lane: 0.6391, d0.loss_cls_H: 0.6391, d0.loss_bbox_lane: 6.4773, d0.loss_bbox_H: 6.4770, d1.loss_cls_lane: 0.6391, d1.loss_cls_H: 0.6391, d1.loss_bbox_lane: 6.4773, d1.loss_bbox_H: 6.4770, d2.loss_cls_lane: 0.6391, d2.loss_cls_H: 0.6391, d2.loss_bbox_lane: 6.4773, d2.loss_bbox_H: 6.4770, d3.loss_cls_lane: 0.6391, d3.loss_cls_H: 0.6391, d3.loss_bbox_lane: 6.4773, d3.loss_bbox_H: 6.4770, d4.loss_cls_lane: 0.6391, d4.loss_cls_H: 0.6391, d4.loss_bbox_lane: 6.4773, d4.loss_bbox_H: 6.4770, vlm_loss: 0.0000, loss: 132.4671, grad_norm: nan
2025-03-01 06:42:57,239 - mmdet - INFO - Iter [10650/168780]	lr: 3.961e-04, eta: 5 days, 11:55:06, time: 2.997, data_time: 0.018, memory: 22041, loss_cls: 1.4964, loss_bbox: 2.7907, d0.loss_cls: 1.4964, d0.loss_bbox: 2.7907, d1.loss_cls: 1.4964, d1.loss_bbox: 2.7907, d2.loss_cls: 1.4964, d2.loss_bbox: 2.7907, d3.loss_cls: 1.4964, d3.loss_bbox: 2.7907, d4.loss_cls: 1.4964, d4.loss_bbox: 2.7907, dn_loss_cls: 0.9393, dn_loss_bbox: 2.4422, d0.dn_loss_cls: 0.9393, d0.dn_loss_bbox: 2.4422, d1.dn_loss_cls: 0.9393, d1.dn_loss_bbox: 2.4422, d2.dn_loss_cls: 0.9393, d2.dn_loss_bbox: 2.4422, d3.dn_loss_cls: 0.9393, d3.dn_loss_bbox: 2.4422, d4.dn_loss_cls: 0.9393, d4.dn_loss_bbox: 2.4422, loss_cls_lane: 0.6418, loss_cls_H: 0.6418, loss_bbox_lane: 6.7074, loss_bbox_H: 6.7008, d0.loss_cls_lane: 0.6418, d0.loss_cls_H: 0.6418, d0.loss_bbox_lane: 6.7074, d0.loss_bbox_H: 6.7008, d1.loss_cls_lane: 0.6418, d1.loss_cls_H: 0.6418, d1.loss_bbox_lane: 6.7074, d1.loss_bbox_H: 6.7008, d2.loss_cls_lane: 0.6418, d2.loss_cls_H: 0.6418, d2.loss_bbox_lane: 6.7074, d2.loss_bbox_H: 6.7008, d3.loss_cls_lane: 0.6418, d3.loss_cls_H: 0.6418, d3.loss_bbox_lane: 6.7074, d3.loss_bbox_H: 6.7008, d4.loss_cls_lane: 0.6418, d4.loss_cls_H: 0.6418, d4.loss_bbox_lane: 6.7074, d4.loss_bbox_H: 6.7008, vlm_loss: 0.0000, loss: 134.1623, grad_norm: nan
2025-03-01 06:45:24,065 - mmdet - INFO - Iter [10700/168780]	lr: 3.961e-04, eta: 5 days, 11:51:47, time: 2.937, data_time: 0.017, memory: 22041, loss_cls: 1.5061, loss_bbox: 2.8845, d0.loss_cls: 1.5061, d0.loss_bbox: 2.8845, d1.loss_cls: 1.5061, d1.loss_bbox: 2.8845, d2.loss_cls: 1.5061, d2.loss_bbox: 2.8845, d3.loss_cls: 1.5061, d3.loss_bbox: 2.8845, d4.loss_cls: 1.5061, d4.loss_bbox: 2.8845, dn_loss_cls: 0.9663, dn_loss_bbox: 2.5581, d0.dn_loss_cls: 0.9663, d0.dn_loss_bbox: 2.5581, d1.dn_loss_cls: 0.9663, d1.dn_loss_bbox: 2.5581, d2.dn_loss_cls: 0.9663, d2.dn_loss_bbox: 2.5581, d3.dn_loss_cls: 0.9663, d3.dn_loss_bbox: 2.5581, d4.dn_loss_cls: 0.9663, d4.dn_loss_bbox: 2.5581, loss_cls_lane: 0.6241, loss_cls_H: 0.7316, loss_bbox_lane: 6.0781, loss_bbox_H: 6.0823, d0.loss_cls_lane: 0.6241, d0.loss_cls_H: 0.7316, d0.loss_bbox_lane: 6.0781, d0.loss_bbox_H: 6.0823, d1.loss_cls_lane: 0.6241, d1.loss_cls_H: 0.7316, d1.loss_bbox_lane: 6.0781, d1.loss_bbox_H: 6.0823, d2.loss_cls_lane: 0.6241, d2.loss_cls_H: 0.7316, d2.loss_bbox_lane: 6.0781, d2.loss_bbox_H: 6.0823, d3.loss_cls_lane: 0.6241, d3.loss_cls_H: 0.7316, d3.loss_bbox_lane: 6.0781, d3.loss_bbox_H: 6.0823, d4.loss_cls_lane: 0.6241, d4.loss_cls_H: 0.7316, d4.loss_bbox_lane: 6.0781, d4.loss_bbox_H: 6.0823, vlm_loss: 0.0000, loss: 128.5867, grad_norm: nan
2025-03-01 06:47:52,599 - mmdet - INFO - Iter [10750/168780]	lr: 3.960e-04, eta: 5 days, 11:48:53, time: 2.971, data_time: 0.017, memory: 22041, loss_cls: 1.6854, loss_bbox: 2.9099, d0.loss_cls: 1.6854, d0.loss_bbox: 2.9099, d1.loss_cls: 1.6854, d1.loss_bbox: 2.9099, d2.loss_cls: 1.6854, d2.loss_bbox: 2.9099, d3.loss_cls: 1.6854, d3.loss_bbox: 2.9099, d4.loss_cls: 1.6854, d4.loss_bbox: 2.9099, dn_loss_cls: 0.9852, dn_loss_bbox: 2.6704, d0.dn_loss_cls: 0.9852, d0.dn_loss_bbox: 2.6704, d1.dn_loss_cls: 0.9852, d1.dn_loss_bbox: 2.6704, d2.dn_loss_cls: 0.9852, d2.dn_loss_bbox: 2.6704, d3.dn_loss_cls: 0.9852, d3.dn_loss_bbox: 2.6704, d4.dn_loss_cls: 0.9852, d4.dn_loss_bbox: 2.6704, loss_cls_lane: 0.6360, loss_cls_H: 0.6360, loss_bbox_lane: 6.5714, loss_bbox_H: 6.5748, d0.loss_cls_lane: 0.6360, d0.loss_cls_H: 0.6360, d0.loss_bbox_lane: 6.5714, d0.loss_bbox_H: 6.5748, d1.loss_cls_lane: 0.6360, d1.loss_cls_H: 0.6360, d1.loss_bbox_lane: 6.5714, d1.loss_bbox_H: 6.5748, d2.loss_cls_lane: 0.6360, d2.loss_cls_H: 0.6360, d2.loss_bbox_lane: 6.5714, d2.loss_bbox_H: 6.5748, d3.loss_cls_lane: 0.6360, d3.loss_cls_H: 0.6360, d3.loss_bbox_lane: 6.5714, d3.loss_bbox_H: 6.5748, d4.loss_cls_lane: 0.6360, d4.loss_cls_H: 0.6360, d4.loss_bbox_lane: 6.5714, d4.loss_bbox_H: 6.5748, vlm_loss: 0.0000, loss: 136.0146, grad_norm: nan
2025-03-01 06:50:19,923 - mmdet - INFO - Iter [10800/168780]	lr: 3.960e-04, eta: 5 days, 11:45:42, time: 2.946, data_time: 0.016, memory: 22041, loss_cls: 1.5266, loss_bbox: 2.6888, d0.loss_cls: 1.5266, d0.loss_bbox: 2.6888, d1.loss_cls: 1.5266, d1.loss_bbox: 2.6888, d2.loss_cls: 1.5266, d2.loss_bbox: 2.6888, d3.loss_cls: 1.5266, d3.loss_bbox: 2.6888, d4.loss_cls: 1.5266, d4.loss_bbox: 2.6888, dn_loss_cls: 0.8742, dn_loss_bbox: 2.2785, d0.dn_loss_cls: 0.8742, d0.dn_loss_bbox: 2.2785, d1.dn_loss_cls: 0.8742, d1.dn_loss_bbox: 2.2785, d2.dn_loss_cls: 0.8742, d2.dn_loss_bbox: 2.2785, d3.dn_loss_cls: 0.8742, d3.dn_loss_bbox: 2.2785, d4.dn_loss_cls: 0.8742, d4.dn_loss_bbox: 2.2785, loss_cls_lane: 0.6431, loss_cls_H: 0.6431, loss_bbox_lane: 6.4254, loss_bbox_H: 6.4703, d0.loss_cls_lane: 0.6431, d0.loss_cls_H: 0.6431, d0.loss_bbox_lane: 6.4254, d0.loss_bbox_H: 6.4703, d1.loss_cls_lane: 0.6431, d1.loss_cls_H: 0.6431, d1.loss_bbox_lane: 6.4254, d1.loss_bbox_H: 6.4703, d2.loss_cls_lane: 0.6431, d2.loss_cls_H: 0.6431, d2.loss_bbox_lane: 6.4254, d2.loss_bbox_H: 6.4703, d3.loss_cls_lane: 0.6431, d3.loss_cls_H: 0.6431, d3.loss_bbox_lane: 6.4254, d3.loss_bbox_H: 6.4703, d4.loss_cls_lane: 0.6431, d4.loss_cls_H: 0.6431, d4.loss_bbox_lane: 6.4254, d4.loss_bbox_H: 6.4703, vlm_loss: 0.0000, loss: 129.2992, grad_norm: nan
2025-03-01 06:52:49,331 - mmdet - INFO - Iter [10850/168780]	lr: 3.959e-04, eta: 5 days, 11:43:01, time: 2.988, data_time: 0.018, memory: 22041, loss_cls: 1.3064, loss_bbox: 2.8952, d0.loss_cls: 1.3064, d0.loss_bbox: 2.8952, d1.loss_cls: 1.3064, d1.loss_bbox: 2.8952, d2.loss_cls: 1.3064, d2.loss_bbox: 2.8952, d3.loss_cls: 1.3064, d3.loss_bbox: 2.8952, d4.loss_cls: 1.3064, d4.loss_bbox: 2.8952, dn_loss_cls: 0.9182, dn_loss_bbox: 2.4621, d0.dn_loss_cls: 0.9182, d0.dn_loss_bbox: 2.4621, d1.dn_loss_cls: 0.9182, d1.dn_loss_bbox: 2.4621, d2.dn_loss_cls: 0.9182, d2.dn_loss_bbox: 2.4621, d3.dn_loss_cls: 0.9182, d3.dn_loss_bbox: 2.4621, d4.dn_loss_cls: 0.9182, d4.dn_loss_bbox: 2.4621, loss_cls_lane: 0.6356, loss_cls_H: 0.6356, loss_bbox_lane: 6.4959, loss_bbox_H: 6.4922, d0.loss_cls_lane: 0.6356, d0.loss_cls_H: 0.6356, d0.loss_bbox_lane: 6.4959, d0.loss_bbox_H: 6.4922, d1.loss_cls_lane: 0.6356, d1.loss_cls_H: 0.6356, d1.loss_bbox_lane: 6.4959, d1.loss_bbox_H: 6.4922, d2.loss_cls_lane: 0.6356, d2.loss_cls_H: 0.6356, d2.loss_bbox_lane: 6.4959, d2.loss_bbox_H: 6.4922, d3.loss_cls_lane: 0.6356, d3.loss_cls_H: 0.6356, d3.loss_bbox_lane: 6.4959, d3.loss_bbox_H: 6.4922, d4.loss_cls_lane: 0.6356, d4.loss_cls_H: 0.6356, d4.loss_bbox_lane: 6.4959, d4.loss_bbox_H: 6.4922, vlm_loss: 0.0000, loss: 131.0475, grad_norm: nan
2025-03-01 06:55:38,891 - mmdet - INFO - Iter [10900/168780]	lr: 3.959e-04, eta: 5 days, 11:45:12, time: 3.391, data_time: 0.016, memory: 22041, loss_cls: 1.8361, loss_bbox: 2.6718, d0.loss_cls: 1.8361, d0.loss_bbox: 2.6718, d1.loss_cls: 1.8361, d1.loss_bbox: 2.6718, d2.loss_cls: 1.8361, d2.loss_bbox: 2.6718, d3.loss_cls: 1.8361, d3.loss_bbox: 2.6718, d4.loss_cls: 1.8361, d4.loss_bbox: 2.6718, dn_loss_cls: 1.0136, dn_loss_bbox: 2.4655, d0.dn_loss_cls: 1.0136, d0.dn_loss_bbox: 2.4655, d1.dn_loss_cls: 1.0136, d1.dn_loss_bbox: 2.4655, d2.dn_loss_cls: 1.0136, d2.dn_loss_bbox: 2.4655, d3.dn_loss_cls: 1.0136, d3.dn_loss_bbox: 2.4655, d4.dn_loss_cls: 1.0136, d4.dn_loss_bbox: 2.4655, loss_cls_lane: 0.6538, loss_cls_H: 0.6538, loss_bbox_lane: 6.8542, loss_bbox_H: 6.8491, d0.loss_cls_lane: 0.6538, d0.loss_cls_H: 0.6538, d0.loss_bbox_lane: 6.8542, d0.loss_bbox_H: 6.8491, d1.loss_cls_lane: 0.6538, d1.loss_cls_H: 0.6538, d1.loss_bbox_lane: 6.8542, d1.loss_bbox_H: 6.8491, d2.loss_cls_lane: 0.6538, d2.loss_cls_H: 0.6538, d2.loss_bbox_lane: 6.8542, d2.loss_bbox_H: 6.8491, d3.loss_cls_lane: 0.6538, d3.loss_cls_H: 0.6538, d3.loss_bbox_lane: 6.8542, d3.loss_bbox_H: 6.8491, d4.loss_cls_lane: 0.6538, d4.loss_cls_H: 0.6538, d4.loss_bbox_lane: 6.8542, d4.loss_bbox_H: 6.8491, vlm_loss: 0.0000, loss: 137.9883, grad_norm: nan
2025-03-01 06:58:08,477 - mmdet - INFO - Iter [10950/168780]	lr: 3.959e-04, eta: 5 days, 11:42:33, time: 2.992, data_time: 0.016, memory: 22041, loss_cls: 1.4115, loss_bbox: 2.8411, d0.loss_cls: 1.4115, d0.loss_bbox: 2.8411, d1.loss_cls: 1.4115, d1.loss_bbox: 2.8411, d2.loss_cls: 1.4115, d2.loss_bbox: 2.8411, d3.loss_cls: 1.4115, d3.loss_bbox: 2.8411, d4.loss_cls: 1.4115, d4.loss_bbox: 2.8411, dn_loss_cls: 0.9144, dn_loss_bbox: 2.5115, d0.dn_loss_cls: 0.9144, d0.dn_loss_bbox: 2.5115, d1.dn_loss_cls: 0.9144, d1.dn_loss_bbox: 2.5115, d2.dn_loss_cls: 0.9144, d2.dn_loss_bbox: 2.5115, d3.dn_loss_cls: 0.9144, d3.dn_loss_bbox: 2.5115, d4.dn_loss_cls: 0.9144, d4.dn_loss_bbox: 2.5115, loss_cls_lane: 0.6372, loss_cls_H: 0.6372, loss_bbox_lane: 6.6702, loss_bbox_H: 6.6608, d0.loss_cls_lane: 0.6372, d0.loss_cls_H: 0.6372, d0.loss_bbox_lane: 6.6702, d0.loss_bbox_H: 6.6608, d1.loss_cls_lane: 0.6372, d1.loss_cls_H: 0.6372, d1.loss_bbox_lane: 6.6702, d1.loss_bbox_H: 6.6608, d2.loss_cls_lane: 0.6372, d2.loss_cls_H: 0.6372, d2.loss_bbox_lane: 6.6702, d2.loss_bbox_H: 6.6608, d3.loss_cls_lane: 0.6372, d3.loss_cls_H: 0.6372, d3.loss_bbox_lane: 6.6702, d3.loss_bbox_H: 6.6608, d4.loss_cls_lane: 0.6372, d4.loss_cls_H: 0.6372, d4.loss_bbox_lane: 6.6702, d4.loss_bbox_H: 6.6608, vlm_loss: 0.0000, loss: 133.7040, grad_norm: nan
2025-03-01 07:00:38,359 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 07:00:38,360 - mmdet - INFO - Iter [11000/168780]	lr: 3.958e-04, eta: 5 days, 11:39:58, time: 2.998, data_time: 0.017, memory: 22041, loss_cls: 1.2853, loss_bbox: 2.8022, d0.loss_cls: 1.2853, d0.loss_bbox: 2.8022, d1.loss_cls: 1.2853, d1.loss_bbox: 2.8022, d2.loss_cls: 1.2853, d2.loss_bbox: 2.8022, d3.loss_cls: 1.2853, d3.loss_bbox: 2.8022, d4.loss_cls: 1.2853, d4.loss_bbox: 2.8022, dn_loss_cls: 0.9510, dn_loss_bbox: 2.4596, d0.dn_loss_cls: 0.9510, d0.dn_loss_bbox: 2.4596, d1.dn_loss_cls: 0.9510, d1.dn_loss_bbox: 2.4596, d2.dn_loss_cls: 0.9510, d2.dn_loss_bbox: 2.4596, d3.dn_loss_cls: 0.9510, d3.dn_loss_bbox: 2.4596, d4.dn_loss_cls: 0.9510, d4.dn_loss_bbox: 2.4596, loss_cls_lane: 0.6554, loss_cls_H: 0.6554, loss_bbox_lane: 6.9734, loss_bbox_H: 6.9682, d0.loss_cls_lane: 0.6554, d0.loss_cls_H: 0.6554, d0.loss_bbox_lane: 6.9734, d0.loss_bbox_H: 6.9682, d1.loss_cls_lane: 0.6554, d1.loss_cls_H: 0.6554, d1.loss_bbox_lane: 6.9734, d1.loss_bbox_H: 6.9682, d2.loss_cls_lane: 0.6554, d2.loss_cls_H: 0.6554, d2.loss_bbox_lane: 6.9734, d2.loss_bbox_H: 6.9682, d3.loss_cls_lane: 0.6554, d3.loss_cls_H: 0.6554, d3.loss_bbox_lane: 6.9734, d3.loss_bbox_H: 6.9682, d4.loss_cls_lane: 0.6554, d4.loss_cls_H: 0.6554, d4.loss_bbox_lane: 6.9734, d4.loss_bbox_H: 6.9682, vlm_loss: 0.0000, loss: 136.5033, grad_norm: nan
2025-03-01 07:03:06,435 - mmdet - INFO - Iter [11050/168780]	lr: 3.958e-04, eta: 5 days, 11:36:58, time: 2.962, data_time: 0.018, memory: 22041, loss_cls: 1.8295, loss_bbox: 2.9286, d0.loss_cls: 1.8295, d0.loss_bbox: 2.9286, d1.loss_cls: 1.8295, d1.loss_bbox: 2.9286, d2.loss_cls: 1.8295, d2.loss_bbox: 2.9286, d3.loss_cls: 1.8295, d3.loss_bbox: 2.9286, d4.loss_cls: 1.8295, d4.loss_bbox: 2.9286, dn_loss_cls: 0.8999, dn_loss_bbox: 2.4795, d0.dn_loss_cls: 0.8999, d0.dn_loss_bbox: 2.4795, d1.dn_loss_cls: 0.8999, d1.dn_loss_bbox: 2.4795, d2.dn_loss_cls: 0.8999, d2.dn_loss_bbox: 2.4795, d3.dn_loss_cls: 0.8999, d3.dn_loss_bbox: 2.4795, d4.dn_loss_cls: 0.8999, d4.dn_loss_bbox: 2.4795, loss_cls_lane: 0.6717, loss_cls_H: 0.7254, loss_bbox_lane: 6.8232, loss_bbox_H: 6.8131, d0.loss_cls_lane: 0.6717, d0.loss_cls_H: 0.7254, d0.loss_bbox_lane: 6.8232, d0.loss_bbox_H: 6.8131, d1.loss_cls_lane: 0.6717, d1.loss_cls_H: 0.7254, d1.loss_bbox_lane: 6.8232, d1.loss_bbox_H: 6.8131, d2.loss_cls_lane: 0.6717, d2.loss_cls_H: 0.7254, d2.loss_bbox_lane: 6.8232, d2.loss_bbox_H: 6.8131, d3.loss_cls_lane: 0.6717, d3.loss_cls_H: 0.7254, d3.loss_bbox_lane: 6.8232, d3.loss_bbox_H: 6.8131, d4.loss_cls_lane: 0.6717, d4.loss_cls_H: 0.7254, d4.loss_bbox_lane: 6.8232, d4.loss_bbox_H: 6.8131, vlm_loss: 0.0000, loss: 139.0254, grad_norm: nan
2025-03-01 07:05:37,026 - mmdet - INFO - Iter [11100/168780]	lr: 3.958e-04, eta: 5 days, 11:34:33, time: 3.012, data_time: 0.017, memory: 22041, loss_cls: 1.3561, loss_bbox: 2.7216, d0.loss_cls: 1.3561, d0.loss_bbox: 2.7216, d1.loss_cls: 1.3561, d1.loss_bbox: 2.7216, d2.loss_cls: 1.3561, d2.loss_bbox: 2.7216, d3.loss_cls: 1.3561, d3.loss_bbox: 2.7216, d4.loss_cls: 1.3561, d4.loss_bbox: 2.7216, dn_loss_cls: 0.8887, dn_loss_bbox: 2.4089, d0.dn_loss_cls: 0.8887, d0.dn_loss_bbox: 2.4089, d1.dn_loss_cls: 0.8887, d1.dn_loss_bbox: 2.4089, d2.dn_loss_cls: 0.8887, d2.dn_loss_bbox: 2.4089, d3.dn_loss_cls: 0.8887, d3.dn_loss_bbox: 2.4089, d4.dn_loss_cls: 0.8887, d4.dn_loss_bbox: 2.4089, loss_cls_lane: 0.6440, loss_cls_H: 0.6440, loss_bbox_lane: 6.5793, loss_bbox_H: 6.5796, d0.loss_cls_lane: 0.6440, d0.loss_cls_H: 0.6440, d0.loss_bbox_lane: 6.5793, d0.loss_bbox_H: 6.5796, d1.loss_cls_lane: 0.6440, d1.loss_cls_H: 0.6440, d1.loss_bbox_lane: 6.5793, d1.loss_bbox_H: 6.5796, d2.loss_cls_lane: 0.6440, d2.loss_cls_H: 0.6440, d2.loss_bbox_lane: 6.5793, d2.loss_bbox_H: 6.5796, d3.loss_cls_lane: 0.6440, d3.loss_cls_H: 0.6440, d3.loss_bbox_lane: 6.5793, d3.loss_bbox_H: 6.5796, d4.loss_cls_lane: 0.6440, d4.loss_cls_H: 0.6440, d4.loss_bbox_lane: 6.5793, d4.loss_bbox_H: 6.5796, vlm_loss: 0.0000, loss: 130.9331, grad_norm: nan
2025-03-01 07:08:08,963 - mmdet - INFO - Iter [11150/168780]	lr: 3.957e-04, eta: 5 days, 11:32:27, time: 3.039, data_time: 0.018, memory: 22041, loss_cls: 1.5094, loss_bbox: 2.8511, d0.loss_cls: 1.5094, d0.loss_bbox: 2.8511, d1.loss_cls: 1.5094, d1.loss_bbox: 2.8511, d2.loss_cls: 1.5094, d2.loss_bbox: 2.8511, d3.loss_cls: 1.5094, d3.loss_bbox: 2.8511, d4.loss_cls: 1.5094, d4.loss_bbox: 2.8511, dn_loss_cls: 1.0070, dn_loss_bbox: 2.5436, d0.dn_loss_cls: 1.0070, d0.dn_loss_bbox: 2.5436, d1.dn_loss_cls: 1.0070, d1.dn_loss_bbox: 2.5436, d2.dn_loss_cls: 1.0070, d2.dn_loss_bbox: 2.5436, d3.dn_loss_cls: 1.0070, d3.dn_loss_bbox: 2.5436, d4.dn_loss_cls: 1.0070, d4.dn_loss_bbox: 2.5436, loss_cls_lane: 0.6452, loss_cls_H: 0.6452, loss_bbox_lane: 6.3639, loss_bbox_H: 6.3725, d0.loss_cls_lane: 0.6452, d0.loss_cls_H: 0.6452, d0.loss_bbox_lane: 6.3639, d0.loss_bbox_H: 6.3725, d1.loss_cls_lane: 0.6452, d1.loss_cls_H: 0.6452, d1.loss_bbox_lane: 6.3639, d1.loss_bbox_H: 6.3725, d2.loss_cls_lane: 0.6452, d2.loss_cls_H: 0.6452, d2.loss_bbox_lane: 6.3639, d2.loss_bbox_H: 6.3725, d3.loss_cls_lane: 0.6452, d3.loss_cls_H: 0.6452, d3.loss_bbox_lane: 6.3639, d3.loss_bbox_H: 6.3725, d4.loss_cls_lane: 0.6452, d4.loss_cls_H: 0.6452, d4.loss_bbox_lane: 6.3639, d4.loss_bbox_H: 6.3725, vlm_loss: 0.0000, loss: 131.6278, grad_norm: nan
2025-03-01 07:10:37,574 - mmdet - INFO - Iter [11200/168780]	lr: 3.957e-04, eta: 5 days, 11:29:35, time: 2.972, data_time: 0.017, memory: 22041, loss_cls: 1.7004, loss_bbox: 3.1515, d0.loss_cls: 1.7004, d0.loss_bbox: 3.1515, d1.loss_cls: 1.7004, d1.loss_bbox: 3.1515, d2.loss_cls: 1.7004, d2.loss_bbox: 3.1515, d3.loss_cls: 1.7004, d3.loss_bbox: 3.1515, d4.loss_cls: 1.7004, d4.loss_bbox: 3.1515, dn_loss_cls: 0.9452, dn_loss_bbox: 2.8521, d0.dn_loss_cls: 0.9452, d0.dn_loss_bbox: 2.8521, d1.dn_loss_cls: 0.9452, d1.dn_loss_bbox: 2.8521, d2.dn_loss_cls: 0.9452, d2.dn_loss_bbox: 2.8521, d3.dn_loss_cls: 0.9452, d3.dn_loss_bbox: 2.8521, d4.dn_loss_cls: 0.9452, d4.dn_loss_bbox: 2.8521, loss_cls_lane: 0.6459, loss_cls_H: 0.6459, loss_bbox_lane: 6.5150, loss_bbox_H: 6.5141, d0.loss_cls_lane: 0.6459, d0.loss_cls_H: 0.6459, d0.loss_bbox_lane: 6.5150, d0.loss_bbox_H: 6.5141, d1.loss_cls_lane: 0.6459, d1.loss_cls_H: 0.6459, d1.loss_bbox_lane: 6.5150, d1.loss_bbox_H: 6.5141, d2.loss_cls_lane: 0.6459, d2.loss_cls_H: 0.6459, d2.loss_bbox_lane: 6.5150, d2.loss_bbox_H: 6.5141, d3.loss_cls_lane: 0.6459, d3.loss_cls_H: 0.6459, d3.loss_bbox_lane: 6.5150, d3.loss_bbox_H: 6.5141, d4.loss_cls_lane: 0.6459, d4.loss_cls_H: 0.6459, d4.loss_bbox_lane: 6.5150, d4.loss_bbox_H: 6.5141, vlm_loss: 0.0000, loss: 137.8203, grad_norm: nan
2025-03-01 07:13:05,376 - mmdet - INFO - Iter [11250/168780]	lr: 3.956e-04, eta: 5 days, 11:26:31, time: 2.956, data_time: 0.016, memory: 22041, loss_cls: 2.1173, loss_bbox: 2.8815, d0.loss_cls: 2.1173, d0.loss_bbox: 2.8815, d1.loss_cls: 2.1173, d1.loss_bbox: 2.8815, d2.loss_cls: 2.1173, d2.loss_bbox: 2.8815, d3.loss_cls: 2.1173, d3.loss_bbox: 2.8815, d4.loss_cls: 2.1173, d4.loss_bbox: 2.8815, dn_loss_cls: 0.8965, dn_loss_bbox: 2.5108, d0.dn_loss_cls: 0.8965, d0.dn_loss_bbox: 2.5108, d1.dn_loss_cls: 0.8965, d1.dn_loss_bbox: 2.5108, d2.dn_loss_cls: 0.8965, d2.dn_loss_bbox: 2.5108, d3.dn_loss_cls: 0.8965, d3.dn_loss_bbox: 2.5108, d4.dn_loss_cls: 0.8965, d4.dn_loss_bbox: 2.5108, loss_cls_lane: 0.6401, loss_cls_H: 0.6401, loss_bbox_lane: 6.6231, loss_bbox_H: 6.6045, d0.loss_cls_lane: 0.6401, d0.loss_cls_H: 0.6401, d0.loss_bbox_lane: 6.6231, d0.loss_bbox_H: 6.6045, d1.loss_cls_lane: 0.6401, d1.loss_cls_H: 0.6401, d1.loss_bbox_lane: 6.6231, d1.loss_bbox_H: 6.6045, d2.loss_cls_lane: 0.6401, d2.loss_cls_H: 0.6401, d2.loss_bbox_lane: 6.6231, d2.loss_bbox_H: 6.6045, d3.loss_cls_lane: 0.6401, d3.loss_cls_H: 0.6401, d3.loss_bbox_lane: 6.6231, d3.loss_bbox_H: 6.6045, d4.loss_cls_lane: 0.6401, d4.loss_cls_H: 0.6401, d4.loss_bbox_lane: 6.6231, d4.loss_bbox_H: 6.6045, vlm_loss: 0.0000, loss: 137.4827, grad_norm: nan
2025-03-01 07:15:32,203 - mmdet - INFO - Iter [11300/168780]	lr: 3.956e-04, eta: 5 days, 11:23:14, time: 2.937, data_time: 0.018, memory: 22041, loss_cls: 1.4050, loss_bbox: 2.6807, d0.loss_cls: 1.4050, d0.loss_bbox: 2.6807, d1.loss_cls: 1.4050, d1.loss_bbox: 2.6807, d2.loss_cls: 1.4050, d2.loss_bbox: 2.6807, d3.loss_cls: 1.4050, d3.loss_bbox: 2.6807, d4.loss_cls: 1.4050, d4.loss_bbox: 2.6807, dn_loss_cls: 0.9647, dn_loss_bbox: 2.2909, d0.dn_loss_cls: 0.9647, d0.dn_loss_bbox: 2.2909, d1.dn_loss_cls: 0.9647, d1.dn_loss_bbox: 2.2909, d2.dn_loss_cls: 0.9647, d2.dn_loss_bbox: 2.2909, d3.dn_loss_cls: 0.9647, d3.dn_loss_bbox: 2.2909, d4.dn_loss_cls: 0.9647, d4.dn_loss_bbox: 2.2909, loss_cls_lane: 0.6515, loss_cls_H: 0.7590, loss_bbox_lane: 6.3072, loss_bbox_H: 6.3012, d0.loss_cls_lane: 0.6515, d0.loss_cls_H: 0.7590, d0.loss_bbox_lane: 6.3072, d0.loss_bbox_H: 6.3012, d1.loss_cls_lane: 0.6515, d1.loss_cls_H: 0.7590, d1.loss_bbox_lane: 6.3072, d1.loss_bbox_H: 6.3012, d2.loss_cls_lane: 0.6515, d2.loss_cls_H: 0.7590, d2.loss_bbox_lane: 6.3072, d2.loss_bbox_H: 6.3012, d3.loss_cls_lane: 0.6515, d3.loss_cls_H: 0.7590, d3.loss_bbox_lane: 6.3072, d3.loss_bbox_H: 6.3012, d4.loss_cls_lane: 0.6515, d4.loss_cls_H: 0.7590, d4.loss_bbox_lane: 6.3072, d4.loss_bbox_H: 6.3012, vlm_loss: 0.0000, loss: 128.1616, grad_norm: nan
2025-03-01 07:18:00,451 - mmdet - INFO - Iter [11350/168780]	lr: 3.956e-04, eta: 5 days, 11:20:17, time: 2.965, data_time: 0.018, memory: 22041, loss_cls: 1.6066, loss_bbox: 2.9770, d0.loss_cls: 1.6066, d0.loss_bbox: 2.9770, d1.loss_cls: 1.6066, d1.loss_bbox: 2.9770, d2.loss_cls: 1.6066, d2.loss_bbox: 2.9770, d3.loss_cls: 1.6066, d3.loss_bbox: 2.9770, d4.loss_cls: 1.6066, d4.loss_bbox: 2.9770, dn_loss_cls: 0.9714, dn_loss_bbox: 2.6265, d0.dn_loss_cls: 0.9714, d0.dn_loss_bbox: 2.6265, d1.dn_loss_cls: 0.9714, d1.dn_loss_bbox: 2.6265, d2.dn_loss_cls: 0.9714, d2.dn_loss_bbox: 2.6265, d3.dn_loss_cls: 0.9714, d3.dn_loss_bbox: 2.6265, d4.dn_loss_cls: 0.9714, d4.dn_loss_bbox: 2.6265, loss_cls_lane: 0.6371, loss_cls_H: 0.6371, loss_bbox_lane: 6.5920, loss_bbox_H: 6.5854, d0.loss_cls_lane: 0.6371, d0.loss_cls_H: 0.6371, d0.loss_bbox_lane: 6.5920, d0.loss_bbox_H: 6.5854, d1.loss_cls_lane: 0.6371, d1.loss_cls_H: 0.6371, d1.loss_bbox_lane: 6.5920, d1.loss_bbox_H: 6.5854, d2.loss_cls_lane: 0.6371, d2.loss_cls_H: 0.6371, d2.loss_bbox_lane: 6.5920, d2.loss_bbox_H: 6.5854, d3.loss_cls_lane: 0.6371, d3.loss_cls_H: 0.6371, d3.loss_bbox_lane: 6.5920, d3.loss_bbox_H: 6.5854, d4.loss_cls_lane: 0.6371, d4.loss_cls_H: 0.6371, d4.loss_bbox_lane: 6.5920, d4.loss_bbox_H: 6.5854, vlm_loss: 0.0000, loss: 135.7981, grad_norm: nan
2025-03-01 07:20:28,894 - mmdet - INFO - Iter [11400/168780]	lr: 3.955e-04, eta: 5 days, 11:17:23, time: 2.969, data_time: 0.016, memory: 22041, loss_cls: 1.8906, loss_bbox: 2.7596, d0.loss_cls: 1.8906, d0.loss_bbox: 2.7596, d1.loss_cls: 1.8906, d1.loss_bbox: 2.7596, d2.loss_cls: 1.8906, d2.loss_bbox: 2.7596, d3.loss_cls: 1.8906, d3.loss_bbox: 2.7596, d4.loss_cls: 1.8906, d4.loss_bbox: 2.7596, dn_loss_cls: 0.9388, dn_loss_bbox: 2.3621, d0.dn_loss_cls: 0.9388, d0.dn_loss_bbox: 2.3621, d1.dn_loss_cls: 0.9388, d1.dn_loss_bbox: 2.3621, d2.dn_loss_cls: 0.9388, d2.dn_loss_bbox: 2.3621, d3.dn_loss_cls: 0.9388, d3.dn_loss_bbox: 2.3621, d4.dn_loss_cls: 0.9388, d4.dn_loss_bbox: 2.3621, loss_cls_lane: 0.6664, loss_cls_H: 0.6664, loss_bbox_lane: 6.6315, loss_bbox_H: 6.6317, d0.loss_cls_lane: 0.6664, d0.loss_cls_H: 0.6664, d0.loss_bbox_lane: 6.6315, d0.loss_bbox_H: 6.6317, d1.loss_cls_lane: 0.6664, d1.loss_cls_H: 0.6664, d1.loss_bbox_lane: 6.6315, d1.loss_bbox_H: 6.6317, d2.loss_cls_lane: 0.6664, d2.loss_cls_H: 0.6664, d2.loss_bbox_lane: 6.6315, d2.loss_bbox_H: 6.6317, d3.loss_cls_lane: 0.6664, d3.loss_cls_H: 0.6664, d3.loss_bbox_lane: 6.6315, d3.loss_bbox_H: 6.6317, d4.loss_cls_lane: 0.6664, d4.loss_cls_H: 0.6664, d4.loss_bbox_lane: 6.6315, d4.loss_bbox_H: 6.6317, vlm_loss: 0.0000, loss: 135.2827, grad_norm: nan
2025-03-01 07:22:57,239 - mmdet - INFO - Iter [11450/168780]	lr: 3.955e-04, eta: 5 days, 11:14:28, time: 2.967, data_time: 0.017, memory: 22041, loss_cls: 1.4278, loss_bbox: 3.0026, d0.loss_cls: 1.4278, d0.loss_bbox: 3.0026, d1.loss_cls: 1.4278, d1.loss_bbox: 3.0026, d2.loss_cls: 1.4278, d2.loss_bbox: 3.0026, d3.loss_cls: 1.4278, d3.loss_bbox: 3.0026, d4.loss_cls: 1.4278, d4.loss_bbox: 3.0026, dn_loss_cls: 0.9509, dn_loss_bbox: 2.7419, d0.dn_loss_cls: 0.9509, d0.dn_loss_bbox: 2.7419, d1.dn_loss_cls: 0.9509, d1.dn_loss_bbox: 2.7419, d2.dn_loss_cls: 0.9509, d2.dn_loss_bbox: 2.7419, d3.dn_loss_cls: 0.9509, d3.dn_loss_bbox: 2.7419, d4.dn_loss_cls: 0.9509, d4.dn_loss_bbox: 2.7419, loss_cls_lane: 0.6433, loss_cls_H: 0.6433, loss_bbox_lane: 6.5732, loss_bbox_H: 6.5632, d0.loss_cls_lane: 0.6433, d0.loss_cls_H: 0.6433, d0.loss_bbox_lane: 6.5732, d0.loss_bbox_H: 6.5632, d1.loss_cls_lane: 0.6433, d1.loss_cls_H: 0.6433, d1.loss_bbox_lane: 6.5732, d1.loss_bbox_H: 6.5632, d2.loss_cls_lane: 0.6433, d2.loss_cls_H: 0.6433, d2.loss_bbox_lane: 6.5732, d2.loss_bbox_H: 6.5632, d3.loss_cls_lane: 0.6433, d3.loss_cls_H: 0.6433, d3.loss_bbox_lane: 6.5732, d3.loss_bbox_H: 6.5632, d4.loss_cls_lane: 0.6433, d4.loss_cls_H: 0.6433, d4.loss_bbox_lane: 6.5732, d4.loss_bbox_H: 6.5632, vlm_loss: 0.0000, loss: 135.2778, grad_norm: nan
2025-03-01 07:25:26,589 - mmdet - INFO - Iter [11500/168780]	lr: 3.954e-04, eta: 5 days, 11:11:47, time: 2.987, data_time: 0.018, memory: 22041, loss_cls: 1.6003, loss_bbox: 2.7980, d0.loss_cls: 1.6003, d0.loss_bbox: 2.7980, d1.loss_cls: 1.6003, d1.loss_bbox: 2.7980, d2.loss_cls: 1.6003, d2.loss_bbox: 2.7980, d3.loss_cls: 1.6003, d3.loss_bbox: 2.7980, d4.loss_cls: 1.6003, d4.loss_bbox: 2.7980, dn_loss_cls: 0.9850, dn_loss_bbox: 2.3508, d0.dn_loss_cls: 0.9850, d0.dn_loss_bbox: 2.3508, d1.dn_loss_cls: 0.9850, d1.dn_loss_bbox: 2.3508, d2.dn_loss_cls: 0.9850, d2.dn_loss_bbox: 2.3508, d3.dn_loss_cls: 0.9850, d3.dn_loss_bbox: 2.3508, d4.dn_loss_cls: 0.9850, d4.dn_loss_bbox: 2.3508, loss_cls_lane: 0.6430, loss_cls_H: 0.6430, loss_bbox_lane: 6.4488, loss_bbox_H: 6.4434, d0.loss_cls_lane: 0.6430, d0.loss_cls_H: 0.6430, d0.loss_bbox_lane: 6.4488, d0.loss_bbox_H: 6.4434, d1.loss_cls_lane: 0.6430, d1.loss_cls_H: 0.6430, d1.loss_bbox_lane: 6.4488, d1.loss_bbox_H: 6.4434, d2.loss_cls_lane: 0.6430, d2.loss_cls_H: 0.6430, d2.loss_bbox_lane: 6.4488, d2.loss_bbox_H: 6.4434, d3.loss_cls_lane: 0.6430, d3.loss_cls_H: 0.6430, d3.loss_bbox_lane: 6.4488, d3.loss_bbox_H: 6.4434, d4.loss_cls_lane: 0.6430, d4.loss_cls_H: 0.6430, d4.loss_bbox_lane: 6.4488, d4.loss_bbox_H: 6.4434, vlm_loss: 0.0000, loss: 131.4745, grad_norm: nan
2025-03-01 07:27:53,686 - mmdet - INFO - Iter [11550/168780]	lr: 3.954e-04, eta: 5 days, 11:08:35, time: 2.942, data_time: 0.017, memory: 22041, loss_cls: 2.5287, loss_bbox: 2.9402, d0.loss_cls: 2.5287, d0.loss_bbox: 2.9402, d1.loss_cls: 2.5287, d1.loss_bbox: 2.9402, d2.loss_cls: 2.5287, d2.loss_bbox: 2.9402, d3.loss_cls: 2.5287, d3.loss_bbox: 2.9402, d4.loss_cls: 2.5287, d4.loss_bbox: 2.9402, dn_loss_cls: 0.8811, dn_loss_bbox: 2.6148, d0.dn_loss_cls: 0.8811, d0.dn_loss_bbox: 2.6148, d1.dn_loss_cls: 0.8811, d1.dn_loss_bbox: 2.6148, d2.dn_loss_cls: 0.8811, d2.dn_loss_bbox: 2.6148, d3.dn_loss_cls: 0.8811, d3.dn_loss_bbox: 2.6148, d4.dn_loss_cls: 0.8811, d4.dn_loss_bbox: 2.6148, loss_cls_lane: 0.6510, loss_cls_H: 0.6510, loss_bbox_lane: 6.7558, loss_bbox_H: 6.7484, d0.loss_cls_lane: 0.6510, d0.loss_cls_H: 0.6510, d0.loss_bbox_lane: 6.7558, d0.loss_bbox_H: 6.7484, d1.loss_cls_lane: 0.6510, d1.loss_cls_H: 0.6510, d1.loss_bbox_lane: 6.7558, d1.loss_bbox_H: 6.7484, d2.loss_cls_lane: 0.6510, d2.loss_cls_H: 0.6510, d2.loss_bbox_lane: 6.7558, d2.loss_bbox_H: 6.7484, d3.loss_cls_lane: 0.6510, d3.loss_cls_H: 0.6510, d3.loss_bbox_lane: 6.7558, d3.loss_bbox_H: 6.7484, d4.loss_cls_lane: 0.6510, d4.loss_cls_H: 0.6510, d4.loss_bbox_lane: 6.7558, d4.loss_bbox_H: 6.7484, vlm_loss: 0.0000, loss: 142.6260, grad_norm: nan
2025-03-01 07:30:22,487 - mmdet - INFO - Iter [11600/168780]	lr: 3.954e-04, eta: 5 days, 11:05:47, time: 2.976, data_time: 0.018, memory: 22041, loss_cls: 1.4964, loss_bbox: 2.8719, d0.loss_cls: 1.4964, d0.loss_bbox: 2.8719, d1.loss_cls: 1.4964, d1.loss_bbox: 2.8719, d2.loss_cls: 1.4964, d2.loss_bbox: 2.8719, d3.loss_cls: 1.4964, d3.loss_bbox: 2.8719, d4.loss_cls: 1.4964, d4.loss_bbox: 2.8719, dn_loss_cls: 0.8725, dn_loss_bbox: 2.5376, d0.dn_loss_cls: 0.8725, d0.dn_loss_bbox: 2.5376, d1.dn_loss_cls: 0.8725, d1.dn_loss_bbox: 2.5376, d2.dn_loss_cls: 0.8725, d2.dn_loss_bbox: 2.5376, d3.dn_loss_cls: 0.8725, d3.dn_loss_bbox: 2.5376, d4.dn_loss_cls: 0.8725, d4.dn_loss_bbox: 2.5376, loss_cls_lane: 0.6334, loss_cls_H: 0.6334, loss_bbox_lane: 6.5524, loss_bbox_H: 6.5522, d0.loss_cls_lane: 0.6334, d0.loss_cls_H: 0.6334, d0.loss_bbox_lane: 6.5524, d0.loss_bbox_H: 6.5522, d1.loss_cls_lane: 0.6334, d1.loss_cls_H: 0.6334, d1.loss_bbox_lane: 6.5524, d1.loss_bbox_H: 6.5522, d2.loss_cls_lane: 0.6334, d2.loss_cls_H: 0.6334, d2.loss_bbox_lane: 6.5524, d2.loss_bbox_H: 6.5522, d3.loss_cls_lane: 0.6334, d3.loss_cls_H: 0.6334, d3.loss_bbox_lane: 6.5524, d3.loss_bbox_H: 6.5522, d4.loss_cls_lane: 0.6334, d4.loss_cls_H: 0.6334, d4.loss_bbox_lane: 6.5524, d4.loss_bbox_H: 6.5522, vlm_loss: 0.0000, loss: 132.8984, grad_norm: nan
2025-03-01 07:32:49,348 - mmdet - INFO - Iter [11650/168780]	lr: 3.953e-04, eta: 5 days, 11:02:33, time: 2.937, data_time: 0.017, memory: 22041, loss_cls: 1.9792, loss_bbox: 2.7804, d0.loss_cls: 1.9792, d0.loss_bbox: 2.7804, d1.loss_cls: 1.9792, d1.loss_bbox: 2.7804, d2.loss_cls: 1.9792, d2.loss_bbox: 2.7804, d3.loss_cls: 1.9792, d3.loss_bbox: 2.7804, d4.loss_cls: 1.9792, d4.loss_bbox: 2.7804, dn_loss_cls: 0.9717, dn_loss_bbox: 2.4106, d0.dn_loss_cls: 0.9717, d0.dn_loss_bbox: 2.4106, d1.dn_loss_cls: 0.9717, d1.dn_loss_bbox: 2.4106, d2.dn_loss_cls: 0.9717, d2.dn_loss_bbox: 2.4106, d3.dn_loss_cls: 0.9717, d3.dn_loss_bbox: 2.4106, d4.dn_loss_cls: 0.9717, d4.dn_loss_bbox: 2.4106, loss_cls_lane: 0.6382, loss_cls_H: 0.6382, loss_bbox_lane: 6.4519, loss_bbox_H: 6.4561, d0.loss_cls_lane: 0.6382, d0.loss_cls_H: 0.6382, d0.loss_bbox_lane: 6.4519, d0.loss_bbox_H: 6.4561, d1.loss_cls_lane: 0.6382, d1.loss_cls_H: 0.6382, d1.loss_bbox_lane: 6.4519, d1.loss_bbox_H: 6.4561, d2.loss_cls_lane: 0.6382, d2.loss_cls_H: 0.6382, d2.loss_bbox_lane: 6.4519, d2.loss_bbox_H: 6.4561, d3.loss_cls_lane: 0.6382, d3.loss_cls_H: 0.6382, d3.loss_bbox_lane: 6.4519, d3.loss_bbox_H: 6.4561, d4.loss_cls_lane: 0.6382, d4.loss_cls_H: 0.6382, d4.loss_bbox_lane: 6.4519, d4.loss_bbox_H: 6.4561, vlm_loss: 0.0000, loss: 133.9582, grad_norm: nan
2025-03-01 07:35:19,158 - mmdet - INFO - Iter [11700/168780]	lr: 3.953e-04, eta: 5 days, 10:59:58, time: 2.996, data_time: 0.017, memory: 22041, loss_cls: 1.6794, loss_bbox: 3.1627, d0.loss_cls: 1.6794, d0.loss_bbox: 3.1627, d1.loss_cls: 1.6794, d1.loss_bbox: 3.1627, d2.loss_cls: 1.6794, d2.loss_bbox: 3.1627, d3.loss_cls: 1.6794, d3.loss_bbox: 3.1627, d4.loss_cls: 1.6794, d4.loss_bbox: 3.1627, dn_loss_cls: 0.9351, dn_loss_bbox: 2.7919, d0.dn_loss_cls: 0.9351, d0.dn_loss_bbox: 2.7919, d1.dn_loss_cls: 0.9351, d1.dn_loss_bbox: 2.7919, d2.dn_loss_cls: 0.9351, d2.dn_loss_bbox: 2.7919, d3.dn_loss_cls: 0.9351, d3.dn_loss_bbox: 2.7919, d4.dn_loss_cls: 0.9351, d4.dn_loss_bbox: 2.7919, loss_cls_lane: 0.6354, loss_cls_H: 0.6354, loss_bbox_lane: 6.4974, loss_bbox_H: 6.4955, d0.loss_cls_lane: 0.6354, d0.loss_cls_H: 0.6354, d0.loss_bbox_lane: 6.4974, d0.loss_bbox_H: 6.4955, d1.loss_cls_lane: 0.6354, d1.loss_cls_H: 0.6354, d1.loss_bbox_lane: 6.4974, d1.loss_bbox_H: 6.4955, d2.loss_cls_lane: 0.6354, d2.loss_cls_H: 0.6354, d2.loss_bbox_lane: 6.4974, d2.loss_bbox_H: 6.4955, d3.loss_cls_lane: 0.6354, d3.loss_cls_H: 0.6354, d3.loss_bbox_lane: 6.4974, d3.loss_bbox_H: 6.4955, d4.loss_cls_lane: 0.6354, d4.loss_cls_H: 0.6354, d4.loss_bbox_lane: 6.4974, d4.loss_bbox_H: 6.4955, vlm_loss: 0.0000, loss: 136.9965, grad_norm: nan
2025-03-01 07:37:46,478 - mmdet - INFO - Iter [11750/168780]	lr: 3.952e-04, eta: 5 days, 10:56:51, time: 2.946, data_time: 0.018, memory: 22041, loss_cls: 1.8141, loss_bbox: 2.6245, d0.loss_cls: 1.8141, d0.loss_bbox: 2.6245, d1.loss_cls: 1.8141, d1.loss_bbox: 2.6245, d2.loss_cls: 1.8141, d2.loss_bbox: 2.6245, d3.loss_cls: 1.8141, d3.loss_bbox: 2.6245, d4.loss_cls: 1.8141, d4.loss_bbox: 2.6245, dn_loss_cls: 1.3316, dn_loss_bbox: 2.1383, d0.dn_loss_cls: 1.3316, d0.dn_loss_bbox: 2.1383, d1.dn_loss_cls: 1.3316, d1.dn_loss_bbox: 2.1383, d2.dn_loss_cls: 1.3316, d2.dn_loss_bbox: 2.1383, d3.dn_loss_cls: 1.3316, d3.dn_loss_bbox: 2.1383, d4.dn_loss_cls: 1.3316, d4.dn_loss_bbox: 2.1383, loss_cls_lane: 0.6414, loss_cls_H: 0.6414, loss_bbox_lane: 6.7510, loss_bbox_H: 6.7374, d0.loss_cls_lane: 0.6414, d0.loss_cls_H: 0.6414, d0.loss_bbox_lane: 6.7510, d0.loss_bbox_H: 6.7374, d1.loss_cls_lane: 0.6414, d1.loss_cls_H: 0.6414, d1.loss_bbox_lane: 6.7510, d1.loss_bbox_H: 6.7374, d2.loss_cls_lane: 0.6414, d2.loss_cls_H: 0.6414, d2.loss_bbox_lane: 6.7510, d2.loss_bbox_H: 6.7374, d3.loss_cls_lane: 0.6414, d3.loss_cls_H: 0.6414, d3.loss_bbox_lane: 6.7510, d3.loss_bbox_H: 6.7374, d4.loss_cls_lane: 0.6414, d4.loss_cls_H: 0.6414, d4.loss_bbox_lane: 6.7510, d4.loss_bbox_H: 6.7374, vlm_loss: 0.0000, loss: 136.0795, grad_norm: nan
2025-03-01 07:40:15,841 - mmdet - INFO - Iter [11800/168780]	lr: 3.952e-04, eta: 5 days, 10:54:11, time: 2.987, data_time: 0.017, memory: 22041, loss_cls: 1.4321, loss_bbox: 2.6455, d0.loss_cls: 1.4321, d0.loss_bbox: 2.6455, d1.loss_cls: 1.4321, d1.loss_bbox: 2.6455, d2.loss_cls: 1.4321, d2.loss_bbox: 2.6455, d3.loss_cls: 1.4321, d3.loss_bbox: 2.6455, d4.loss_cls: 1.4321, d4.loss_bbox: 2.6455, dn_loss_cls: 0.9691, dn_loss_bbox: 2.4973, d0.dn_loss_cls: 0.9691, d0.dn_loss_bbox: 2.4973, d1.dn_loss_cls: 0.9691, d1.dn_loss_bbox: 2.4973, d2.dn_loss_cls: 0.9691, d2.dn_loss_bbox: 2.4973, d3.dn_loss_cls: 0.9691, d3.dn_loss_bbox: 2.4973, d4.dn_loss_cls: 0.9691, d4.dn_loss_bbox: 2.4973, loss_cls_lane: 0.6418, loss_cls_H: 0.6418, loss_bbox_lane: 6.5233, loss_bbox_H: 6.5196, d0.loss_cls_lane: 0.6418, d0.loss_cls_H: 0.6418, d0.loss_bbox_lane: 6.5233, d0.loss_bbox_H: 6.5196, d1.loss_cls_lane: 0.6418, d1.loss_cls_H: 0.6418, d1.loss_bbox_lane: 6.5233, d1.loss_bbox_H: 6.5196, d2.loss_cls_lane: 0.6418, d2.loss_cls_H: 0.6418, d2.loss_bbox_lane: 6.5233, d2.loss_bbox_H: 6.5196, d3.loss_cls_lane: 0.6418, d3.loss_cls_H: 0.6418, d3.loss_bbox_lane: 6.5233, d3.loss_bbox_H: 6.5196, d4.loss_cls_lane: 0.6418, d4.loss_cls_H: 0.6418, d4.loss_bbox_lane: 6.5233, d4.loss_bbox_H: 6.5196, vlm_loss: 0.0000, loss: 131.2226, grad_norm: nan
2025-03-01 07:42:44,074 - mmdet - INFO - Iter [11850/168780]	lr: 3.952e-04, eta: 5 days, 10:51:16, time: 2.965, data_time: 0.017, memory: 22041, loss_cls: 1.8217, loss_bbox: 2.7820, d0.loss_cls: 1.8217, d0.loss_bbox: 2.7820, d1.loss_cls: 1.8217, d1.loss_bbox: 2.7820, d2.loss_cls: 1.8217, d2.loss_bbox: 2.7820, d3.loss_cls: 1.8217, d3.loss_bbox: 2.7820, d4.loss_cls: 1.8217, d4.loss_bbox: 2.7820, dn_loss_cls: 0.8812, dn_loss_bbox: 2.4612, d0.dn_loss_cls: 0.8812, d0.dn_loss_bbox: 2.4612, d1.dn_loss_cls: 0.8812, d1.dn_loss_bbox: 2.4612, d2.dn_loss_cls: 0.8812, d2.dn_loss_bbox: 2.4612, d3.dn_loss_cls: 0.8812, d3.dn_loss_bbox: 2.4612, d4.dn_loss_cls: 0.8812, d4.dn_loss_bbox: 2.4612, loss_cls_lane: 0.6417, loss_cls_H: 0.6417, loss_bbox_lane: 6.3501, loss_bbox_H: 6.3469, d0.loss_cls_lane: 0.6417, d0.loss_cls_H: 0.6417, d0.loss_bbox_lane: 6.3501, d0.loss_bbox_H: 6.3469, d1.loss_cls_lane: 0.6417, d1.loss_cls_H: 0.6417, d1.loss_bbox_lane: 6.3501, d1.loss_bbox_H: 6.3469, d2.loss_cls_lane: 0.6417, d2.loss_cls_H: 0.6417, d2.loss_bbox_lane: 6.3501, d2.loss_bbox_H: 6.3469, d3.loss_cls_lane: 0.6417, d3.loss_cls_H: 0.6417, d3.loss_bbox_lane: 6.3501, d3.loss_bbox_H: 6.3469, d4.loss_cls_lane: 0.6417, d4.loss_cls_H: 0.6417, d4.loss_bbox_lane: 6.3501, d4.loss_bbox_H: 6.3469, vlm_loss: 0.0000, loss: 131.5594, grad_norm: nan
2025-03-01 07:45:12,952 - mmdet - INFO - Iter [11900/168780]	lr: 3.951e-04, eta: 5 days, 10:48:30, time: 2.978, data_time: 0.017, memory: 22041, loss_cls: 1.5325, loss_bbox: 2.6484, d0.loss_cls: 1.5325, d0.loss_bbox: 2.6484, d1.loss_cls: 1.5325, d1.loss_bbox: 2.6484, d2.loss_cls: 1.5325, d2.loss_bbox: 2.6484, d3.loss_cls: 1.5325, d3.loss_bbox: 2.6484, d4.loss_cls: 1.5325, d4.loss_bbox: 2.6484, dn_loss_cls: 0.8570, dn_loss_bbox: 2.2208, d0.dn_loss_cls: 0.8570, d0.dn_loss_bbox: 2.2208, d1.dn_loss_cls: 0.8570, d1.dn_loss_bbox: 2.2208, d2.dn_loss_cls: 0.8570, d2.dn_loss_bbox: 2.2208, d3.dn_loss_cls: 0.8570, d3.dn_loss_bbox: 2.2208, d4.dn_loss_cls: 0.8570, d4.dn_loss_bbox: 2.2208, loss_cls_lane: 0.6437, loss_cls_H: 0.6437, loss_bbox_lane: 6.5676, loss_bbox_H: 6.5720, d0.loss_cls_lane: 0.6437, d0.loss_cls_H: 0.6437, d0.loss_bbox_lane: 6.5676, d0.loss_bbox_H: 6.5720, d1.loss_cls_lane: 0.6437, d1.loss_cls_H: 0.6437, d1.loss_bbox_lane: 6.5676, d1.loss_bbox_H: 6.5720, d2.loss_cls_lane: 0.6437, d2.loss_cls_H: 0.6437, d2.loss_bbox_lane: 6.5676, d2.loss_bbox_H: 6.5720, d3.loss_cls_lane: 0.6437, d3.loss_cls_H: 0.6437, d3.loss_bbox_lane: 6.5676, d3.loss_bbox_H: 6.5720, d4.loss_cls_lane: 0.6437, d4.loss_cls_H: 0.6437, d4.loss_bbox_lane: 6.5676, d4.loss_bbox_H: 6.5720, vlm_loss: 0.0000, loss: 130.1137, grad_norm: nan
2025-03-01 07:47:41,477 - mmdet - INFO - Iter [11950/168780]	lr: 3.951e-04, eta: 5 days, 10:45:39, time: 2.971, data_time: 0.018, memory: 22041, loss_cls: 2.4684, loss_bbox: 2.7752, d0.loss_cls: 2.4684, d0.loss_bbox: 2.7752, d1.loss_cls: 2.4684, d1.loss_bbox: 2.7752, d2.loss_cls: 2.4684, d2.loss_bbox: 2.7752, d3.loss_cls: 2.4684, d3.loss_bbox: 2.7752, d4.loss_cls: 2.4684, d4.loss_bbox: 2.7752, dn_loss_cls: 0.8965, dn_loss_bbox: 2.4266, d0.dn_loss_cls: 0.8965, d0.dn_loss_bbox: 2.4266, d1.dn_loss_cls: 0.8965, d1.dn_loss_bbox: 2.4266, d2.dn_loss_cls: 0.8965, d2.dn_loss_bbox: 2.4266, d3.dn_loss_cls: 0.8965, d3.dn_loss_bbox: 2.4266, d4.dn_loss_cls: 0.8965, d4.dn_loss_bbox: 2.4266, loss_cls_lane: 0.6300, loss_cls_H: 0.6300, loss_bbox_lane: 6.5621, loss_bbox_H: 6.5465, d0.loss_cls_lane: 0.6300, d0.loss_cls_H: 0.6300, d0.loss_bbox_lane: 6.5621, d0.loss_bbox_H: 6.5465, d1.loss_cls_lane: 0.6300, d1.loss_cls_H: 0.6300, d1.loss_bbox_lane: 6.5621, d1.loss_bbox_H: 6.5465, d2.loss_cls_lane: 0.6300, d2.loss_cls_H: 0.6300, d2.loss_bbox_lane: 6.5621, d2.loss_bbox_H: 6.5465, d3.loss_cls_lane: 0.6300, d3.loss_cls_H: 0.6300, d3.loss_bbox_lane: 6.5621, d3.loss_bbox_H: 6.5465, d4.loss_cls_lane: 0.6300, d4.loss_cls_H: 0.6300, d4.loss_bbox_lane: 6.5621, d4.loss_bbox_H: 6.5465, vlm_loss: 0.0000, loss: 137.6118, grad_norm: nan
2025-03-01 07:50:09,942 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 07:50:09,942 - mmdet - INFO - Iter [12000/168780]	lr: 3.950e-04, eta: 5 days, 10:42:48, time: 2.969, data_time: 0.017, memory: 22041, loss_cls: 2.0636, loss_bbox: 3.1864, d0.loss_cls: 2.0636, d0.loss_bbox: 3.1864, d1.loss_cls: 2.0636, d1.loss_bbox: 3.1864, d2.loss_cls: 2.0636, d2.loss_bbox: 3.1864, d3.loss_cls: 2.0636, d3.loss_bbox: 3.1864, d4.loss_cls: 2.0636, d4.loss_bbox: 3.1864, dn_loss_cls: 0.9385, dn_loss_bbox: 2.9040, d0.dn_loss_cls: 0.9385, d0.dn_loss_bbox: 2.9040, d1.dn_loss_cls: 0.9385, d1.dn_loss_bbox: 2.9040, d2.dn_loss_cls: 0.9385, d2.dn_loss_bbox: 2.9040, d3.dn_loss_cls: 0.9385, d3.dn_loss_bbox: 2.9040, d4.dn_loss_cls: 0.9385, d4.dn_loss_bbox: 2.9040, loss_cls_lane: 0.6318, loss_cls_H: 0.6318, loss_bbox_lane: 6.6041, loss_bbox_H: 6.6050, d0.loss_cls_lane: 0.6318, d0.loss_cls_H: 0.6318, d0.loss_bbox_lane: 6.6041, d0.loss_bbox_H: 6.6050, d1.loss_cls_lane: 0.6318, d1.loss_cls_H: 0.6318, d1.loss_bbox_lane: 6.6041, d1.loss_bbox_H: 6.6050, d2.loss_cls_lane: 0.6318, d2.loss_cls_H: 0.6318, d2.loss_bbox_lane: 6.6041, d2.loss_bbox_H: 6.6050, d3.loss_cls_lane: 0.6318, d3.loss_cls_H: 0.6318, d3.loss_bbox_lane: 6.6041, d3.loss_bbox_H: 6.6050, d4.loss_cls_lane: 0.6318, d4.loss_cls_H: 0.6318, d4.loss_bbox_lane: 6.6041, d4.loss_bbox_H: 6.6050, vlm_loss: 0.0000, loss: 141.3914, grad_norm: nan
2025-03-01 07:52:40,198 - mmdet - INFO - Iter [12050/168780]	lr: 3.950e-04, eta: 5 days, 10:40:21, time: 3.005, data_time: 0.018, memory: 22041, loss_cls: 1.4166, loss_bbox: 2.6910, d0.loss_cls: 1.4166, d0.loss_bbox: 2.6910, d1.loss_cls: 1.4166, d1.loss_bbox: 2.6910, d2.loss_cls: 1.4166, d2.loss_bbox: 2.6910, d3.loss_cls: 1.4166, d3.loss_bbox: 2.6910, d4.loss_cls: 1.4166, d4.loss_bbox: 2.6910, dn_loss_cls: 1.0009, dn_loss_bbox: 2.2794, d0.dn_loss_cls: 1.0009, d0.dn_loss_bbox: 2.2794, d1.dn_loss_cls: 1.0009, d1.dn_loss_bbox: 2.2794, d2.dn_loss_cls: 1.0009, d2.dn_loss_bbox: 2.2794, d3.dn_loss_cls: 1.0009, d3.dn_loss_bbox: 2.2794, d4.dn_loss_cls: 1.0009, d4.dn_loss_bbox: 2.2794, loss_cls_lane: 0.6441, loss_cls_H: 0.6441, loss_bbox_lane: 6.6105, loss_bbox_H: 6.6031, d0.loss_cls_lane: 0.6441, d0.loss_cls_H: 0.6441, d0.loss_bbox_lane: 6.6105, d0.loss_bbox_H: 6.6031, d1.loss_cls_lane: 0.6441, d1.loss_cls_H: 0.6441, d1.loss_bbox_lane: 6.6105, d1.loss_bbox_H: 6.6031, d2.loss_cls_lane: 0.6441, d2.loss_cls_H: 0.6441, d2.loss_bbox_lane: 6.6105, d2.loss_bbox_H: 6.6031, d3.loss_cls_lane: 0.6441, d3.loss_cls_H: 0.6441, d3.loss_bbox_lane: 6.6105, d3.loss_bbox_H: 6.6031, d4.loss_cls_lane: 0.6441, d4.loss_cls_H: 0.6441, d4.loss_bbox_lane: 6.6105, d4.loss_bbox_H: 6.6031, vlm_loss: 0.0000, loss: 131.3375, grad_norm: nan
2025-03-01 07:55:06,709 - mmdet - INFO - Iter [12100/168780]	lr: 3.950e-04, eta: 5 days, 10:37:04, time: 2.930, data_time: 0.016, memory: 22041, loss_cls: 1.5408, loss_bbox: 2.9311, d0.loss_cls: 1.5408, d0.loss_bbox: 2.9311, d1.loss_cls: 1.5408, d1.loss_bbox: 2.9311, d2.loss_cls: 1.5408, d2.loss_bbox: 2.9311, d3.loss_cls: 1.5408, d3.loss_bbox: 2.9311, d4.loss_cls: 1.5408, d4.loss_bbox: 2.9311, dn_loss_cls: 0.9042, dn_loss_bbox: 2.5553, d0.dn_loss_cls: 0.9042, d0.dn_loss_bbox: 2.5553, d1.dn_loss_cls: 0.9042, d1.dn_loss_bbox: 2.5553, d2.dn_loss_cls: 0.9042, d2.dn_loss_bbox: 2.5553, d3.dn_loss_cls: 0.9042, d3.dn_loss_bbox: 2.5553, d4.dn_loss_cls: 0.9042, d4.dn_loss_bbox: 2.5553, loss_cls_lane: 0.6401, loss_cls_H: 0.6401, loss_bbox_lane: 6.4658, loss_bbox_H: 6.4679, d0.loss_cls_lane: 0.6401, d0.loss_cls_H: 0.6401, d0.loss_bbox_lane: 6.4658, d0.loss_bbox_H: 6.4679, d1.loss_cls_lane: 0.6401, d1.loss_cls_H: 0.6401, d1.loss_bbox_lane: 6.4658, d1.loss_bbox_H: 6.4679, d2.loss_cls_lane: 0.6401, d2.loss_cls_H: 0.6401, d2.loss_bbox_lane: 6.4658, d2.loss_bbox_H: 6.4679, d3.loss_cls_lane: 0.6401, d3.loss_cls_H: 0.6401, d3.loss_bbox_lane: 6.4658, d3.loss_bbox_H: 6.4679, d4.loss_cls_lane: 0.6401, d4.loss_cls_H: 0.6401, d4.loss_bbox_lane: 6.4658, d4.loss_bbox_H: 6.4679, vlm_loss: 0.0000, loss: 132.8713, grad_norm: nan
2025-03-01 07:57:35,432 - mmdet - INFO - Iter [12150/168780]	lr: 3.949e-04, eta: 5 days, 10:34:17, time: 2.974, data_time: 0.017, memory: 22041, loss_cls: 1.7799, loss_bbox: 2.5706, d0.loss_cls: 1.7799, d0.loss_bbox: 2.5706, d1.loss_cls: 1.7799, d1.loss_bbox: 2.5706, d2.loss_cls: 1.7799, d2.loss_bbox: 2.5706, d3.loss_cls: 1.7799, d3.loss_bbox: 2.5706, d4.loss_cls: 1.7799, d4.loss_bbox: 2.5706, dn_loss_cls: 0.9882, dn_loss_bbox: 2.2212, d0.dn_loss_cls: 0.9882, d0.dn_loss_bbox: 2.2212, d1.dn_loss_cls: 0.9882, d1.dn_loss_bbox: 2.2212, d2.dn_loss_cls: 0.9882, d2.dn_loss_bbox: 2.2212, d3.dn_loss_cls: 0.9882, d3.dn_loss_bbox: 2.2212, d4.dn_loss_cls: 0.9882, d4.dn_loss_bbox: 2.2212, loss_cls_lane: 0.6583, loss_cls_H: 0.6583, loss_bbox_lane: 6.9275, loss_bbox_H: 6.9182, d0.loss_cls_lane: 0.6583, d0.loss_cls_H: 0.6583, d0.loss_bbox_lane: 6.9275, d0.loss_bbox_H: 6.9182, d1.loss_cls_lane: 0.6583, d1.loss_cls_H: 0.6583, d1.loss_bbox_lane: 6.9275, d1.loss_bbox_H: 6.9182, d2.loss_cls_lane: 0.6583, d2.loss_cls_H: 0.6583, d2.loss_bbox_lane: 6.9275, d2.loss_bbox_H: 6.9182, d3.loss_cls_lane: 0.6583, d3.loss_cls_H: 0.6583, d3.loss_bbox_lane: 6.9275, d3.loss_bbox_H: 6.9182, d4.loss_cls_lane: 0.6583, d4.loss_cls_H: 0.6583, d4.loss_bbox_lane: 6.9275, d4.loss_bbox_H: 6.9182, vlm_loss: 0.0000, loss: 136.3328, grad_norm: nan
2025-03-01 08:00:04,024 - mmdet - INFO - Iter [12200/168780]	lr: 3.949e-04, eta: 5 days, 10:31:28, time: 2.972, data_time: 0.017, memory: 22041, loss_cls: 1.8085, loss_bbox: 2.7837, d0.loss_cls: 1.8085, d0.loss_bbox: 2.7837, d1.loss_cls: 1.8085, d1.loss_bbox: 2.7837, d2.loss_cls: 1.8085, d2.loss_bbox: 2.7837, d3.loss_cls: 1.8085, d3.loss_bbox: 2.7837, d4.loss_cls: 1.8085, d4.loss_bbox: 2.7837, dn_loss_cls: 1.0502, dn_loss_bbox: 2.4437, d0.dn_loss_cls: 1.0502, d0.dn_loss_bbox: 2.4437, d1.dn_loss_cls: 1.0502, d1.dn_loss_bbox: 2.4437, d2.dn_loss_cls: 1.0502, d2.dn_loss_bbox: 2.4437, d3.dn_loss_cls: 1.0502, d3.dn_loss_bbox: 2.4437, d4.dn_loss_cls: 1.0502, d4.dn_loss_bbox: 2.4437, loss_cls_lane: 0.6435, loss_cls_H: 0.6435, loss_bbox_lane: 6.6150, loss_bbox_H: 6.6026, d0.loss_cls_lane: 0.6435, d0.loss_cls_H: 0.6435, d0.loss_bbox_lane: 6.6150, d0.loss_bbox_H: 6.6026, d1.loss_cls_lane: 0.6435, d1.loss_cls_H: 0.6435, d1.loss_bbox_lane: 6.6150, d1.loss_bbox_H: 6.6026, d2.loss_cls_lane: 0.6435, d2.loss_cls_H: 0.6435, d2.loss_bbox_lane: 6.6150, d2.loss_bbox_H: 6.6026, d3.loss_cls_lane: 0.6435, d3.loss_cls_H: 0.6435, d3.loss_bbox_lane: 6.6150, d3.loss_bbox_H: 6.6026, d4.loss_cls_lane: 0.6435, d4.loss_cls_H: 0.6435, d4.loss_bbox_lane: 6.6150, d4.loss_bbox_H: 6.6026, vlm_loss: 0.0000, loss: 135.5443, grad_norm: nan
2025-03-01 08:02:32,066 - mmdet - INFO - Iter [12250/168780]	lr: 3.948e-04, eta: 5 days, 10:28:33, time: 2.961, data_time: 0.017, memory: 22041, loss_cls: 2.0891, loss_bbox: 3.0891, d0.loss_cls: 2.0891, d0.loss_bbox: 3.0891, d1.loss_cls: 2.0891, d1.loss_bbox: 3.0891, d2.loss_cls: 2.0891, d2.loss_bbox: 3.0891, d3.loss_cls: 2.0891, d3.loss_bbox: 3.0891, d4.loss_cls: 2.0891, d4.loss_bbox: 3.0891, dn_loss_cls: 0.9058, dn_loss_bbox: 2.6805, d0.dn_loss_cls: 0.9058, d0.dn_loss_bbox: 2.6805, d1.dn_loss_cls: 0.9058, d1.dn_loss_bbox: 2.6805, d2.dn_loss_cls: 0.9058, d2.dn_loss_bbox: 2.6805, d3.dn_loss_cls: 0.9058, d3.dn_loss_bbox: 2.6805, d4.dn_loss_cls: 0.9058, d4.dn_loss_bbox: 2.6805, loss_cls_lane: 0.6340, loss_cls_H: 0.6340, loss_bbox_lane: 6.5013, loss_bbox_H: 6.4911, d0.loss_cls_lane: 0.6340, d0.loss_cls_H: 0.6340, d0.loss_bbox_lane: 6.5013, d0.loss_bbox_H: 6.4911, d1.loss_cls_lane: 0.6340, d1.loss_cls_H: 0.6340, d1.loss_bbox_lane: 6.5013, d1.loss_bbox_H: 6.4911, d2.loss_cls_lane: 0.6340, d2.loss_cls_H: 0.6340, d2.loss_bbox_lane: 6.5013, d2.loss_bbox_H: 6.4911, d3.loss_cls_lane: 0.6340, d3.loss_cls_H: 0.6340, d3.loss_bbox_lane: 6.5013, d3.loss_bbox_H: 6.4911, d4.loss_cls_lane: 0.6340, d4.loss_cls_H: 0.6340, d4.loss_bbox_lane: 6.5013, d4.loss_bbox_H: 6.4911, vlm_loss: 0.0000, loss: 138.1503, grad_norm: nan
2025-03-01 08:05:02,704 - mmdet - INFO - Iter [12300/168780]	lr: 3.948e-04, eta: 5 days, 10:26:10, time: 3.013, data_time: 0.018, memory: 22041, loss_cls: 1.3686, loss_bbox: 2.9294, d0.loss_cls: 1.3686, d0.loss_bbox: 2.9294, d1.loss_cls: 1.3686, d1.loss_bbox: 2.9294, d2.loss_cls: 1.3686, d2.loss_bbox: 2.9294, d3.loss_cls: 1.3686, d3.loss_bbox: 2.9294, d4.loss_cls: 1.3686, d4.loss_bbox: 2.9294, dn_loss_cls: 0.9776, dn_loss_bbox: 2.4927, d0.dn_loss_cls: 0.9776, d0.dn_loss_bbox: 2.4927, d1.dn_loss_cls: 0.9776, d1.dn_loss_bbox: 2.4927, d2.dn_loss_cls: 0.9776, d2.dn_loss_bbox: 2.4927, d3.dn_loss_cls: 0.9776, d3.dn_loss_bbox: 2.4927, d4.dn_loss_cls: 0.9776, d4.dn_loss_bbox: 2.4927, loss_cls_lane: 0.6296, loss_cls_H: 0.6296, loss_bbox_lane: 6.3952, loss_bbox_H: 6.3993, d0.loss_cls_lane: 0.6296, d0.loss_cls_H: 0.6296, d0.loss_bbox_lane: 6.3952, d0.loss_bbox_H: 6.3993, d1.loss_cls_lane: 0.6296, d1.loss_cls_H: 0.6296, d1.loss_bbox_lane: 6.3952, d1.loss_bbox_H: 6.3993, d2.loss_cls_lane: 0.6296, d2.loss_cls_H: 0.6296, d2.loss_bbox_lane: 6.3952, d2.loss_bbox_H: 6.3993, d3.loss_cls_lane: 0.6296, d3.loss_cls_H: 0.6296, d3.loss_bbox_lane: 6.3952, d3.loss_bbox_H: 6.3993, d4.loss_cls_lane: 0.6296, d4.loss_cls_H: 0.6296, d4.loss_bbox_lane: 6.3952, d4.loss_bbox_H: 6.3993, vlm_loss: 0.0000, loss: 130.9315, grad_norm: nan
2025-03-01 08:07:32,974 - mmdet - INFO - Iter [12350/168780]	lr: 3.947e-04, eta: 5 days, 10:23:43, time: 3.005, data_time: 0.017, memory: 22041, loss_cls: 1.2786, loss_bbox: 2.7225, d0.loss_cls: 1.2786, d0.loss_bbox: 2.7225, d1.loss_cls: 1.2786, d1.loss_bbox: 2.7225, d2.loss_cls: 1.2786, d2.loss_bbox: 2.7225, d3.loss_cls: 1.2786, d3.loss_bbox: 2.7225, d4.loss_cls: 1.2786, d4.loss_bbox: 2.7225, dn_loss_cls: 0.9252, dn_loss_bbox: 2.3375, d0.dn_loss_cls: 0.9252, d0.dn_loss_bbox: 2.3375, d1.dn_loss_cls: 0.9252, d1.dn_loss_bbox: 2.3375, d2.dn_loss_cls: 0.9252, d2.dn_loss_bbox: 2.3375, d3.dn_loss_cls: 0.9252, d3.dn_loss_bbox: 2.3375, d4.dn_loss_cls: 0.9252, d4.dn_loss_bbox: 2.3375, loss_cls_lane: 0.6406, loss_cls_H: 0.6406, loss_bbox_lane: 6.9613, loss_bbox_H: 6.9565, d0.loss_cls_lane: 0.6406, d0.loss_cls_H: 0.6406, d0.loss_bbox_lane: 6.9613, d0.loss_bbox_H: 6.9565, d1.loss_cls_lane: 0.6406, d1.loss_cls_H: 0.6406, d1.loss_bbox_lane: 6.9613, d1.loss_bbox_H: 6.9565, d2.loss_cls_lane: 0.6406, d2.loss_cls_H: 0.6406, d2.loss_bbox_lane: 6.9613, d2.loss_bbox_H: 6.9565, d3.loss_cls_lane: 0.6406, d3.loss_cls_H: 0.6406, d3.loss_bbox_lane: 6.9613, d3.loss_bbox_H: 6.9565, d4.loss_cls_lane: 0.6406, d4.loss_cls_H: 0.6406, d4.loss_bbox_lane: 6.9613, d4.loss_bbox_H: 6.9565, vlm_loss: 0.0000, loss: 134.7767, grad_norm: nan
2025-03-01 08:10:00,801 - mmdet - INFO - Iter [12400/168780]	lr: 3.947e-04, eta: 5 days, 10:20:45, time: 2.957, data_time: 0.017, memory: 22041, loss_cls: 1.7753, loss_bbox: 2.9777, d0.loss_cls: 1.7753, d0.loss_bbox: 2.9777, d1.loss_cls: 1.7753, d1.loss_bbox: 2.9777, d2.loss_cls: 1.7753, d2.loss_bbox: 2.9777, d3.loss_cls: 1.7753, d3.loss_bbox: 2.9777, d4.loss_cls: 1.7753, d4.loss_bbox: 2.9777, dn_loss_cls: 0.8392, dn_loss_bbox: 2.7042, d0.dn_loss_cls: 0.8392, d0.dn_loss_bbox: 2.7042, d1.dn_loss_cls: 0.8392, d1.dn_loss_bbox: 2.7042, d2.dn_loss_cls: 0.8392, d2.dn_loss_bbox: 2.7042, d3.dn_loss_cls: 0.8392, d3.dn_loss_bbox: 2.7042, d4.dn_loss_cls: 0.8392, d4.dn_loss_bbox: 2.7042, loss_cls_lane: 0.6409, loss_cls_H: 0.6409, loss_bbox_lane: 6.6525, loss_bbox_H: 6.6378, d0.loss_cls_lane: 0.6409, d0.loss_cls_H: 0.6409, d0.loss_bbox_lane: 6.6525, d0.loss_bbox_H: 6.6378, d1.loss_cls_lane: 0.6409, d1.loss_cls_H: 0.6409, d1.loss_bbox_lane: 6.6525, d1.loss_bbox_H: 6.6378, d2.loss_cls_lane: 0.6409, d2.loss_cls_H: 0.6409, d2.loss_bbox_lane: 6.6525, d2.loss_bbox_H: 6.6378, d3.loss_cls_lane: 0.6409, d3.loss_cls_H: 0.6409, d3.loss_bbox_lane: 6.6525, d3.loss_bbox_H: 6.6378, d4.loss_cls_lane: 0.6409, d4.loss_cls_H: 0.6409, d4.loss_bbox_lane: 6.6525, d4.loss_bbox_H: 6.6378, vlm_loss: 0.0000, loss: 137.2114, grad_norm: nan
2025-03-01 08:12:26,490 - mmdet - INFO - Iter [12450/168780]	lr: 3.947e-04, eta: 5 days, 10:17:20, time: 2.914, data_time: 0.016, memory: 22041, loss_cls: 2.2465, loss_bbox: 2.7898, d0.loss_cls: 2.2465, d0.loss_bbox: 2.7898, d1.loss_cls: 2.2465, d1.loss_bbox: 2.7898, d2.loss_cls: 2.2465, d2.loss_bbox: 2.7898, d3.loss_cls: 2.2465, d3.loss_bbox: 2.7898, d4.loss_cls: 2.2465, d4.loss_bbox: 2.7898, dn_loss_cls: 0.9024, dn_loss_bbox: 2.5141, d0.dn_loss_cls: 0.9024, d0.dn_loss_bbox: 2.5141, d1.dn_loss_cls: 0.9024, d1.dn_loss_bbox: 2.5141, d2.dn_loss_cls: 0.9024, d2.dn_loss_bbox: 2.5141, d3.dn_loss_cls: 0.9024, d3.dn_loss_bbox: 2.5141, d4.dn_loss_cls: 0.9024, d4.dn_loss_bbox: 2.5141, loss_cls_lane: 0.6473, loss_cls_H: 0.6473, loss_bbox_lane: 6.4709, loss_bbox_H: 6.4725, d0.loss_cls_lane: 0.6473, d0.loss_cls_H: 0.6473, d0.loss_bbox_lane: 6.4709, d0.loss_bbox_H: 6.4725, d1.loss_cls_lane: 0.6473, d1.loss_cls_H: 0.6473, d1.loss_bbox_lane: 6.4709, d1.loss_bbox_H: 6.4725, d2.loss_cls_lane: 0.6473, d2.loss_cls_H: 0.6473, d2.loss_bbox_lane: 6.4709, d2.loss_bbox_H: 6.4725, d3.loss_cls_lane: 0.6473, d3.loss_cls_H: 0.6473, d3.loss_bbox_lane: 6.4709, d3.loss_bbox_H: 6.4725, d4.loss_cls_lane: 0.6473, d4.loss_cls_H: 0.6473, d4.loss_bbox_lane: 6.4709, d4.loss_bbox_H: 6.4725, vlm_loss: 0.0000, loss: 136.1457, grad_norm: nan
2025-03-01 08:14:55,130 - mmdet - INFO - Iter [12500/168780]	lr: 3.946e-04, eta: 5 days, 10:14:33, time: 2.973, data_time: 0.018, memory: 22041, loss_cls: 1.5784, loss_bbox: 2.7673, d0.loss_cls: 1.5784, d0.loss_bbox: 2.7673, d1.loss_cls: 1.5784, d1.loss_bbox: 2.7673, d2.loss_cls: 1.5784, d2.loss_bbox: 2.7673, d3.loss_cls: 1.5784, d3.loss_bbox: 2.7673, d4.loss_cls: 1.5784, d4.loss_bbox: 2.7673, dn_loss_cls: 0.8861, dn_loss_bbox: 2.4483, d0.dn_loss_cls: 0.8861, d0.dn_loss_bbox: 2.4483, d1.dn_loss_cls: 0.8861, d1.dn_loss_bbox: 2.4483, d2.dn_loss_cls: 0.8861, d2.dn_loss_bbox: 2.4483, d3.dn_loss_cls: 0.8861, d3.dn_loss_bbox: 2.4483, d4.dn_loss_cls: 0.8861, d4.dn_loss_bbox: 2.4483, loss_cls_lane: 0.6456, loss_cls_H: 0.6456, loss_bbox_lane: 6.5660, loss_bbox_H: 6.5722, d0.loss_cls_lane: 0.6456, d0.loss_cls_H: 0.6456, d0.loss_bbox_lane: 6.5660, d0.loss_bbox_H: 6.5722, d1.loss_cls_lane: 0.6456, d1.loss_cls_H: 0.6456, d1.loss_bbox_lane: 6.5660, d1.loss_bbox_H: 6.5722, d2.loss_cls_lane: 0.6456, d2.loss_cls_H: 0.6456, d2.loss_bbox_lane: 6.5660, d2.loss_bbox_H: 6.5722, d3.loss_cls_lane: 0.6456, d3.loss_cls_H: 0.6456, d3.loss_bbox_lane: 6.5660, d3.loss_bbox_H: 6.5722, d4.loss_cls_lane: 0.6456, d4.loss_cls_H: 0.6456, d4.loss_bbox_lane: 6.5660, d4.loss_bbox_H: 6.5722, vlm_loss: 0.0000, loss: 132.6585, grad_norm: nan
2025-03-01 08:17:24,237 - mmdet - INFO - Iter [12550/168780]	lr: 3.946e-04, eta: 5 days, 10:11:52, time: 2.982, data_time: 0.018, memory: 22041, loss_cls: 1.6717, loss_bbox: 2.9686, d0.loss_cls: 1.6717, d0.loss_bbox: 2.9686, d1.loss_cls: 1.6717, d1.loss_bbox: 2.9686, d2.loss_cls: 1.6717, d2.loss_bbox: 2.9686, d3.loss_cls: 1.6717, d3.loss_bbox: 2.9686, d4.loss_cls: 1.6717, d4.loss_bbox: 2.9686, dn_loss_cls: 0.9428, dn_loss_bbox: 2.6539, d0.dn_loss_cls: 0.9428, d0.dn_loss_bbox: 2.6539, d1.dn_loss_cls: 0.9428, d1.dn_loss_bbox: 2.6539, d2.dn_loss_cls: 0.9428, d2.dn_loss_bbox: 2.6539, d3.dn_loss_cls: 0.9428, d3.dn_loss_bbox: 2.6539, d4.dn_loss_cls: 0.9428, d4.dn_loss_bbox: 2.6539, loss_cls_lane: 0.6311, loss_cls_H: 0.6311, loss_bbox_lane: 6.3476, loss_bbox_H: 6.3590, d0.loss_cls_lane: 0.6311, d0.loss_cls_H: 0.6311, d0.loss_bbox_lane: 6.3476, d0.loss_bbox_H: 6.3590, d1.loss_cls_lane: 0.6311, d1.loss_cls_H: 0.6311, d1.loss_bbox_lane: 6.3476, d1.loss_bbox_H: 6.3590, d2.loss_cls_lane: 0.6311, d2.loss_cls_H: 0.6311, d2.loss_bbox_lane: 6.3476, d2.loss_bbox_H: 6.3590, d3.loss_cls_lane: 0.6311, d3.loss_cls_H: 0.6311, d3.loss_bbox_lane: 6.3476, d3.loss_bbox_H: 6.3590, d4.loss_cls_lane: 0.6311, d4.loss_cls_H: 0.6311, d4.loss_bbox_lane: 6.3476, d4.loss_bbox_H: 6.3590, vlm_loss: 0.0000, loss: 133.2352, grad_norm: nan
2025-03-01 08:19:54,174 - mmdet - INFO - Iter [12600/168780]	lr: 3.945e-04, eta: 5 days, 10:09:21, time: 2.999, data_time: 0.017, memory: 22041, loss_cls: 1.2742, loss_bbox: 2.6252, d0.loss_cls: 1.2742, d0.loss_bbox: 2.6252, d1.loss_cls: 1.2742, d1.loss_bbox: 2.6252, d2.loss_cls: 1.2742, d2.loss_bbox: 2.6252, d3.loss_cls: 1.2742, d3.loss_bbox: 2.6252, d4.loss_cls: 1.2742, d4.loss_bbox: 2.6252, dn_loss_cls: 0.9614, dn_loss_bbox: 2.2784, d0.dn_loss_cls: 0.9614, d0.dn_loss_bbox: 2.2784, d1.dn_loss_cls: 0.9614, d1.dn_loss_bbox: 2.2784, d2.dn_loss_cls: 0.9614, d2.dn_loss_bbox: 2.2784, d3.dn_loss_cls: 0.9614, d3.dn_loss_bbox: 2.2784, d4.dn_loss_cls: 0.9614, d4.dn_loss_bbox: 2.2784, loss_cls_lane: 0.6414, loss_cls_H: 0.6414, loss_bbox_lane: 6.7447, loss_bbox_H: 6.7482, d0.loss_cls_lane: 0.6414, d0.loss_cls_H: 0.6414, d0.loss_bbox_lane: 6.7447, d0.loss_bbox_H: 6.7482, d1.loss_cls_lane: 0.6414, d1.loss_cls_H: 0.6414, d1.loss_bbox_lane: 6.7447, d1.loss_bbox_H: 6.7482, d2.loss_cls_lane: 0.6414, d2.loss_cls_H: 0.6414, d2.loss_bbox_lane: 6.7447, d2.loss_bbox_H: 6.7482, d3.loss_cls_lane: 0.6414, d3.loss_cls_H: 0.6414, d3.loss_bbox_lane: 6.7447, d3.loss_bbox_H: 6.7482, d4.loss_cls_lane: 0.6414, d4.loss_cls_H: 0.6414, d4.loss_bbox_lane: 6.7447, d4.loss_bbox_H: 6.7482, vlm_loss: 0.0000, loss: 131.4902, grad_norm: nan
2025-03-01 08:22:42,433 - mmdet - INFO - Iter [12650/168780]	lr: 3.945e-04, eta: 5 days, 10:10:36, time: 3.365, data_time: 0.017, memory: 22041, loss_cls: 1.4356, loss_bbox: 2.7163, d0.loss_cls: 1.4356, d0.loss_bbox: 2.7163, d1.loss_cls: 1.4356, d1.loss_bbox: 2.7163, d2.loss_cls: 1.4356, d2.loss_bbox: 2.7163, d3.loss_cls: 1.4356, d3.loss_bbox: 2.7163, d4.loss_cls: 1.4356, d4.loss_bbox: 2.7163, dn_loss_cls: 0.9876, dn_loss_bbox: 2.4185, d0.dn_loss_cls: 0.9876, d0.dn_loss_bbox: 2.4185, d1.dn_loss_cls: 0.9876, d1.dn_loss_bbox: 2.4185, d2.dn_loss_cls: 0.9876, d2.dn_loss_bbox: 2.4185, d3.dn_loss_cls: 0.9876, d3.dn_loss_bbox: 2.4185, d4.dn_loss_cls: 0.9876, d4.dn_loss_bbox: 2.4185, loss_cls_lane: 0.6324, loss_cls_H: 0.6324, loss_bbox_lane: 6.4765, loss_bbox_H: 6.4820, d0.loss_cls_lane: 0.6324, d0.loss_cls_H: 0.6324, d0.loss_bbox_lane: 6.4765, d0.loss_bbox_H: 6.4820, d1.loss_cls_lane: 0.6324, d1.loss_cls_H: 0.6324, d1.loss_bbox_lane: 6.4765, d1.loss_bbox_H: 6.4820, d2.loss_cls_lane: 0.6324, d2.loss_cls_H: 0.6324, d2.loss_bbox_lane: 6.4765, d2.loss_bbox_H: 6.4820, d3.loss_cls_lane: 0.6324, d3.loss_cls_H: 0.6324, d3.loss_bbox_lane: 6.4765, d3.loss_bbox_H: 6.4820, d4.loss_cls_lane: 0.6324, d4.loss_cls_H: 0.6324, d4.loss_bbox_lane: 6.4765, d4.loss_bbox_H: 6.4820, vlm_loss: 0.0000, loss: 130.6888, grad_norm: nan
2025-03-01 08:25:10,989 - mmdet - INFO - Iter [12700/168780]	lr: 3.944e-04, eta: 5 days, 10:07:48, time: 2.971, data_time: 0.018, memory: 22041, loss_cls: 1.3486, loss_bbox: 2.7651, d0.loss_cls: 1.3486, d0.loss_bbox: 2.7651, d1.loss_cls: 1.3486, d1.loss_bbox: 2.7651, d2.loss_cls: 1.3486, d2.loss_bbox: 2.7651, d3.loss_cls: 1.3486, d3.loss_bbox: 2.7651, d4.loss_cls: 1.3486, d4.loss_bbox: 2.7651, dn_loss_cls: 0.8998, dn_loss_bbox: 2.3913, d0.dn_loss_cls: 0.8998, d0.dn_loss_bbox: 2.3913, d1.dn_loss_cls: 0.8998, d1.dn_loss_bbox: 2.3913, d2.dn_loss_cls: 0.8998, d2.dn_loss_bbox: 2.3913, d3.dn_loss_cls: 0.8998, d3.dn_loss_bbox: 2.3913, d4.dn_loss_cls: 0.8998, d4.dn_loss_bbox: 2.3913, loss_cls_lane: 0.6285, loss_cls_H: 0.6285, loss_bbox_lane: 6.5660, loss_bbox_H: 6.5593, d0.loss_cls_lane: 0.6285, d0.loss_cls_H: 0.6285, d0.loss_bbox_lane: 6.5660, d0.loss_bbox_H: 6.5593, d1.loss_cls_lane: 0.6285, d1.loss_cls_H: 0.6285, d1.loss_bbox_lane: 6.5660, d1.loss_bbox_H: 6.5593, d2.loss_cls_lane: 0.6285, d2.loss_cls_H: 0.6285, d2.loss_bbox_lane: 6.5660, d2.loss_bbox_H: 6.5593, d3.loss_cls_lane: 0.6285, d3.loss_cls_H: 0.6285, d3.loss_bbox_lane: 6.5660, d3.loss_bbox_H: 6.5593, d4.loss_cls_lane: 0.6285, d4.loss_cls_H: 0.6285, d4.loss_bbox_lane: 6.5660, d4.loss_bbox_H: 6.5593, vlm_loss: 0.0000, loss: 130.7230, grad_norm: nan
2025-03-01 08:27:39,608 - mmdet - INFO - Iter [12750/168780]	lr: 3.944e-04, eta: 5 days, 10:05:00, time: 2.972, data_time: 0.017, memory: 22041, loss_cls: 1.8032, loss_bbox: 2.8112, d0.loss_cls: 1.8032, d0.loss_bbox: 2.8112, d1.loss_cls: 1.8032, d1.loss_bbox: 2.8112, d2.loss_cls: 1.8032, d2.loss_bbox: 2.8112, d3.loss_cls: 1.8032, d3.loss_bbox: 2.8112, d4.loss_cls: 1.8032, d4.loss_bbox: 2.8112, dn_loss_cls: 0.9064, dn_loss_bbox: 2.3527, d0.dn_loss_cls: 0.9064, d0.dn_loss_bbox: 2.3527, d1.dn_loss_cls: 0.9064, d1.dn_loss_bbox: 2.3527, d2.dn_loss_cls: 0.9064, d2.dn_loss_bbox: 2.3527, d3.dn_loss_cls: 0.9064, d3.dn_loss_bbox: 2.3527, d4.dn_loss_cls: 0.9064, d4.dn_loss_bbox: 2.3527, loss_cls_lane: 0.6322, loss_cls_H: 0.6322, loss_bbox_lane: 6.1652, loss_bbox_H: 6.1785, d0.loss_cls_lane: 0.6322, d0.loss_cls_H: 0.6322, d0.loss_bbox_lane: 6.1652, d0.loss_bbox_H: 6.1785, d1.loss_cls_lane: 0.6322, d1.loss_cls_H: 0.6322, d1.loss_bbox_lane: 6.1652, d1.loss_bbox_H: 6.1785, d2.loss_cls_lane: 0.6322, d2.loss_cls_H: 0.6322, d2.loss_bbox_lane: 6.1652, d2.loss_bbox_H: 6.1785, d3.loss_cls_lane: 0.6322, d3.loss_cls_H: 0.6322, d3.loss_bbox_lane: 6.1652, d3.loss_bbox_H: 6.1785, d4.loss_cls_lane: 0.6322, d4.loss_cls_H: 0.6322, d4.loss_bbox_lane: 6.1652, d4.loss_bbox_H: 6.1785, vlm_loss: 0.0000, loss: 128.8899, grad_norm: nan
2025-03-01 08:30:28,871 - mmdet - INFO - Iter [12800/168780]	lr: 3.944e-04, eta: 5 days, 10:06:24, time: 3.385, data_time: 0.017, memory: 22041, loss_cls: 1.6115, loss_bbox: 2.6234, d0.loss_cls: 1.6115, d0.loss_bbox: 2.6234, d1.loss_cls: 1.6115, d1.loss_bbox: 2.6234, d2.loss_cls: 1.6115, d2.loss_bbox: 2.6234, d3.loss_cls: 1.6115, d3.loss_bbox: 2.6234, d4.loss_cls: 1.6115, d4.loss_bbox: 2.6234, dn_loss_cls: 0.9410, dn_loss_bbox: 2.2394, d0.dn_loss_cls: 0.9410, d0.dn_loss_bbox: 2.2394, d1.dn_loss_cls: 0.9410, d1.dn_loss_bbox: 2.2394, d2.dn_loss_cls: 0.9410, d2.dn_loss_bbox: 2.2394, d3.dn_loss_cls: 0.9410, d3.dn_loss_bbox: 2.2394, d4.dn_loss_cls: 0.9410, d4.dn_loss_bbox: 2.2394, loss_cls_lane: 0.6389, loss_cls_H: 0.6389, loss_bbox_lane: 6.4075, loss_bbox_H: 6.4109, d0.loss_cls_lane: 0.6389, d0.loss_cls_H: 0.6389, d0.loss_bbox_lane: 6.4075, d0.loss_bbox_H: 6.4109, d1.loss_cls_lane: 0.6389, d1.loss_cls_H: 0.6389, d1.loss_bbox_lane: 6.4075, d1.loss_bbox_H: 6.4109, d2.loss_cls_lane: 0.6389, d2.loss_cls_H: 0.6389, d2.loss_bbox_lane: 6.4075, d2.loss_bbox_H: 6.4109, d3.loss_cls_lane: 0.6389, d3.loss_cls_H: 0.6389, d3.loss_bbox_lane: 6.4075, d3.loss_bbox_H: 6.4109, d4.loss_cls_lane: 0.6389, d4.loss_cls_H: 0.6389, d4.loss_bbox_lane: 6.4075, d4.loss_bbox_H: 6.4109, vlm_loss: 0.0000, loss: 129.0689, grad_norm: nan
2025-03-01 08:32:58,340 - mmdet - INFO - Iter [12850/168780]	lr: 3.943e-04, eta: 5 days, 10:03:45, time: 2.989, data_time: 0.017, memory: 22041, loss_cls: 1.5281, loss_bbox: 2.8723, d0.loss_cls: 1.5281, d0.loss_bbox: 2.8723, d1.loss_cls: 1.5281, d1.loss_bbox: 2.8723, d2.loss_cls: 1.5281, d2.loss_bbox: 2.8723, d3.loss_cls: 1.5281, d3.loss_bbox: 2.8723, d4.loss_cls: 1.5281, d4.loss_bbox: 2.8723, dn_loss_cls: 0.9064, dn_loss_bbox: 2.5252, d0.dn_loss_cls: 0.9064, d0.dn_loss_bbox: 2.5252, d1.dn_loss_cls: 0.9064, d1.dn_loss_bbox: 2.5252, d2.dn_loss_cls: 0.9064, d2.dn_loss_bbox: 2.5252, d3.dn_loss_cls: 0.9064, d3.dn_loss_bbox: 2.5252, d4.dn_loss_cls: 0.9064, d4.dn_loss_bbox: 2.5252, loss_cls_lane: 0.6405, loss_cls_H: 0.6405, loss_bbox_lane: 6.6466, loss_bbox_H: 6.6359, d0.loss_cls_lane: 0.6405, d0.loss_cls_H: 0.6405, d0.loss_bbox_lane: 6.6466, d0.loss_bbox_H: 6.6359, d1.loss_cls_lane: 0.6405, d1.loss_cls_H: 0.6405, d1.loss_bbox_lane: 6.6466, d1.loss_bbox_H: 6.6359, d2.loss_cls_lane: 0.6405, d2.loss_cls_H: 0.6405, d2.loss_bbox_lane: 6.6466, d2.loss_bbox_H: 6.6359, d3.loss_cls_lane: 0.6405, d3.loss_cls_H: 0.6405, d3.loss_bbox_lane: 6.6466, d3.loss_bbox_H: 6.6359, d4.loss_cls_lane: 0.6405, d4.loss_cls_H: 0.6405, d4.loss_bbox_lane: 6.6466, d4.loss_bbox_H: 6.6359, vlm_loss: 0.0000, loss: 134.3723, grad_norm: nan
2025-03-01 08:35:23,486 - mmdet - INFO - Iter [12900/168780]	lr: 3.943e-04, eta: 5 days, 10:00:15, time: 2.903, data_time: 0.017, memory: 22041, loss_cls: 2.7635, loss_bbox: 2.8821, d0.loss_cls: 2.7635, d0.loss_bbox: 2.8821, d1.loss_cls: 2.7635, d1.loss_bbox: 2.8821, d2.loss_cls: 2.7635, d2.loss_bbox: 2.8821, d3.loss_cls: 2.7635, d3.loss_bbox: 2.8821, d4.loss_cls: 2.7635, d4.loss_bbox: 2.8821, dn_loss_cls: 1.6543, dn_loss_bbox: 2.4993, d0.dn_loss_cls: 1.6543, d0.dn_loss_bbox: 2.4993, d1.dn_loss_cls: 1.6543, d1.dn_loss_bbox: 2.4993, d2.dn_loss_cls: 1.6543, d2.dn_loss_bbox: 2.4993, d3.dn_loss_cls: 1.6543, d3.dn_loss_bbox: 2.4993, d4.dn_loss_cls: 1.6543, d4.dn_loss_bbox: 2.4993, loss_cls_lane: 0.6392, loss_cls_H: 0.6929, loss_bbox_lane: 6.3130, loss_bbox_H: 6.3085, d0.loss_cls_lane: 0.6392, d0.loss_cls_H: 0.6929, d0.loss_bbox_lane: 6.3130, d0.loss_bbox_H: 6.3085, d1.loss_cls_lane: 0.6392, d1.loss_cls_H: 0.6929, d1.loss_bbox_lane: 6.3130, d1.loss_bbox_H: 6.3085, d2.loss_cls_lane: 0.6392, d2.loss_cls_H: 0.6929, d2.loss_bbox_lane: 6.3130, d2.loss_bbox_H: 6.3085, d3.loss_cls_lane: 0.6392, d3.loss_cls_H: 0.6929, d3.loss_bbox_lane: 6.3130, d3.loss_bbox_H: 6.3085, d4.loss_cls_lane: 0.6392, d4.loss_cls_H: 0.6929, d4.loss_bbox_lane: 6.3130, d4.loss_bbox_H: 6.3085, vlm_loss: 0.0000, loss: 142.5167, grad_norm: nan
2025-03-01 08:37:49,259 - mmdet - INFO - Iter [12950/168780]	lr: 3.942e-04, eta: 5 days, 9:56:52, time: 2.915, data_time: 0.016, memory: 22041, loss_cls: 2.6322, loss_bbox: 2.7043, d0.loss_cls: 2.6322, d0.loss_bbox: 2.7043, d1.loss_cls: 2.6322, d1.loss_bbox: 2.7043, d2.loss_cls: 2.6322, d2.loss_bbox: 2.7043, d3.loss_cls: 2.6322, d3.loss_bbox: 2.7043, d4.loss_cls: 2.6322, d4.loss_bbox: 2.7043, dn_loss_cls: 0.9959, dn_loss_bbox: 2.2492, d0.dn_loss_cls: 0.9959, d0.dn_loss_bbox: 2.2492, d1.dn_loss_cls: 0.9959, d1.dn_loss_bbox: 2.2492, d2.dn_loss_cls: 0.9959, d2.dn_loss_bbox: 2.2492, d3.dn_loss_cls: 0.9959, d3.dn_loss_bbox: 2.2492, d4.dn_loss_cls: 0.9959, d4.dn_loss_bbox: 2.2492, loss_cls_lane: 0.6620, loss_cls_H: 0.6620, loss_bbox_lane: 7.0381, loss_bbox_H: 7.0268, d0.loss_cls_lane: 0.6620, d0.loss_cls_H: 0.6620, d0.loss_bbox_lane: 7.0381, d0.loss_bbox_H: 7.0268, d1.loss_cls_lane: 0.6620, d1.loss_cls_H: 0.6620, d1.loss_bbox_lane: 7.0381, d1.loss_bbox_H: 7.0268, d2.loss_cls_lane: 0.6620, d2.loss_cls_H: 0.6620, d2.loss_bbox_lane: 7.0381, d2.loss_bbox_H: 7.0268, d3.loss_cls_lane: 0.6620, d3.loss_cls_H: 0.6620, d3.loss_bbox_lane: 7.0381, d3.loss_bbox_H: 7.0268, d4.loss_cls_lane: 0.6620, d4.loss_cls_H: 0.6620, d4.loss_bbox_lane: 7.0381, d4.loss_bbox_H: 7.0268, vlm_loss: 0.0000, loss: 143.8239, grad_norm: nan
2025-03-01 08:40:18,715 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 08:40:18,715 - mmdet - INFO - Iter [13000/168780]	lr: 3.942e-04, eta: 5 days, 9:54:14, time: 2.989, data_time: 0.018, memory: 22041, loss_cls: 1.3155, loss_bbox: 2.6513, d0.loss_cls: 1.3155, d0.loss_bbox: 2.6513, d1.loss_cls: 1.3155, d1.loss_bbox: 2.6513, d2.loss_cls: 1.3155, d2.loss_bbox: 2.6513, d3.loss_cls: 1.3155, d3.loss_bbox: 2.6513, d4.loss_cls: 1.3155, d4.loss_bbox: 2.6513, dn_loss_cls: 0.9641, dn_loss_bbox: 2.2773, d0.dn_loss_cls: 0.9641, d0.dn_loss_bbox: 2.2773, d1.dn_loss_cls: 0.9641, d1.dn_loss_bbox: 2.2773, d2.dn_loss_cls: 0.9641, d2.dn_loss_bbox: 2.2773, d3.dn_loss_cls: 0.9641, d3.dn_loss_bbox: 2.2773, d4.dn_loss_cls: 0.9641, d4.dn_loss_bbox: 2.2773, loss_cls_lane: 0.6762, loss_cls_H: 0.6762, loss_bbox_lane: 6.7713, loss_bbox_H: 6.7676, d0.loss_cls_lane: 0.6762, d0.loss_cls_H: 0.6762, d0.loss_bbox_lane: 6.7713, d0.loss_bbox_H: 6.7676, d1.loss_cls_lane: 0.6762, d1.loss_cls_H: 0.6762, d1.loss_bbox_lane: 6.7713, d1.loss_bbox_H: 6.7676, d2.loss_cls_lane: 0.6762, d2.loss_cls_H: 0.6762, d2.loss_bbox_lane: 6.7713, d2.loss_bbox_H: 6.7676, d3.loss_cls_lane: 0.6762, d3.loss_cls_H: 0.6762, d3.loss_bbox_lane: 6.7713, d3.loss_bbox_H: 6.7676, d4.loss_cls_lane: 0.6762, d4.loss_cls_H: 0.6762, d4.loss_bbox_lane: 6.7713, d4.loss_bbox_H: 6.7676, vlm_loss: 0.0000, loss: 132.5973, grad_norm: nan
2025-03-01 08:42:48,261 - mmdet - INFO - Iter [13050/168780]	lr: 3.941e-04, eta: 5 days, 9:51:38, time: 2.991, data_time: 0.016, memory: 22041, loss_cls: 1.6130, loss_bbox: 3.0534, d0.loss_cls: 1.6130, d0.loss_bbox: 3.0534, d1.loss_cls: 1.6130, d1.loss_bbox: 3.0534, d2.loss_cls: 1.6130, d2.loss_bbox: 3.0534, d3.loss_cls: 1.6130, d3.loss_bbox: 3.0534, d4.loss_cls: 1.6130, d4.loss_bbox: 3.0534, dn_loss_cls: 0.9328, dn_loss_bbox: 2.4970, d0.dn_loss_cls: 0.9328, d0.dn_loss_bbox: 2.4970, d1.dn_loss_cls: 0.9328, d1.dn_loss_bbox: 2.4970, d2.dn_loss_cls: 0.9328, d2.dn_loss_bbox: 2.4970, d3.dn_loss_cls: 0.9328, d3.dn_loss_bbox: 2.4970, d4.dn_loss_cls: 0.9328, d4.dn_loss_bbox: 2.4970, loss_cls_lane: 0.6331, loss_cls_H: 0.6331, loss_bbox_lane: 6.3992, loss_bbox_H: 6.3877, d0.loss_cls_lane: 0.6331, d0.loss_cls_H: 0.6331, d0.loss_bbox_lane: 6.3992, d0.loss_bbox_H: 6.3877, d1.loss_cls_lane: 0.6331, d1.loss_cls_H: 0.6331, d1.loss_bbox_lane: 6.3992, d1.loss_bbox_H: 6.3877, d2.loss_cls_lane: 0.6331, d2.loss_cls_H: 0.6331, d2.loss_bbox_lane: 6.3992, d2.loss_bbox_H: 6.3877, d3.loss_cls_lane: 0.6331, d3.loss_cls_H: 0.6331, d3.loss_bbox_lane: 6.3992, d3.loss_bbox_H: 6.3877, d4.loss_cls_lane: 0.6331, d4.loss_cls_H: 0.6331, d4.loss_bbox_lane: 6.3992, d4.loss_bbox_H: 6.3877, vlm_loss: 0.0000, loss: 132.8958, grad_norm: nan
2025-03-01 08:45:28,251 - mmdet - INFO - Iter [13100/168780]	lr: 3.941e-04, eta: 5 days, 9:51:05, time: 3.200, data_time: 0.017, memory: 22041, loss_cls: 2.0937, loss_bbox: 2.9596, d0.loss_cls: 2.0937, d0.loss_bbox: 2.9596, d1.loss_cls: 2.0937, d1.loss_bbox: 2.9596, d2.loss_cls: 2.0937, d2.loss_bbox: 2.9596, d3.loss_cls: 2.0937, d3.loss_bbox: 2.9596, d4.loss_cls: 2.0937, d4.loss_bbox: 2.9596, dn_loss_cls: 0.9448, dn_loss_bbox: 2.6980, d0.dn_loss_cls: 0.9448, d0.dn_loss_bbox: 2.6980, d1.dn_loss_cls: 0.9448, d1.dn_loss_bbox: 2.6980, d2.dn_loss_cls: 0.9448, d2.dn_loss_bbox: 2.6980, d3.dn_loss_cls: 0.9448, d3.dn_loss_bbox: 2.6980, d4.dn_loss_cls: 0.9448, d4.dn_loss_bbox: 2.6980, loss_cls_lane: 0.6644, loss_cls_H: 0.6644, loss_bbox_lane: 7.1902, loss_bbox_H: 7.1850, d0.loss_cls_lane: 0.6644, d0.loss_cls_H: 0.6644, d0.loss_bbox_lane: 7.1902, d0.loss_bbox_H: 7.1850, d1.loss_cls_lane: 0.6644, d1.loss_cls_H: 0.6644, d1.loss_bbox_lane: 7.1902, d1.loss_bbox_H: 7.1850, d2.loss_cls_lane: 0.6644, d2.loss_cls_H: 0.6644, d2.loss_bbox_lane: 7.1902, d2.loss_bbox_H: 7.1850, d3.loss_cls_lane: 0.6644, d3.loss_cls_H: 0.6644, d3.loss_bbox_lane: 7.1902, d3.loss_bbox_H: 7.1850, d4.loss_cls_lane: 0.6644, d4.loss_cls_H: 0.6644, d4.loss_bbox_lane: 7.1902, d4.loss_bbox_H: 7.1850, vlm_loss: 0.0000, loss: 146.4009, grad_norm: nan
2025-03-01 08:47:58,036 - mmdet - INFO - Iter [13150/168780]	lr: 3.940e-04, eta: 5 days, 9:48:31, time: 2.996, data_time: 0.017, memory: 22041, loss_cls: 1.7004, loss_bbox: 3.0993, d0.loss_cls: 1.7004, d0.loss_bbox: 3.0993, d1.loss_cls: 1.7004, d1.loss_bbox: 3.0993, d2.loss_cls: 1.7004, d2.loss_bbox: 3.0993, d3.loss_cls: 1.7004, d3.loss_bbox: 3.0993, d4.loss_cls: 1.7004, d4.loss_bbox: 3.0993, dn_loss_cls: 0.9558, dn_loss_bbox: 2.6621, d0.dn_loss_cls: 0.9558, d0.dn_loss_bbox: 2.6621, d1.dn_loss_cls: 0.9558, d1.dn_loss_bbox: 2.6621, d2.dn_loss_cls: 0.9558, d2.dn_loss_bbox: 2.6621, d3.dn_loss_cls: 0.9558, d3.dn_loss_bbox: 2.6621, d4.dn_loss_cls: 0.9558, d4.dn_loss_bbox: 2.6621, loss_cls_lane: 0.6313, loss_cls_H: 0.6313, loss_bbox_lane: 6.4712, loss_bbox_H: 6.4731, d0.loss_cls_lane: 0.6313, d0.loss_cls_H: 0.6313, d0.loss_bbox_lane: 6.4712, d0.loss_bbox_H: 6.4731, d1.loss_cls_lane: 0.6313, d1.loss_cls_H: 0.6313, d1.loss_bbox_lane: 6.4712, d1.loss_bbox_H: 6.4731, d2.loss_cls_lane: 0.6313, d2.loss_cls_H: 0.6313, d2.loss_bbox_lane: 6.4712, d2.loss_bbox_H: 6.4731, d3.loss_cls_lane: 0.6313, d3.loss_cls_H: 0.6313, d3.loss_bbox_lane: 6.4712, d3.loss_bbox_H: 6.4731, d4.loss_cls_lane: 0.6313, d4.loss_cls_H: 0.6313, d4.loss_bbox_lane: 6.4712, d4.loss_bbox_H: 6.4731, vlm_loss: 0.0000, loss: 135.7470, grad_norm: nan
2025-03-01 08:51:24,713 - mmdet - INFO - Iter [13200/168780]	lr: 3.940e-04, eta: 5 days, 9:57:07, time: 4.134, data_time: 0.017, memory: 22041, loss_cls: 1.4196, loss_bbox: 2.8927, d0.loss_cls: 1.4196, d0.loss_bbox: 2.8927, d1.loss_cls: 1.4196, d1.loss_bbox: 2.8927, d2.loss_cls: 1.4196, d2.loss_bbox: 2.8927, d3.loss_cls: 1.4196, d3.loss_bbox: 2.8927, d4.loss_cls: 1.4196, d4.loss_bbox: 2.8927, dn_loss_cls: 0.9200, dn_loss_bbox: 2.5106, d0.dn_loss_cls: 0.9200, d0.dn_loss_bbox: 2.5106, d1.dn_loss_cls: 0.9200, d1.dn_loss_bbox: 2.5106, d2.dn_loss_cls: 0.9200, d2.dn_loss_bbox: 2.5106, d3.dn_loss_cls: 0.9200, d3.dn_loss_bbox: 2.5106, d4.dn_loss_cls: 0.9200, d4.dn_loss_bbox: 2.5106, loss_cls_lane: 0.6365, loss_cls_H: 0.6365, loss_bbox_lane: 6.4717, loss_bbox_H: 6.4697, d0.loss_cls_lane: 0.6365, d0.loss_cls_H: 0.6365, d0.loss_bbox_lane: 6.4717, d0.loss_bbox_H: 6.4697, d1.loss_cls_lane: 0.6365, d1.loss_cls_H: 0.6365, d1.loss_bbox_lane: 6.4717, d1.loss_bbox_H: 6.4697, d2.loss_cls_lane: 0.6365, d2.loss_cls_H: 0.6365, d2.loss_bbox_lane: 6.4717, d2.loss_bbox_H: 6.4697, d3.loss_cls_lane: 0.6365, d3.loss_cls_H: 0.6365, d3.loss_bbox_lane: 6.4717, d3.loss_bbox_H: 6.4697, d4.loss_cls_lane: 0.6365, d4.loss_cls_H: 0.6365, d4.loss_bbox_lane: 6.4717, d4.loss_bbox_H: 6.4697, vlm_loss: 0.0000, loss: 131.7436, grad_norm: nan
2025-03-01 08:53:51,271 - mmdet - INFO - Iter [13250/168780]	lr: 3.940e-04, eta: 5 days, 9:53:52, time: 2.931, data_time: 0.016, memory: 22041, loss_cls: 1.7202, loss_bbox: 2.9318, d0.loss_cls: 1.7202, d0.loss_bbox: 2.9318, d1.loss_cls: 1.7202, d1.loss_bbox: 2.9318, d2.loss_cls: 1.7202, d2.loss_bbox: 2.9318, d3.loss_cls: 1.7202, d3.loss_bbox: 2.9318, d4.loss_cls: 1.7202, d4.loss_bbox: 2.9318, dn_loss_cls: 0.9493, dn_loss_bbox: 2.6154, d0.dn_loss_cls: 0.9493, d0.dn_loss_bbox: 2.6154, d1.dn_loss_cls: 0.9493, d1.dn_loss_bbox: 2.6154, d2.dn_loss_cls: 0.9493, d2.dn_loss_bbox: 2.6154, d3.dn_loss_cls: 0.9493, d3.dn_loss_bbox: 2.6154, d4.dn_loss_cls: 0.9493, d4.dn_loss_bbox: 2.6154, loss_cls_lane: 0.6335, loss_cls_H: 0.6335, loss_bbox_lane: 6.4128, loss_bbox_H: 6.4299, d0.loss_cls_lane: 0.6335, d0.loss_cls_H: 0.6335, d0.loss_bbox_lane: 6.4128, d0.loss_bbox_H: 6.4299, d1.loss_cls_lane: 0.6335, d1.loss_cls_H: 0.6335, d1.loss_bbox_lane: 6.4128, d1.loss_bbox_H: 6.4299, d2.loss_cls_lane: 0.6335, d2.loss_cls_H: 0.6335, d2.loss_bbox_lane: 6.4128, d2.loss_bbox_H: 6.4299, d3.loss_cls_lane: 0.6335, d3.loss_cls_H: 0.6335, d3.loss_bbox_lane: 6.4128, d3.loss_bbox_H: 6.4299, d4.loss_cls_lane: 0.6335, d4.loss_cls_H: 0.6335, d4.loss_bbox_lane: 6.4128, d4.loss_bbox_H: 6.4299, vlm_loss: 0.0000, loss: 133.9590, grad_norm: nan
2025-03-01 08:56:23,837 - mmdet - INFO - Iter [13300/168780]	lr: 3.939e-04, eta: 5 days, 9:51:48, time: 3.051, data_time: 0.017, memory: 22041, loss_cls: 1.6047, loss_bbox: 2.8908, d0.loss_cls: 1.6047, d0.loss_bbox: 2.8908, d1.loss_cls: 1.6047, d1.loss_bbox: 2.8908, d2.loss_cls: 1.6047, d2.loss_bbox: 2.8908, d3.loss_cls: 1.6047, d3.loss_bbox: 2.8908, d4.loss_cls: 1.6047, d4.loss_bbox: 2.8908, dn_loss_cls: 0.9777, dn_loss_bbox: 2.6893, d0.dn_loss_cls: 0.9777, d0.dn_loss_bbox: 2.6893, d1.dn_loss_cls: 0.9777, d1.dn_loss_bbox: 2.6893, d2.dn_loss_cls: 0.9777, d2.dn_loss_bbox: 2.6893, d3.dn_loss_cls: 0.9777, d3.dn_loss_bbox: 2.6893, d4.dn_loss_cls: 0.9777, d4.dn_loss_bbox: 2.6893, loss_cls_lane: 0.6516, loss_cls_H: 0.6516, loss_bbox_lane: 6.5341, loss_bbox_H: 6.5324, d0.loss_cls_lane: 0.6516, d0.loss_cls_H: 0.6516, d0.loss_bbox_lane: 6.5341, d0.loss_bbox_H: 6.5324, d1.loss_cls_lane: 0.6516, d1.loss_cls_H: 0.6516, d1.loss_bbox_lane: 6.5341, d1.loss_bbox_H: 6.5324, d2.loss_cls_lane: 0.6516, d2.loss_cls_H: 0.6516, d2.loss_bbox_lane: 6.5341, d2.loss_bbox_H: 6.5324, d3.loss_cls_lane: 0.6516, d3.loss_cls_H: 0.6516, d3.loss_bbox_lane: 6.5341, d3.loss_bbox_H: 6.5324, d4.loss_cls_lane: 0.6516, d4.loss_cls_H: 0.6516, d4.loss_bbox_lane: 6.5341, d4.loss_bbox_H: 6.5324, vlm_loss: 0.0000, loss: 135.1936, grad_norm: nan
2025-03-01 08:58:51,923 - mmdet - INFO - Iter [13350/168780]	lr: 3.939e-04, eta: 5 days, 9:48:51, time: 2.962, data_time: 0.017, memory: 22041, loss_cls: 1.4466, loss_bbox: 2.9657, d0.loss_cls: 1.4466, d0.loss_bbox: 2.9657, d1.loss_cls: 1.4466, d1.loss_bbox: 2.9657, d2.loss_cls: 1.4466, d2.loss_bbox: 2.9657, d3.loss_cls: 1.4466, d3.loss_bbox: 2.9657, d4.loss_cls: 1.4466, d4.loss_bbox: 2.9657, dn_loss_cls: 0.9118, dn_loss_bbox: 2.5662, d0.dn_loss_cls: 0.9118, d0.dn_loss_bbox: 2.5662, d1.dn_loss_cls: 0.9118, d1.dn_loss_bbox: 2.5662, d2.dn_loss_cls: 0.9118, d2.dn_loss_bbox: 2.5662, d3.dn_loss_cls: 0.9118, d3.dn_loss_bbox: 2.5662, d4.dn_loss_cls: 0.9118, d4.dn_loss_bbox: 2.5662, loss_cls_lane: 0.6341, loss_cls_H: 0.6341, loss_bbox_lane: 6.5235, loss_bbox_H: 6.5225, d0.loss_cls_lane: 0.6341, d0.loss_cls_H: 0.6341, d0.loss_bbox_lane: 6.5235, d0.loss_bbox_H: 6.5225, d1.loss_cls_lane: 0.6341, d1.loss_cls_H: 0.6341, d1.loss_bbox_lane: 6.5235, d1.loss_bbox_H: 6.5225, d2.loss_cls_lane: 0.6341, d2.loss_cls_H: 0.6341, d2.loss_bbox_lane: 6.5235, d2.loss_bbox_H: 6.5225, d3.loss_cls_lane: 0.6341, d3.loss_cls_H: 0.6341, d3.loss_bbox_lane: 6.5235, d3.loss_bbox_H: 6.5225, d4.loss_cls_lane: 0.6341, d4.loss_cls_H: 0.6341, d4.loss_bbox_lane: 6.5235, d4.loss_bbox_H: 6.5225, vlm_loss: 0.0000, loss: 133.2260, grad_norm: nan
2025-03-01 09:01:21,449 - mmdet - INFO - Iter [13400/168780]	lr: 3.938e-04, eta: 5 days, 9:46:12, time: 2.991, data_time: 0.017, memory: 22041, loss_cls: 1.5693, loss_bbox: 2.9035, d0.loss_cls: 1.5693, d0.loss_bbox: 2.9035, d1.loss_cls: 1.5693, d1.loss_bbox: 2.9035, d2.loss_cls: 1.5693, d2.loss_bbox: 2.9035, d3.loss_cls: 1.5693, d3.loss_bbox: 2.9035, d4.loss_cls: 1.5693, d4.loss_bbox: 2.9035, dn_loss_cls: 1.0062, dn_loss_bbox: 2.5719, d0.dn_loss_cls: 1.0062, d0.dn_loss_bbox: 2.5719, d1.dn_loss_cls: 1.0062, d1.dn_loss_bbox: 2.5719, d2.dn_loss_cls: 1.0062, d2.dn_loss_bbox: 2.5719, d3.dn_loss_cls: 1.0062, d3.dn_loss_bbox: 2.5719, d4.dn_loss_cls: 1.0062, d4.dn_loss_bbox: 2.5719, loss_cls_lane: 0.6486, loss_cls_H: 0.6486, loss_bbox_lane: 6.5999, loss_bbox_H: 6.6018, d0.loss_cls_lane: 0.6486, d0.loss_cls_H: 0.6486, d0.loss_bbox_lane: 6.5999, d0.loss_bbox_H: 6.6018, d1.loss_cls_lane: 0.6486, d1.loss_cls_H: 0.6486, d1.loss_bbox_lane: 6.5999, d1.loss_bbox_H: 6.6018, d2.loss_cls_lane: 0.6486, d2.loss_cls_H: 0.6486, d2.loss_bbox_lane: 6.5999, d2.loss_bbox_H: 6.6018, d3.loss_cls_lane: 0.6486, d3.loss_cls_H: 0.6486, d3.loss_bbox_lane: 6.5999, d3.loss_bbox_H: 6.6018, d4.loss_cls_lane: 0.6486, d4.loss_cls_H: 0.6486, d4.loss_bbox_lane: 6.5999, d4.loss_bbox_H: 6.6018, vlm_loss: 0.0000, loss: 135.2995, grad_norm: nan
2025-03-01 09:03:48,178 - mmdet - INFO - Iter [13450/168780]	lr: 3.938e-04, eta: 5 days, 9:43:00, time: 2.935, data_time: 0.017, memory: 22041, loss_cls: 2.9471, loss_bbox: 2.9122, d0.loss_cls: 2.9471, d0.loss_bbox: 2.9122, d1.loss_cls: 2.9471, d1.loss_bbox: 2.9122, d2.loss_cls: 2.9471, d2.loss_bbox: 2.9122, d3.loss_cls: 2.9471, d3.loss_bbox: 2.9122, d4.loss_cls: 2.9471, d4.loss_bbox: 2.9122, dn_loss_cls: 0.9298, dn_loss_bbox: 2.6586, d0.dn_loss_cls: 0.9298, d0.dn_loss_bbox: 2.6586, d1.dn_loss_cls: 0.9298, d1.dn_loss_bbox: 2.6586, d2.dn_loss_cls: 0.9298, d2.dn_loss_bbox: 2.6586, d3.dn_loss_cls: 0.9298, d3.dn_loss_bbox: 2.6586, d4.dn_loss_cls: 0.9298, d4.dn_loss_bbox: 2.6586, loss_cls_lane: 0.6398, loss_cls_H: 0.6398, loss_bbox_lane: 6.7020, loss_bbox_H: 6.6891, d0.loss_cls_lane: 0.6398, d0.loss_cls_H: 0.6398, d0.loss_bbox_lane: 6.7020, d0.loss_bbox_H: 6.6891, d1.loss_cls_lane: 0.6398, d1.loss_cls_H: 0.6398, d1.loss_bbox_lane: 6.7020, d1.loss_bbox_H: 6.6891, d2.loss_cls_lane: 0.6398, d2.loss_cls_H: 0.6398, d2.loss_bbox_lane: 6.7020, d2.loss_bbox_H: 6.6891, d3.loss_cls_lane: 0.6398, d3.loss_cls_H: 0.6398, d3.loss_bbox_lane: 6.7020, d3.loss_bbox_H: 6.6891, d4.loss_cls_lane: 0.6398, d4.loss_cls_H: 0.6398, d4.loss_bbox_lane: 6.7020, d4.loss_bbox_H: 6.6891, vlm_loss: 0.0000, loss: 144.7108, grad_norm: nan
2025-03-01 09:06:16,438 - mmdet - INFO - Iter [13500/168780]	lr: 3.937e-04, eta: 5 days, 9:40:06, time: 2.965, data_time: 0.018, memory: 22041, loss_cls: 1.4864, loss_bbox: 2.7128, d0.loss_cls: 1.4864, d0.loss_bbox: 2.7128, d1.loss_cls: 1.4864, d1.loss_bbox: 2.7128, d2.loss_cls: 1.4864, d2.loss_bbox: 2.7128, d3.loss_cls: 1.4864, d3.loss_bbox: 2.7128, d4.loss_cls: 1.4864, d4.loss_bbox: 2.7128, dn_loss_cls: 0.9235, dn_loss_bbox: 2.3821, d0.dn_loss_cls: 0.9235, d0.dn_loss_bbox: 2.3821, d1.dn_loss_cls: 0.9235, d1.dn_loss_bbox: 2.3821, d2.dn_loss_cls: 0.9235, d2.dn_loss_bbox: 2.3821, d3.dn_loss_cls: 0.9235, d3.dn_loss_bbox: 2.3821, d4.dn_loss_cls: 0.9235, d4.dn_loss_bbox: 2.3821, loss_cls_lane: 0.6314, loss_cls_H: 0.6314, loss_bbox_lane: 6.4660, loss_bbox_H: 6.4578, d0.loss_cls_lane: 0.6314, d0.loss_cls_H: 0.6314, d0.loss_bbox_lane: 6.4660, d0.loss_bbox_H: 6.4578, d1.loss_cls_lane: 0.6314, d1.loss_cls_H: 0.6314, d1.loss_bbox_lane: 6.4660, d1.loss_bbox_H: 6.4578, d2.loss_cls_lane: 0.6314, d2.loss_cls_H: 0.6314, d2.loss_bbox_lane: 6.4660, d2.loss_bbox_H: 6.4578, d3.loss_cls_lane: 0.6314, d3.loss_cls_H: 0.6314, d3.loss_bbox_lane: 6.4660, d3.loss_bbox_H: 6.4578, d4.loss_cls_lane: 0.6314, d4.loss_cls_H: 0.6314, d4.loss_bbox_lane: 6.4660, d4.loss_bbox_H: 6.4578, vlm_loss: 0.0000, loss: 130.1489, grad_norm: nan
2025-03-01 09:08:45,667 - mmdet - INFO - Iter [13550/168780]	lr: 3.937e-04, eta: 5 days, 9:37:23, time: 2.985, data_time: 0.017, memory: 22041, loss_cls: 1.4756, loss_bbox: 2.8683, d0.loss_cls: 1.4756, d0.loss_bbox: 2.8683, d1.loss_cls: 1.4756, d1.loss_bbox: 2.8683, d2.loss_cls: 1.4756, d2.loss_bbox: 2.8683, d3.loss_cls: 1.4756, d3.loss_bbox: 2.8683, d4.loss_cls: 1.4756, d4.loss_bbox: 2.8683, dn_loss_cls: 0.8858, dn_loss_bbox: 2.5458, d0.dn_loss_cls: 0.8858, d0.dn_loss_bbox: 2.5458, d1.dn_loss_cls: 0.8858, d1.dn_loss_bbox: 2.5458, d2.dn_loss_cls: 0.8858, d2.dn_loss_bbox: 2.5458, d3.dn_loss_cls: 0.8858, d3.dn_loss_bbox: 2.5458, d4.dn_loss_cls: 0.8858, d4.dn_loss_bbox: 2.5458, loss_cls_lane: 0.6357, loss_cls_H: 0.6357, loss_bbox_lane: 6.5441, loss_bbox_H: 6.5472, d0.loss_cls_lane: 0.6357, d0.loss_cls_H: 0.6357, d0.loss_bbox_lane: 6.5441, d0.loss_bbox_H: 6.5472, d1.loss_cls_lane: 0.6357, d1.loss_cls_H: 0.6357, d1.loss_bbox_lane: 6.5441, d1.loss_bbox_H: 6.5472, d2.loss_cls_lane: 0.6357, d2.loss_cls_H: 0.6357, d2.loss_bbox_lane: 6.5441, d2.loss_bbox_H: 6.5472, d3.loss_cls_lane: 0.6357, d3.loss_cls_H: 0.6357, d3.loss_bbox_lane: 6.5441, d3.loss_bbox_H: 6.5472, d4.loss_cls_lane: 0.6357, d4.loss_cls_H: 0.6357, d4.loss_bbox_lane: 6.5441, d4.loss_bbox_H: 6.5472, vlm_loss: 0.0000, loss: 132.8306, grad_norm: nan
2025-03-01 09:11:12,774 - mmdet - INFO - Iter [13600/168780]	lr: 3.936e-04, eta: 5 days, 9:34:16, time: 2.942, data_time: 0.016, memory: 22041, loss_cls: 3.1667, loss_bbox: 3.1204, d0.loss_cls: 3.1667, d0.loss_bbox: 3.1204, d1.loss_cls: 3.1667, d1.loss_bbox: 3.1204, d2.loss_cls: 3.1667, d2.loss_bbox: 3.1204, d3.loss_cls: 3.1667, d3.loss_bbox: 3.1204, d4.loss_cls: 3.1667, d4.loss_bbox: 3.1204, dn_loss_cls: 0.9346, dn_loss_bbox: 2.7879, d0.dn_loss_cls: 0.9346, d0.dn_loss_bbox: 2.7879, d1.dn_loss_cls: 0.9346, d1.dn_loss_bbox: 2.7879, d2.dn_loss_cls: 0.9346, d2.dn_loss_bbox: 2.7879, d3.dn_loss_cls: 0.9346, d3.dn_loss_bbox: 2.7879, d4.dn_loss_cls: 0.9346, d4.dn_loss_bbox: 2.7879, loss_cls_lane: 0.6438, loss_cls_H: 0.6438, loss_bbox_lane: 6.4909, loss_bbox_H: 6.4728, d0.loss_cls_lane: 0.6438, d0.loss_cls_H: 0.6438, d0.loss_bbox_lane: 6.4909, d0.loss_bbox_H: 6.4728, d1.loss_cls_lane: 0.6438, d1.loss_cls_H: 0.6438, d1.loss_bbox_lane: 6.4909, d1.loss_bbox_H: 6.4728, d2.loss_cls_lane: 0.6438, d2.loss_cls_H: 0.6438, d2.loss_bbox_lane: 6.4909, d2.loss_bbox_H: 6.4728, d3.loss_cls_lane: 0.6438, d3.loss_cls_H: 0.6438, d3.loss_bbox_lane: 6.4909, d3.loss_bbox_H: 6.4728, d4.loss_cls_lane: 0.6438, d4.loss_cls_H: 0.6438, d4.loss_bbox_lane: 6.4909, d4.loss_bbox_H: 6.4728, vlm_loss: 0.0000, loss: 145.5641, grad_norm: nan
2025-03-01 09:13:41,830 - mmdet - INFO - Iter [13650/168780]	lr: 3.936e-04, eta: 5 days, 9:31:32, time: 2.981, data_time: 0.017, memory: 22041, loss_cls: 1.3480, loss_bbox: 2.6372, d0.loss_cls: 1.3480, d0.loss_bbox: 2.6372, d1.loss_cls: 1.3480, d1.loss_bbox: 2.6372, d2.loss_cls: 1.3480, d2.loss_bbox: 2.6372, d3.loss_cls: 1.3480, d3.loss_bbox: 2.6372, d4.loss_cls: 1.3480, d4.loss_bbox: 2.6372, dn_loss_cls: 0.9457, dn_loss_bbox: 2.3034, d0.dn_loss_cls: 0.9457, d0.dn_loss_bbox: 2.3034, d1.dn_loss_cls: 0.9457, d1.dn_loss_bbox: 2.3034, d2.dn_loss_cls: 0.9457, d2.dn_loss_bbox: 2.3034, d3.dn_loss_cls: 0.9457, d3.dn_loss_bbox: 2.3034, d4.dn_loss_cls: 0.9457, d4.dn_loss_bbox: 2.3034, loss_cls_lane: 0.6379, loss_cls_H: 0.6379, loss_bbox_lane: 6.3550, loss_bbox_H: 6.3605, d0.loss_cls_lane: 0.6379, d0.loss_cls_H: 0.6379, d0.loss_bbox_lane: 6.3550, d0.loss_bbox_H: 6.3605, d1.loss_cls_lane: 0.6379, d1.loss_cls_H: 0.6379, d1.loss_bbox_lane: 6.3550, d1.loss_bbox_H: 6.3605, d2.loss_cls_lane: 0.6379, d2.loss_cls_H: 0.6379, d2.loss_bbox_lane: 6.3550, d2.loss_bbox_H: 6.3605, d3.loss_cls_lane: 0.6379, d3.loss_cls_H: 0.6379, d3.loss_bbox_lane: 6.3550, d3.loss_bbox_H: 6.3605, d4.loss_cls_lane: 0.6379, d4.loss_cls_H: 0.6379, d4.loss_bbox_lane: 6.3550, d4.loss_bbox_H: 6.3605, vlm_loss: 0.0000, loss: 127.3533, grad_norm: nan
2025-03-01 09:16:11,234 - mmdet - INFO - Iter [13700/168780]	lr: 3.935e-04, eta: 5 days, 9:28:51, time: 2.988, data_time: 0.017, memory: 22041, loss_cls: 1.9380, loss_bbox: 2.8857, d0.loss_cls: 1.9380, d0.loss_bbox: 2.8857, d1.loss_cls: 1.9380, d1.loss_bbox: 2.8857, d2.loss_cls: 1.9380, d2.loss_bbox: 2.8857, d3.loss_cls: 1.9380, d3.loss_bbox: 2.8857, d4.loss_cls: 1.9380, d4.loss_bbox: 2.8857, dn_loss_cls: 1.0261, dn_loss_bbox: 2.5531, d0.dn_loss_cls: 1.0261, d0.dn_loss_bbox: 2.5531, d1.dn_loss_cls: 1.0261, d1.dn_loss_bbox: 2.5531, d2.dn_loss_cls: 1.0261, d2.dn_loss_bbox: 2.5531, d3.dn_loss_cls: 1.0261, d3.dn_loss_bbox: 2.5531, d4.dn_loss_cls: 1.0261, d4.dn_loss_bbox: 2.5531, loss_cls_lane: 0.6385, loss_cls_H: 0.6385, loss_bbox_lane: 6.4825, loss_bbox_H: 6.5104, d0.loss_cls_lane: 0.6385, d0.loss_cls_H: 0.6385, d0.loss_bbox_lane: 6.4825, d0.loss_bbox_H: 6.5104, d1.loss_cls_lane: 0.6385, d1.loss_cls_H: 0.6385, d1.loss_bbox_lane: 6.4825, d1.loss_bbox_H: 6.5104, d2.loss_cls_lane: 0.6385, d2.loss_cls_H: 0.6385, d2.loss_bbox_lane: 6.4825, d2.loss_bbox_H: 6.5104, d3.loss_cls_lane: 0.6385, d3.loss_cls_H: 0.6385, d3.loss_bbox_lane: 6.4825, d3.loss_bbox_H: 6.5104, d4.loss_cls_lane: 0.6385, d4.loss_cls_H: 0.6385, d4.loss_bbox_lane: 6.4825, d4.loss_bbox_H: 6.5104, vlm_loss: 0.0000, loss: 136.0379, grad_norm: nan
2025-03-01 09:18:57,653 - mmdet - INFO - Iter [13750/168780]	lr: 3.935e-04, eta: 5 days, 9:29:23, time: 3.328, data_time: 0.018, memory: 22041, loss_cls: 1.2728, loss_bbox: 2.6720, d0.loss_cls: 1.2728, d0.loss_bbox: 2.6720, d1.loss_cls: 1.2728, d1.loss_bbox: 2.6720, d2.loss_cls: 1.2728, d2.loss_bbox: 2.6720, d3.loss_cls: 1.2728, d3.loss_bbox: 2.6720, d4.loss_cls: 1.2728, d4.loss_bbox: 2.6720, dn_loss_cls: 0.9878, dn_loss_bbox: 2.2564, d0.dn_loss_cls: 0.9878, d0.dn_loss_bbox: 2.2564, d1.dn_loss_cls: 0.9878, d1.dn_loss_bbox: 2.2564, d2.dn_loss_cls: 0.9878, d2.dn_loss_bbox: 2.2564, d3.dn_loss_cls: 0.9878, d3.dn_loss_bbox: 2.2564, d4.dn_loss_cls: 0.9878, d4.dn_loss_bbox: 2.2564, loss_cls_lane: 0.6367, loss_cls_H: 0.6367, loss_bbox_lane: 6.7047, loss_bbox_H: 6.7007, d0.loss_cls_lane: 0.6367, d0.loss_cls_H: 0.6367, d0.loss_bbox_lane: 6.7047, d0.loss_bbox_H: 6.7007, d1.loss_cls_lane: 0.6367, d1.loss_cls_H: 0.6367, d1.loss_bbox_lane: 6.7047, d1.loss_bbox_H: 6.7007, d2.loss_cls_lane: 0.6367, d2.loss_cls_H: 0.6367, d2.loss_bbox_lane: 6.7047, d2.loss_bbox_H: 6.7007, d3.loss_cls_lane: 0.6367, d3.loss_cls_H: 0.6367, d3.loss_bbox_lane: 6.7047, d3.loss_bbox_H: 6.7007, d4.loss_cls_lane: 0.6367, d4.loss_cls_H: 0.6367, d4.loss_bbox_lane: 6.7047, d4.loss_bbox_H: 6.7007, vlm_loss: 0.0000, loss: 131.2073, grad_norm: nan
2025-03-01 09:21:25,124 - mmdet - INFO - Iter [13800/168780]	lr: 3.934e-04, eta: 5 days, 9:26:20, time: 2.949, data_time: 0.018, memory: 22041, loss_cls: 1.4605, loss_bbox: 2.6956, d0.loss_cls: 1.4605, d0.loss_bbox: 2.6956, d1.loss_cls: 1.4605, d1.loss_bbox: 2.6956, d2.loss_cls: 1.4605, d2.loss_bbox: 2.6956, d3.loss_cls: 1.4605, d3.loss_bbox: 2.6956, d4.loss_cls: 1.4605, d4.loss_bbox: 2.6956, dn_loss_cls: 0.9456, dn_loss_bbox: 2.3824, d0.dn_loss_cls: 0.9456, d0.dn_loss_bbox: 2.3824, d1.dn_loss_cls: 0.9456, d1.dn_loss_bbox: 2.3824, d2.dn_loss_cls: 0.9456, d2.dn_loss_bbox: 2.3824, d3.dn_loss_cls: 0.9456, d3.dn_loss_bbox: 2.3824, d4.dn_loss_cls: 0.9456, d4.dn_loss_bbox: 2.3824, loss_cls_lane: 0.6332, loss_cls_H: 0.7407, loss_bbox_lane: 6.3324, loss_bbox_H: 6.3315, d0.loss_cls_lane: 0.6332, d0.loss_cls_H: 0.7407, d0.loss_bbox_lane: 6.3324, d0.loss_bbox_H: 6.3315, d1.loss_cls_lane: 0.6332, d1.loss_cls_H: 0.7407, d1.loss_bbox_lane: 6.3324, d1.loss_bbox_H: 6.3315, d2.loss_cls_lane: 0.6332, d2.loss_cls_H: 0.7407, d2.loss_bbox_lane: 6.3324, d2.loss_bbox_H: 6.3315, d3.loss_cls_lane: 0.6332, d3.loss_cls_H: 0.7407, d3.loss_bbox_lane: 6.3324, d3.loss_bbox_H: 6.3315, d4.loss_cls_lane: 0.6332, d4.loss_cls_H: 0.7407, d4.loss_bbox_lane: 6.3324, d4.loss_bbox_H: 6.3315, vlm_loss: 0.0000, loss: 129.1321, grad_norm: nan
2025-03-01 09:23:51,736 - mmdet - INFO - Iter [13850/168780]	lr: 3.934e-04, eta: 5 days, 9:23:08, time: 2.932, data_time: 0.018, memory: 22041, loss_cls: 2.1615, loss_bbox: 2.8785, d0.loss_cls: 2.1615, d0.loss_bbox: 2.8785, d1.loss_cls: 2.1615, d1.loss_bbox: 2.8785, d2.loss_cls: 2.1615, d2.loss_bbox: 2.8785, d3.loss_cls: 2.1615, d3.loss_bbox: 2.8785, d4.loss_cls: 2.1615, d4.loss_bbox: 2.8785, dn_loss_cls: 0.9348, dn_loss_bbox: 2.4969, d0.dn_loss_cls: 0.9348, d0.dn_loss_bbox: 2.4969, d1.dn_loss_cls: 0.9348, d1.dn_loss_bbox: 2.4969, d2.dn_loss_cls: 0.9348, d2.dn_loss_bbox: 2.4969, d3.dn_loss_cls: 0.9348, d3.dn_loss_bbox: 2.4969, d4.dn_loss_cls: 0.9348, d4.dn_loss_bbox: 2.4969, loss_cls_lane: 0.6739, loss_cls_H: 0.7814, loss_bbox_lane: 7.0186, loss_bbox_H: 7.0023, d0.loss_cls_lane: 0.6739, d0.loss_cls_H: 0.7814, d0.loss_bbox_lane: 7.0186, d0.loss_bbox_H: 7.0023, d1.loss_cls_lane: 0.6739, d1.loss_cls_H: 0.7814, d1.loss_bbox_lane: 7.0186, d1.loss_bbox_H: 7.0023, d2.loss_cls_lane: 0.6739, d2.loss_cls_H: 0.7814, d2.loss_bbox_lane: 7.0186, d2.loss_bbox_H: 7.0023, d3.loss_cls_lane: 0.6739, d3.loss_cls_H: 0.7814, d3.loss_bbox_lane: 7.0186, d3.loss_bbox_H: 7.0023, d4.loss_cls_lane: 0.6739, d4.loss_cls_H: 0.7814, d4.loss_bbox_lane: 7.0186, d4.loss_bbox_H: 7.0023, vlm_loss: 0.0000, loss: 143.6871, grad_norm: nan
2025-03-01 09:26:20,520 - mmdet - INFO - Iter [13900/168780]	lr: 3.934e-04, eta: 5 days, 9:20:21, time: 2.976, data_time: 0.018, memory: 22041, loss_cls: 1.4151, loss_bbox: 2.8268, d0.loss_cls: 1.4151, d0.loss_bbox: 2.8268, d1.loss_cls: 1.4151, d1.loss_bbox: 2.8268, d2.loss_cls: 1.4151, d2.loss_bbox: 2.8268, d3.loss_cls: 1.4151, d3.loss_bbox: 2.8268, d4.loss_cls: 1.4151, d4.loss_bbox: 2.8268, dn_loss_cls: 1.0264, dn_loss_bbox: 2.3638, d0.dn_loss_cls: 1.0264, d0.dn_loss_bbox: 2.3638, d1.dn_loss_cls: 1.0264, d1.dn_loss_bbox: 2.3638, d2.dn_loss_cls: 1.0264, d2.dn_loss_bbox: 2.3638, d3.dn_loss_cls: 1.0264, d3.dn_loss_bbox: 2.3638, d4.dn_loss_cls: 1.0264, d4.dn_loss_bbox: 2.3638, loss_cls_lane: 0.6416, loss_cls_H: 0.6416, loss_bbox_lane: 6.7289, loss_bbox_H: 6.7144, d0.loss_cls_lane: 0.6416, d0.loss_cls_H: 0.6416, d0.loss_bbox_lane: 6.7289, d0.loss_bbox_H: 6.7144, d1.loss_cls_lane: 0.6416, d1.loss_cls_H: 0.6416, d1.loss_bbox_lane: 6.7289, d1.loss_bbox_H: 6.7144, d2.loss_cls_lane: 0.6416, d2.loss_cls_H: 0.6416, d2.loss_bbox_lane: 6.7289, d2.loss_bbox_H: 6.7144, d3.loss_cls_lane: 0.6416, d3.loss_cls_H: 0.6416, d3.loss_bbox_lane: 6.7289, d3.loss_bbox_H: 6.7144, d4.loss_cls_lane: 0.6416, d4.loss_cls_H: 0.6416, d4.loss_bbox_lane: 6.7289, d4.loss_bbox_H: 6.7144, vlm_loss: 0.0000, loss: 134.1514, grad_norm: nan
2025-03-01 09:28:49,054 - mmdet - INFO - Iter [13950/168780]	lr: 3.933e-04, eta: 5 days, 9:17:31, time: 2.971, data_time: 0.018, memory: 22041, loss_cls: 1.7019, loss_bbox: 2.7571, d0.loss_cls: 1.7019, d0.loss_bbox: 2.7571, d1.loss_cls: 1.7019, d1.loss_bbox: 2.7571, d2.loss_cls: 1.7019, d2.loss_bbox: 2.7571, d3.loss_cls: 1.7019, d3.loss_bbox: 2.7571, d4.loss_cls: 1.7019, d4.loss_bbox: 2.7571, dn_loss_cls: 0.9221, dn_loss_bbox: 2.3883, d0.dn_loss_cls: 0.9221, d0.dn_loss_bbox: 2.3883, d1.dn_loss_cls: 0.9221, d1.dn_loss_bbox: 2.3883, d2.dn_loss_cls: 0.9221, d2.dn_loss_bbox: 2.3883, d3.dn_loss_cls: 0.9221, d3.dn_loss_bbox: 2.3883, d4.dn_loss_cls: 0.9221, d4.dn_loss_bbox: 2.3883, loss_cls_lane: 0.6393, loss_cls_H: 0.6393, loss_bbox_lane: 6.5205, loss_bbox_H: 6.5176, d0.loss_cls_lane: 0.6393, d0.loss_cls_H: 0.6393, d0.loss_bbox_lane: 6.5205, d0.loss_bbox_H: 6.5176, d1.loss_cls_lane: 0.6393, d1.loss_cls_H: 0.6393, d1.loss_bbox_lane: 6.5205, d1.loss_bbox_H: 6.5176, d2.loss_cls_lane: 0.6393, d2.loss_cls_H: 0.6393, d2.loss_bbox_lane: 6.5205, d2.loss_bbox_H: 6.5176, d3.loss_cls_lane: 0.6393, d3.loss_cls_H: 0.6393, d3.loss_bbox_lane: 6.5205, d3.loss_bbox_H: 6.5176, d4.loss_cls_lane: 0.6393, d4.loss_cls_H: 0.6393, d4.loss_bbox_lane: 6.5205, d4.loss_bbox_H: 6.5176, vlm_loss: 0.0000, loss: 132.5157, grad_norm: nan
2025-03-01 09:31:16,805 - mmdet - INFO - Exp name: mask_eva_lane_det_vlm.py
2025-03-01 09:31:16,805 - mmdet - INFO - Iter [14000/168780]	lr: 3.933e-04, eta: 5 days, 9:14:32, time: 2.955, data_time: 0.017, memory: 22041, loss_cls: 1.8193, loss_bbox: 2.8987, d0.loss_cls: 1.8193, d0.loss_bbox: 2.8987, d1.loss_cls: 1.8193, d1.loss_bbox: 2.8987, d2.loss_cls: 1.8193, d2.loss_bbox: 2.8987, d3.loss_cls: 1.8193, d3.loss_bbox: 2.8987, d4.loss_cls: 1.8193, d4.loss_bbox: 2.8987, dn_loss_cls: 0.9666, dn_loss_bbox: 2.7283, d0.dn_loss_cls: 0.9666, d0.dn_loss_bbox: 2.7283, d1.dn_loss_cls: 0.9666, d1.dn_loss_bbox: 2.7283, d2.dn_loss_cls: 0.9666, d2.dn_loss_bbox: 2.7283, d3.dn_loss_cls: 0.9666, d3.dn_loss_bbox: 2.7283, d4.dn_loss_cls: 0.9666, d4.dn_loss_bbox: 2.7283, loss_cls_lane: 0.6240, loss_cls_H: 0.6240, loss_bbox_lane: 6.6108, loss_bbox_H: 6.5986, d0.loss_cls_lane: 0.6240, d0.loss_cls_H: 0.6240, d0.loss_bbox_lane: 6.6108, d0.loss_bbox_H: 6.5986, d1.loss_cls_lane: 0.6240, d1.loss_cls_H: 0.6240, d1.loss_bbox_lane: 6.6108, d1.loss_bbox_H: 6.5986, d2.loss_cls_lane: 0.6240, d2.loss_cls_H: 0.6240, d2.loss_bbox_lane: 6.6108, d2.loss_bbox_H: 6.5986, d3.loss_cls_lane: 0.6240, d3.loss_cls_H: 0.6240, d3.loss_bbox_lane: 6.6108, d3.loss_bbox_H: 6.5986, d4.loss_cls_lane: 0.6240, d4.loss_cls_H: 0.6240, d4.loss_bbox_lane: 6.6108, d4.loss_bbox_H: 6.5986, vlm_loss: 0.0000, loss: 137.2219, grad_norm: nan
2025-03-01 09:33:46,208 - mmdet - INFO - Iter [14050/168780]	lr: 3.932e-04, eta: 5 days, 9:11:52, time: 2.988, data_time: 0.017, memory: 22041, loss_cls: 1.5159, loss_bbox: 2.8792, d0.loss_cls: 1.5159, d0.loss_bbox: 2.8792, d1.loss_cls: 1.5159, d1.loss_bbox: 2.8792, d2.loss_cls: 1.5159, d2.loss_bbox: 2.8792, d3.loss_cls: 1.5159, d3.loss_bbox: 2.8792, d4.loss_cls: 1.5159, d4.loss_bbox: 2.8792, dn_loss_cls: 0.8859, dn_loss_bbox: 2.5214, d0.dn_loss_cls: 0.8859, d0.dn_loss_bbox: 2.5214, d1.dn_loss_cls: 0.8859, d1.dn_loss_bbox: 2.5214, d2.dn_loss_cls: 0.8859, d2.dn_loss_bbox: 2.5214, d3.dn_loss_cls: 0.8859, d3.dn_loss_bbox: 2.5214, d4.dn_loss_cls: 0.8859, d4.dn_loss_bbox: 2.5214, loss_cls_lane: 0.6433, loss_cls_H: 0.6970, loss_bbox_lane: 6.1242, loss_bbox_H: 6.1318, d0.loss_cls_lane: 0.6433, d0.loss_cls_H: 0.6970, d0.loss_bbox_lane: 6.1242, d0.loss_bbox_H: 6.1318, d1.loss_cls_lane: 0.6433, d1.loss_cls_H: 0.6970, d1.loss_bbox_lane: 6.1242, d1.loss_bbox_H: 6.1318, d2.loss_cls_lane: 0.6433, d2.loss_cls_H: 0.6970, d2.loss_bbox_lane: 6.1242, d2.loss_bbox_H: 6.1318, d3.loss_cls_lane: 0.6433, d3.loss_cls_H: 0.6970, d3.loss_bbox_lane: 6.1242, d3.loss_bbox_H: 6.1318, d4.loss_cls_lane: 0.6433, d4.loss_cls_H: 0.6970, d4.loss_bbox_lane: 6.1242, d4.loss_bbox_H: 6.1318, vlm_loss: 0.0000, loss: 128.3911, grad_norm: nan
2025-03-01 09:34:31,055 - mmdet - INFO - Saving checkpoint at 14065 iterations
