{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/lib/wsl/lib:\" + \"/usr/local/cuda-11.8/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If you are on a cluster, make sure you are on a CUDA machine!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mist_sophia/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/cuda_setup/paths.py:93: UserWarning: /home/mist_sophia/miniconda3/envs/omnidrive did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n",
      "/home/mist_sophia/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/mist_sophia/miniconda3/envs/omnidrive/lib/python3.9/site-packages/cv2/../../lib64')}\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cuDeviceGetCount'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprojects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmdet3d_plugin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n",
      "File \u001b[0;32m~/ad/test/OmniDrive/projects/mmdet3d_plugin/__init__.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense_heads\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m  \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetectors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnecks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/ad/test/OmniDrive/projects/mmdet3d_plugin/models/dense_heads/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfocal_head\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FocalHead\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreampetr_head\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StreamPETRHead\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpetr_head_map\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PETRHeadM\n",
      "File \u001b[0;32m~/ad/test/OmniDrive/projects/mmdet3d_plugin/models/dense_heads/focal_head.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HEADS, build_loss\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense_heads\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manchor_free_head\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnchorFreeHead\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprojects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmdet3d_plugin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_heatmap_gaussian, apply_center_offset, apply_ltrb\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bbox_overlaps\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmdet3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clip_sigmoid\n",
      "File \u001b[0;32m~/ad/test/OmniDrive/projects/mmdet3d_plugin/models/utils/misc.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bbox_xyxy_to_cxcywh\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inverse_sigmoid\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense_heads\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava_llama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlavaLlamaForCausalLM\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/peft/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.12.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     AutoPeftModel,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     26\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     27\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     28\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     29\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     33\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     inject_adapter_in_model,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftMixedModel\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/peft/auto.py:31\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     AutoModel,\n\u001b[1;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     PeftModel,\n\u001b[1;32m     35\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/peft/config.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_NAME, PeftType, TaskType\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPeftConfigMixin\u001b[39;00m(PushToHubMixin):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    This is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    PEFT adapter models. This class inherits from [`~transformers.utils.PushToHubMixin`] which contains the methods to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m        peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/peft/utils/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloftq_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m replace_lora_weights_loftq\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftType, TaskType\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mother\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n\u001b[1;32m     25\u001b[0m     TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n\u001b[1;32m     26\u001b[0m     TRANSFORMERS_MODELS_TO_ADALORA_TARGET_MODULES_MAPPING,\n\u001b[1;32m     27\u001b[0m     TRANSFORMERS_MODELS_TO_IA3_TARGET_MODULES_MAPPING,\n\u001b[1;32m     28\u001b[0m     TRANSFORMERS_MODELS_TO_IA3_FEEDFORWARD_MODULES_MAPPING,\n\u001b[1;32m     29\u001b[0m     TRANSFORMERS_MODELS_TO_LNTUNING_TARGET_MODULES_MAPPING,\n\u001b[1;32m     30\u001b[0m     TRANSFORMERS_MODELS_TO_VERA_TARGET_MODULES_MAPPING,\n\u001b[1;32m     31\u001b[0m     TRANSFORMERS_MODELS_TO_FOURIERFT_TARGET_MODULES_MAPPING,\n\u001b[1;32m     32\u001b[0m     CONFIG_NAME,\n\u001b[1;32m     33\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m     34\u001b[0m     SAFETENSORS_WEIGHTS_NAME,\n\u001b[1;32m     35\u001b[0m     INCLUDE_LINEAR_LAYERS_SHORTHAND,\n\u001b[1;32m     36\u001b[0m     _set_trainable,\n\u001b[1;32m     37\u001b[0m     bloom_model_postprocess_past_key_value,\n\u001b[1;32m     38\u001b[0m     prepare_model_for_kbit_training,\n\u001b[1;32m     39\u001b[0m     shift_tokens_right,\n\u001b[1;32m     40\u001b[0m     transpose,\n\u001b[1;32m     41\u001b[0m     _get_batch_size,\n\u001b[1;32m     42\u001b[0m     _get_submodules,\n\u001b[1;32m     43\u001b[0m     _set_adapter,\n\u001b[1;32m     44\u001b[0m     _freeze_adapter,\n\u001b[1;32m     45\u001b[0m     ModulesToSaveWrapper,\n\u001b[1;32m     46\u001b[0m     _prepare_prompt_learning_config,\n\u001b[1;32m     47\u001b[0m     _is_valid_match,\n\u001b[1;32m     48\u001b[0m     infer_device,\n\u001b[1;32m     49\u001b[0m     get_auto_gptq_quant_linear,\n\u001b[1;32m     50\u001b[0m     get_quantization_config,\n\u001b[1;32m     51\u001b[0m     id_tensor_storage,\n\u001b[1;32m     52\u001b[0m     cast_mixed_precision_params,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave_and_load\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_peft_model_state_dict, set_peft_model_state_dict, load_peft_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/peft/utils/other.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nullcontext\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_hook_to_module, remove_hook_from_module\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.21.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m skip_first_batches\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/accelerate/accelerator.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SCALER_NAME,\n\u001b[1;32m     29\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     30\u001b[0m     get_pretty_name,\n\u001b[1;32m     31\u001b[0m     is_tpu_available,\n\u001b[1;32m     32\u001b[0m     is_xpu_available,\n\u001b[1;32m     33\u001b[0m     save,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxm\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/accelerate/utils/__init__.py:131\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_deepspeed_available():\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m         DeepSpeedEngineWrapper,\n\u001b[1;32m    124\u001b[0m         DeepSpeedOptimizerWrapper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         HfDeepSpeedConfig,\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     PrepareForLaunch,\n\u001b[1;32m    135\u001b[0m     _filter_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     prepare_tpu,\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/accelerate/utils/bnb.py:42\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     find_tied_parameters,\n\u001b[1;32m     33\u001b[0m     get_balanced_memory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     set_module_tensor_to_device,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_available():\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m     47\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     MatmulLtState,\n\u001b[1;32m      8\u001b[0m     bmm_cublas,\n\u001b[1;32m      9\u001b[0m     matmul,\n\u001b[1;32m     10\u001b[0m     matmul_cublas,\n\u001b[1;32m     11\u001b[0m     mm_cublas,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m modules\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reduce  \u001b[38;5;66;03m# Required in Python 3\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/functional.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA, lib\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reduce  \u001b[38;5;66;03m# Required in Python 3\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# math.prod not compatible with python < 3.8\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/cextension.py:41\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance\n\u001b[0;32m---> 41\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mCUDALibrary_Singleton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlib\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     lib\u001b[38;5;241m.\u001b[39mcadam32bit_g32\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/cextension.py:37\u001b[0m, in \u001b[0;36mCUDALibrary_Singleton.get_instance\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/cextension.py:15\u001b[0m, in \u001b[0;36mCUDALibrary_Singleton.initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minitialize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     binary_name \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_cuda_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     package_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m     17\u001b[0m     binary_path \u001b[38;5;241m=\u001b[39m package_dir \u001b[38;5;241m/\u001b[39m binary_name\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:132\u001b[0m, in \u001b[0;36mevaluate_cuda_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA SETUP: CUDA runtime path found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcudart_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m cuda \u001b[38;5;241m=\u001b[39m get_cuda_lib_handle()\n\u001b[0;32m--> 132\u001b[0m cc \u001b[38;5;241m=\u001b[39m \u001b[43mget_compute_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA SETUP: Highest compute capability among GPUs detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m cuda_version_string \u001b[38;5;241m=\u001b[39m get_cuda_version(cuda, cudart_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:105\u001b[0m, in \u001b[0;36mget_compute_capability\u001b[0;34m(cuda)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_compute_capability\u001b[39m(cuda):\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Extracts the highest compute capbility from all available GPUs, as compute\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    capabilities are downwards compatible. If no GPUs are detected, it returns\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    None.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     ccs \u001b[38;5;241m=\u001b[39m \u001b[43mget_compute_capabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ccs:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# TODO: handle different compute capabilities; for now, take the max\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ccs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:83\u001b[0m, in \u001b[0;36mget_compute_capabilities\u001b[0;34m(cuda)\u001b[0m\n\u001b[1;32m     79\u001b[0m cc_minor \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m     81\u001b[0m device \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[0;32m---> 83\u001b[0m check_cuda_result(cuda, \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuDeviceGetCount\u001b[49m(ctypes\u001b[38;5;241m.\u001b[39mbyref(nGpus)))\n\u001b[1;32m     84\u001b[0m ccs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nGpus\u001b[38;5;241m.\u001b[39mvalue):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cuDeviceGetCount'"
     ]
    }
   ],
   "source": [
    "from projects.mmdet3d_plugin.models.utils.misc import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3819b23646bf4acbbcf2f52f98357895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "/home/mist_sophia/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cget_col_row_stats",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m use_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m frozen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m lm_head \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lora\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ad/test/OmniDrive/projects/mmdet3d_plugin/models/utils/misc.py:222\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(base_model, use_lora, frozen)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(base_model, use_lora, frozen):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# model = LlavaLlamaForCausalLM.from_pretrained(base_model, torch_dtype=torch.float16, device_map='cpu')\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable INT8 quantization\u001b[39;49;00m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Let the library decide optimal device mapping\u001b[39;49;00m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frozen:\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/transformers/modeling_utils.py:2903\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2894\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2896\u001b[0m     (\n\u001b[1;32m   2897\u001b[0m         model,\n\u001b[1;32m   2898\u001b[0m         missing_keys,\n\u001b[1;32m   2899\u001b[0m         unexpected_keys,\n\u001b[1;32m   2900\u001b[0m         mismatched_keys,\n\u001b[1;32m   2901\u001b[0m         offload_index,\n\u001b[1;32m   2902\u001b[0m         error_msgs,\n\u001b[0;32m-> 2903\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2921\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   2922\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/transformers/modeling_utils.py:3260\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3250\u001b[0m mismatched_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3251\u001b[0m     state_dict,\n\u001b[1;32m   3252\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3256\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3257\u001b[0m )\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m-> 3260\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3268\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3275\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3276\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/transformers/modeling_utils.py:725\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    722\u001b[0m             fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_name:\n\u001b[0;32m--> 725\u001b[0m             \u001b[43mset_module_quantized_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/transformers/utils/bitsandbytes.py:97\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m     95\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_8bit:\n\u001b[0;32m---> 97\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInt8Params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_4bit:\n\u001b[1;32m     99\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParams4bit(new_value, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:196\u001b[0m, in \u001b[0;36mInt8Params.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    192\u001b[0m     device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m ):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     new_param \u001b[38;5;241m=\u001b[39m Int8Params(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    200\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m         has_fp16_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_fp16_weights,\n\u001b[1;32m    204\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:160\u001b[0m, in \u001b[0;36mInt8Params.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# we store the 8-bit rows-major weight\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# we convert this weight to the turning/ampere weight during the first inference pass\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mcuda(device)\n\u001b[0;32m--> 160\u001b[0m     CB, CBt, SCB, SCBt, coo_tensorB \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m CBt\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m SCBt\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/functional.py:1616\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   1613\u001b[0m     rows \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_stats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m col_stats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1616\u001b[0m     row_stats, col_stats, nnz_row_ptr \u001b[38;5;241m=\u001b[39m \u001b[43mget_colrow_absmax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1621\u001b[0m     out_col \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(A\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint8)\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/functional.py:1505\u001b[0m, in \u001b[0;36mget_colrow_absmax\u001b[0;34m(A, row_stats, col_stats, nnz_block_ptr, threshold)\u001b[0m\n\u001b[1;32m   1503\u001b[0m prev_device \u001b[38;5;241m=\u001b[39m pre_call(A\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1504\u001b[0m is_on_gpu([A, row_stats, col_stats, nnz_block_ptr])\n\u001b[0;32m-> 1505\u001b[0m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcget_col_row_stats\u001b[49m(ptrA, ptrRowStats, ptrColStats, ptrNnzrows, ct\u001b[38;5;241m.\u001b[39mc_float(threshold), rows, cols)\n\u001b[1;32m   1506\u001b[0m post_call(prev_device)\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/ctypes/__init__.py:395\u001b[0m, in \u001b[0;36mCDLL.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name)\n\u001b[0;32m--> 395\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, func)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/ctypes/__init__.py:400\u001b[0m, in \u001b[0;36mCDLL.__getitem__\u001b[0;34m(self, name_or_ordinal)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_ordinal):\n\u001b[0;32m--> 400\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_FuncPtr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_ordinal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_ordinal, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    402\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m name_or_ordinal\n",
      "\u001b[0;31mAttributeError\u001b[0m: /home/mist_sophia/miniconda3/envs/omnidrive/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cget_col_row_stats"
     ]
    }
   ],
   "source": [
    "llm_path = 'ckpts/pretrain_qformer/'\n",
    "use_lora = True\n",
    "frozen = True\n",
    "lm_head = load_model(llm_path, use_lora, frozen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096]) cpu\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 16]) cpu\n",
      "total_params: 16777216\n",
      "vram usage: 64.0 MB\n"
     ]
    }
   ],
   "source": [
    "# print all trainable parameters and the sum of parameters\n",
    "total_params = 0\n",
    "for name, param in lm_head.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.size(), param.device)\n",
    "        total_params += param.numel()\n",
    "print('total_params:', total_params)\n",
    "# calculate the vram usage\n",
    "print('vram usage:', total_params * 4 / 1024 / 1024, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"addmm_impl_cpu_\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m lm_head\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m200\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)) \u001b[38;5;66;03m# 50265 is the vocab size， \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/peft/peft_model.py:1577\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1576\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ad/test/OmniDrive/projects/mmdet3d_plugin/models/dense_heads/llava_llama.py:117\u001b[0m, in \u001b[0;36mLlavaLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict)\u001b[0m\n\u001b[1;32m    114\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:305\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    302\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(value_states, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    307\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/peft/tuners/lora/layer.py:544\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m     torch_result_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m active_adapter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/omnidrive/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"addmm_impl_cpu_\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "# # test inference\n",
    "# lm_head.eval()\n",
    "# input_ids = torch.randint(0, 200, (1, 10)) # 50265 is the vocab size， \n",
    "# output = lm_head(input_ids)\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[-4.1250,  3.6855,  4.9648,  ..., -0.2080, -2.0977, -1.0938],\n",
      "         [-4.1172,  3.7070,  4.9844,  ..., -0.2076, -2.0918, -1.0859],\n",
      "         [-4.1875,  3.4023,  5.1406,  ..., -0.1011, -2.0684, -0.9219],\n",
      "         ...,\n",
      "         [-3.6445,  4.0117, 10.2031,  ..., -1.4746, -4.4258, -2.0957],\n",
      "         [-2.8359,  3.2422,  9.6172,  ..., -2.1465, -3.4727, -2.1797],\n",
      "         [-4.8008,  1.6445,  8.6484,  ..., -2.3789, -5.0234, -3.3418]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "lm_head = lm_head.cuda()\n",
    "input_ids = torch.randint(0, 200, (1, 10)).cuda()\n",
    "output = lm_head(input_ids)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 loss: -2.0485732555389404\n",
      "iter: 1 loss: -3.696895122528076\n",
      "iter: 2 loss: -4.588339805603027\n",
      "iter: 3 loss: -4.525819778442383\n",
      "iter: 4 loss: -4.069866180419922\n",
      "iter: 5 loss: -2.3532967567443848\n",
      "iter: 6 loss: -2.5736355781555176\n",
      "iter: 7 loss: -3.713627576828003\n",
      "iter: 8 loss: -0.38266563415527344\n",
      "iter: 9 loss: -3.4557642936706543\n",
      "iter: 10 loss: -4.066431999206543\n",
      "iter: 11 loss: -2.7196075916290283\n",
      "iter: 12 loss: -2.879365921020508\n",
      "iter: 13 loss: -2.307417869567871\n",
      "iter: 14 loss: -3.119807720184326\n",
      "iter: 15 loss: -0.26283368468284607\n",
      "iter: 16 loss: -3.4330461025238037\n",
      "iter: 17 loss: -2.933001756668091\n",
      "iter: 18 loss: -4.130236625671387\n",
      "iter: 19 loss: -4.039763927459717\n",
      "iter: 20 loss: -2.822705030441284\n",
      "iter: 21 loss: -3.1969642639160156\n",
      "iter: 22 loss: -4.301186561584473\n",
      "iter: 23 loss: -4.305310249328613\n",
      "iter: 24 loss: -3.644876003265381\n",
      "iter: 25 loss: -3.6827425956726074\n",
      "iter: 26 loss: -3.849670171737671\n",
      "iter: 27 loss: -2.803415298461914\n",
      "iter: 28 loss: -4.035284042358398\n",
      "iter: 29 loss: -4.065495491027832\n",
      "iter: 30 loss: -2.229296922683716\n",
      "iter: 31 loss: -1.4550600051879883\n",
      "iter: 32 loss: -4.592149257659912\n",
      "iter: 33 loss: -3.833770275115967\n",
      "iter: 34 loss: -3.7962749004364014\n",
      "iter: 35 loss: -4.473836421966553\n",
      "iter: 36 loss: -3.013256072998047\n",
      "iter: 37 loss: -3.8269662857055664\n",
      "iter: 38 loss: -0.2916733920574188\n",
      "iter: 39 loss: -2.5117945671081543\n",
      "iter: 40 loss: -3.699793577194214\n",
      "iter: 41 loss: -0.30789056420326233\n",
      "iter: 42 loss: -2.6180217266082764\n",
      "iter: 43 loss: -2.8271355628967285\n",
      "iter: 44 loss: -3.4418256282806396\n",
      "iter: 45 loss: -4.204350471496582\n",
      "iter: 46 loss: -3.4315967559814453\n",
      "iter: 47 loss: -2.508777618408203\n",
      "iter: 48 loss: -3.655911684036255\n",
      "iter: 49 loss: -3.5893781185150146\n",
      "iter: 50 loss: -3.7619779109954834\n",
      "iter: 51 loss: -3.259516716003418\n",
      "iter: 52 loss: -3.744879961013794\n",
      "iter: 53 loss: -3.576540470123291\n",
      "iter: 54 loss: -2.2991793155670166\n",
      "iter: 55 loss: -4.48537015914917\n",
      "iter: 56 loss: -4.633018493652344\n",
      "iter: 57 loss: -4.540092468261719\n",
      "iter: 58 loss: -2.675877332687378\n",
      "iter: 59 loss: -4.964188575744629\n",
      "iter: 60 loss: -4.018563747406006\n",
      "iter: 61 loss: -3.8422608375549316\n",
      "iter: 62 loss: -3.835434675216675\n",
      "iter: 63 loss: -4.603667736053467\n",
      "iter: 64 loss: -2.8388450145721436\n",
      "iter: 65 loss: -4.02072811126709\n",
      "iter: 66 loss: -3.875516653060913\n",
      "iter: 67 loss: -3.8969740867614746\n",
      "iter: 68 loss: -3.769730806350708\n",
      "iter: 69 loss: -3.1848764419555664\n",
      "iter: 70 loss: -3.083733320236206\n",
      "iter: 71 loss: -3.4148619174957275\n",
      "iter: 72 loss: -4.071559429168701\n",
      "iter: 73 loss: -3.734706163406372\n",
      "iter: 74 loss: -3.751840591430664\n",
      "iter: 75 loss: -3.9858975410461426\n",
      "iter: 76 loss: -0.7372825741767883\n",
      "iter: 77 loss: -4.452963829040527\n",
      "iter: 78 loss: -3.8899919986724854\n",
      "iter: 79 loss: -4.308371543884277\n",
      "iter: 80 loss: -2.233232259750366\n",
      "iter: 81 loss: -4.196660041809082\n",
      "iter: 82 loss: -0.36890581250190735\n",
      "iter: 83 loss: -2.785595655441284\n",
      "iter: 84 loss: -3.9331846237182617\n",
      "iter: 85 loss: -3.7156288623809814\n",
      "iter: 86 loss: -4.099510192871094\n",
      "iter: 87 loss: -4.807897567749023\n",
      "iter: 88 loss: -4.518273830413818\n",
      "iter: 89 loss: -4.9202728271484375\n",
      "iter: 90 loss: -4.420037269592285\n",
      "iter: 91 loss: -4.180782794952393\n",
      "iter: 92 loss: -2.509929656982422\n",
      "iter: 93 loss: -3.763373374938965\n",
      "iter: 94 loss: -4.499797344207764\n",
      "iter: 95 loss: -3.6616744995117188\n",
      "iter: 96 loss: -3.8331964015960693\n",
      "iter: 97 loss: -2.3513596057891846\n",
      "iter: 98 loss: -3.9584014415740967\n",
      "iter: 99 loss: -3.539383888244629\n"
     ]
    }
   ],
   "source": [
    "# simple training test\n",
    "for it in range(100):\n",
    "    input_ids = torch.randint(0, 2000, (1, 10)).cuda() #  ,2000 is \n",
    "    output = lm_head(input_ids)\n",
    "    loss = output.logits.mean()\n",
    "    loss.backward()\n",
    "    print('iter:', it, 'loss:', loss.item())\n",
    "    # update lora\n",
    "    # LlavaLlamaForCausalLM' object has no attribute 'update_lora'\n",
    "    \n",
    "    # lm_head.update_lora()\n",
    "    # print('lora updated')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnidrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
